{"metadata": {"dbt_schema_version": "https://schemas.getdbt.com/dbt/run-results/v6.json", "dbt_version": "1.10.15", "generated_at": "2025-12-30T08:46:04.167191Z", "invocation_id": "4d9e4c1f-020f-4ae7-a65d-576b3ed74576", "invocation_started_at": "2025-12-30T08:45:56.400608Z", "env": {}}, "results": [{"status": "success", "timing": [{"name": "compile", "started_at": "2025-12-30T08:45:59.083197Z", "completed_at": "2025-12-30T08:45:59.138541Z"}, {"name": "execute", "started_at": "2025-12-30T08:45:59.139898Z", "completed_at": "2025-12-30T08:45:59.527254Z"}], "thread_id": "Thread-1 (worker)", "execution_time": 0.4611539840698242, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.stg_arbitrum__deposits", "compiled": true, "compiled_code": "-- Staging model for FundsDeposited events from Arbitrum\n-- This model extracts deposit events where users initiate cross-chain bridge transactions\n\nWITH raw_deposits AS (\n\n    SELECT\n        -- Core event metadata - these identify WHEN and WHERE the event happened\n        timestamp_datetime,              -- When the event occurred (from blockchain)\n        transactionHash,                 -- Unique transaction identifier\n        blockchain,                      -- Which blockchain (should be 'arbitrum')\n        source_file,                     -- Which source file this came from (for lineage)\n        \n        -- Indexed fields from topics (these are searchable/filterable in blockchain)\n        -- Topics are like \"indexed columns\" in SQL - they're stored separately for fast queries\n        topic_destination_chain_id,      -- Chain ID where the funds are being sent TO\n        topic_deposit_id,                -- Unique deposit identifier (links deposits to fills)\n        topic_depositor,                  -- Address of the user who initiated the deposit\n        \n        -- Non-indexed fields from the event's data field\n        -- These are the actual business data about the deposit\n        funds_deposited_data_input_token,          -- Token address being deposited (on origin chain)\n        funds_deposited_data_output_token,         -- Token address to receive (on destination chain)\n        funds_deposited_data_input_amount,          -- Amount deposited (in input token units)\n        funds_deposited_data_output_amount,         -- Expected amount to receive (in output token units)\n        funds_deposited_data_recipient             -- Final recipient of the bridged funds\n        -- Optional fields: Not required for capital flow analysis, commented out in database schema\n        -- funds_deposited_data_quote_timestamp,       -- When the exchange rate quote was generated\n        -- funds_deposited_data_fill_deadline,         -- Deadline by which the deposit must be filled\n        -- funds_deposited_data_exclusivity_deadline,  -- Deadline for exclusive relayer rights\n        -- funds_deposited_data_exclusive_relayer      -- Address with exclusive fill rights (if any)\n        \n    FROM raw.arbitrum_logs_processed\n    \n    -- Filter: Only include rows where FundsDeposited data exists\n    -- Topic_0 is the event signature hash that identifies the event type\n    WHERE topic_0 = '0x32ed1a409ef04c7b0227189c3a103dc5ac10e775a15b785dcc510201f7c25ad3'\n        AND funds_deposited_data_input_amount IS NOT NULL\n        AND funds_deposited_data_output_amount IS NOT NULL\n),\n\n-- INPUT token metadata: filtered to origin chain (Arbitrum)\n-- This is correct because input_token is always on the origin chain\ninput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n    WHERE chain = 'arbitrum'\n\n),\n\n-- OUTPUT token metadata: includes chain_id for matching with destination_chain_id\n-- This allows us to find the correct token on whichever chain the funds are going to\noutput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals,\n        chain_id\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n\n),\n\ncleaned_deposits AS (\n    SELECT\n        \n        -- Timestamp: Convert text to proper timestamp type\n        -- The ETL process may store timestamps as Unix integers or ISO strings\n        CASE \n            WHEN timestamp_datetime ~ '^\\d+$' THEN          -- If it's pure digits, treat as Unix timestamp (seconds since 1970-01-01)\n                TO_TIMESTAMP(timestamp_datetime::BIGINT)    -- PostgreSQL's TO_TIMESTAMP converts Unix seconds to timestamp\n            ELSE \n                timestamp_datetime::TIMESTAMP               -- Otherwise, try to parse as ISO format string (e.g., \"2025-12-03 10:30:00\")\n        END AS deposit_timestamp,\n        \n        transactionHash AS transaction_hash,\n        blockchain,\n        source_file,\n        \n        -- ============================================================\n        -- INDEXED FIELDS (from topics) - Already decoded by ETL\n        -- ============================================================\n        -- These come from the event's \"topics\" array, which is indexed for fast blockchain queries\n        \n        -- Destination chain ID: Which blockchain the funds are being sent TO\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\n        -- Example: 1 = Ethereum, 8453 = Base, 137 = Polygon\n        -- Cast via NUMERIC first to handle decimal strings like \"8453.0\" from ETL\n        (topic_destination_chain_id::NUMERIC)::BIGINT AS destination_chain_id,\n        \n        -- Deposit ID: Unique identifier linking this deposit to its fill(s)\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\n        -- This is the KEY that connects deposits \u2194 fills across chains\n        -- Cast via NUMERIC first to handle decimal strings like \"12345.0\" from ETL\n        (topic_deposit_id::NUMERIC)::BIGINT AS deposit_id,\n        \n        -- Depositor address: Who initiated the bridge transaction\n        -- Already decoded by ETL to proper address format (0x...)\n        topic_depositor AS depositor_address,\n        \n        -- ============================================================\n        -- TOKEN INFORMATION (what was deposited)\n        -- ============================================================\n        \n        -- Input token: The token address being deposited on the origin chain\n        -- Already decoded by ETL to proper address format\n        -- Example: 0xaf88d065aC88dCc5619a6eeFdD463aAbdE3eE2c3 = USDC on Arbitrum\n        -- Normalize to lowercase for joining with token metadata\n        LOWER(funds_deposited_data_input_token) AS input_token_address,\n        \n        -- Output token: The token address to receive on the destination chain\n        -- Already decoded by ETL to proper address format\n        -- Example: 0xA0b86991c6218b36c1d19D4a2e9Eb0cE3606eB48 = USDC on Ethereum\n        -- Normalize to lowercase for joining with token metadata\n        LOWER(funds_deposited_data_output_token) AS output_token_address,\n        \n        -- ============================================================\n        -- AMOUNT INFORMATION (how much was deposited)\n        -- ============================================================\n        \n        -- Input amount: How much was deposited on the origin chain\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\n        -- Store as RAW amount first (before rescaling by decimals)\n        funds_deposited_data_input_amount::NUMERIC AS input_amount_raw,\n        \n        -- Output amount: Expected amount to receive on destination chain\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\n        -- Store as RAW amount first (before rescaling by decimals)\n        funds_deposited_data_output_amount::NUMERIC AS output_amount_raw,\n        \n        -- ============================================================\n        -- TIMING INFORMATION (deadlines and quotes)\n        -- ============================================================\n        -- NOTE: These fields are NOT required for capital flow analysis and are commented out in database schema\n        \n        -- Quote timestamp: When the exchange rate quote was generated\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\n        -- Used to determine which exchange rate was used for the bridge\n        -- Cast via NUMERIC first to handle decimal strings like \"1733234567.0\" from ETL\n        -- (funds_deposited_data_quote_timestamp::NUMERIC)::BIGINT AS quote_timestamp,\n        \n        -- Fill deadline: Latest timestamp by which the deposit must be filled\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\n        -- If not filled by this time, the deposit can be refunded\n        -- Cast via NUMERIC first to handle decimal strings like \"1733234567.0\" from ETL\n        -- (funds_deposited_data_fill_deadline::NUMERIC)::BIGINT AS fill_deadline,\n        \n        -- Exclusivity deadline: Latest timestamp for exclusive relayer rights\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\n        -- Before this deadline, only the exclusive_relayer can fill this deposit\n        -- Cast via NUMERIC first to handle decimal strings like \"1733234567.0\" from ETL\n        -- (funds_deposited_data_exclusivity_deadline::NUMERIC)::BIGINT AS exclusivity_deadline,\n        \n        -- ============================================================\n        -- USER & RELAYER INFORMATION\n        -- ============================================================\n        \n        -- Recipient: Who receives the funds on the destination chain\n        -- Already decoded by ETL to proper address format\n        -- Usually the same as depositor, but can be different (gift/transfer)\n        funds_deposited_data_recipient AS recipient_address\n        \n        -- Exclusive relayer: Address with exclusive fill rights (if any)\n        -- Already decoded by ETL to proper address format\n        -- Used for priority/guaranteed fills (NULL if no exclusive relayer)\n        -- NOTE: Field is NOT required for capital flow analysis and is commented out in database schema\n        -- funds_deposited_data_exclusive_relayer AS exclusive_relayer_address\n        \n    FROM raw_deposits\n)\n\n\n-- ============================================================\n-- FINAL SELECT: Join with token metadata and rescale amounts\n-- ============================================================\nSELECT\n    -- Event identity\n    c.deposit_timestamp,\n    c.transaction_hash,\n    c.blockchain,\n    c.source_file,\n    \n    -- Indexed fields\n    c.destination_chain_id,\n    c.deposit_id,\n    c.depositor_address,\n    \n    -- \u2728 NEW: Token information WITH NAMES\n    -- Input token (what was deposited on origin chain - Arbitrum)\n    c.input_token_address,\n    input_tok.token_symbol AS input_token_symbol,        -- e.g., 'USDC', 'WETH'\n    \n    -- Output token (what will be received on destination chain)\n    c.output_token_address,\n    output_tok.token_symbol AS output_token_symbol,      -- e.g., 'USDC', 'WETH'\n    \n    -- \u2728 NEW: Rescaled amounts (human-readable)\n    -- These use the rescale_amount macro which divides raw amount by 10^decimals\n    -- Example: 5000000 raw USDC (6 decimals) \u2192 5.0 USDC\n    \n    c.input_amount_raw / POWER(10, COALESCE(input_tok.decimals, 18))\n AS input_amount,\n    \n    CASE \n        WHEN c.output_token_address = '0x0000000000000000000000000000000000000000' \n        THEN c.output_amount_raw / POWER(10, COALESCE(input_tok.decimals, 18))\n        ELSE c.output_amount_raw / POWER(10, COALESCE(output_tok.decimals, 18))\n    END\n AS output_amount,\n    \n    -- \u2728 NEW: Raw amounts (preserved for auditing)\n    -- These are the original blockchain values before rescaling\n    -- Example: 5.0 USDC is stored as 5000000 on blockchain\n    c.input_amount_raw,\n    c.output_amount_raw,\n    \n    -- Timing (commented out - available in schema but not included in output)\n    -- quote_timestamp,\n    -- fill_deadline,\n    -- exclusivity_deadline,\n    \n    -- User info\n    c.recipient_address\n    -- exclusive_relayer_address  -- Commented out - available in schema but not included in output\n    \nFROM cleaned_deposits c\n\n-- Join with token metadata to get decimals and symbols for INPUT token\n-- Join on address only - we already filtered to origin chain (Arbitrum)\nLEFT JOIN input_token_meta AS input_tok\n    ON c.input_token_address = input_tok.token_address\n\n-- Join with token metadata to get decimals and symbols for OUTPUT token\n-- Join on BOTH address AND chain_id to match the correct destination chain\nLEFT JOIN output_token_meta AS output_tok\n    ON c.output_token_address = output_tok.token_address\n    AND c.destination_chain_id = output_tok.chain_id\n\n-- Data quality: Only include rows with essential fields populated\nWHERE c.deposit_id IS NOT NULL\n    AND c.depositor_address IS NOT NULL\n    AND c.input_token_address IS NOT NULL\n    AND c.output_token_address IS NOT NULL\n    AND c.input_amount_raw IS NOT NULL\n    AND c.output_amount_raw IS NOT NULL", "relation_name": "\"across_analytics\".\"dbt_staging\".\"stg_arbitrum__deposits\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2025-12-30T08:45:59.112560Z", "completed_at": "2025-12-30T08:45:59.194805Z"}, {"name": "execute", "started_at": "2025-12-30T08:45:59.195743Z", "completed_at": "2025-12-30T08:45:59.562799Z"}], "thread_id": "Thread-4 (worker)", "execution_time": 0.49158549308776855, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.stg_ethereum__deposits", "compiled": true, "compiled_code": "-- Staging model for FundsDeposited events from Ethereum\n-- This model extracts deposit events where users initiate cross-chain bridge transactions\n\nWITH raw_deposits AS (\n\n    SELECT\n        -- Core event metadata - these identify WHEN and WHERE the event happened\n        timestamp_datetime,              -- When the event occurred (from blockchain)\n        transactionHash,                 -- Unique transaction identifier\n        blockchain,                      -- Which blockchain (should be 'ethereum')\n        source_file,                     -- Which source file this came from (for lineage)\n        \n        -- Indexed fields from topics (these are searchable/filterable in blockchain)\n        -- Topics are like \"indexed columns\" in SQL - they're stored separately for fast queries\n        topic_destination_chain_id,      -- Chain ID where the funds are being sent TO\n        topic_deposit_id,                -- Unique deposit identifier (links deposits to fills)\n        topic_depositor,                  -- Address of the user who initiated the deposit\n        \n        -- Non-indexed fields from the event's data field\n        -- These are the actual business data about the deposit\n        funds_deposited_data_input_token,          -- Token address being deposited (on origin chain)\n        funds_deposited_data_output_token,         -- Token address to receive (on destination chain)\n        funds_deposited_data_input_amount,          -- Amount deposited (in input token units)\n        funds_deposited_data_output_amount,         -- Expected amount to receive (in output token units)\n        funds_deposited_data_recipient             -- Final recipient of the bridged funds\n        \n    FROM raw.ethereum_logs_processed\n    \n    -- Filter: Only include rows where FundsDeposited data exists\n    -- Topic_0 is the event signature hash that identifies the event type\n    WHERE topic_0 = '0x32ed1a409ef04c7b0227189c3a103dc5ac10e775a15b785dcc510201f7c25ad3'\n        AND funds_deposited_data_input_amount IS NOT NULL\n        AND funds_deposited_data_output_amount IS NOT NULL\n),\n\n-- INPUT token metadata: filtered to origin chain (Ethereum)\n-- This is correct because input_token is always on the origin chain\ninput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n    WHERE chain = 'ethereum'\n\n),\n\n-- OUTPUT token metadata: includes chain_id for matching with destination_chain_id\n-- This allows us to find the correct token on whichever chain the funds are going to\noutput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals,\n        chain_id\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n\n),\n\ncleaned_deposits AS (\n    SELECT\n        \n        -- Timestamp: Convert text to proper timestamp type\n        CASE \n            WHEN timestamp_datetime ~ '^\\d+$' THEN\n                TO_TIMESTAMP(timestamp_datetime::BIGINT)\n            ELSE \n                timestamp_datetime::TIMESTAMP\n        END AS deposit_timestamp,\n        \n        transactionHash AS transaction_hash,\n        blockchain,\n        source_file,\n        \n        -- Destination chain ID\n        (topic_destination_chain_id::NUMERIC)::BIGINT AS destination_chain_id,\n        \n        -- Deposit ID\n        (topic_deposit_id::NUMERIC)::BIGINT AS deposit_id,\n        \n        -- Depositor address\n        topic_depositor AS depositor_address,\n        \n        -- Token addresses (normalized to lowercase for joining)\n        LOWER(funds_deposited_data_input_token) AS input_token_address,\n        LOWER(funds_deposited_data_output_token) AS output_token_address,\n        \n        -- Raw amounts (before rescaling)\n        funds_deposited_data_input_amount::NUMERIC AS input_amount_raw,\n        funds_deposited_data_output_amount::NUMERIC AS output_amount_raw,\n        \n        -- Recipient\n        funds_deposited_data_recipient AS recipient_address\n        \n    FROM raw_deposits\n)\n\n-- Final SELECT: Join with token metadata and rescale amounts\nSELECT\n    -- Event identity\n    c.deposit_timestamp,\n    c.transaction_hash,\n    c.blockchain,\n    c.source_file,\n    \n    -- Indexed fields\n    c.destination_chain_id,\n    c.deposit_id,\n    c.depositor_address,\n    \n    -- Input token (what was deposited on origin chain - Ethereum)\n    c.input_token_address,\n    input_tok.token_symbol AS input_token_symbol,\n    \n    -- Output token (what will be received on destination chain)\n    c.output_token_address,\n    output_tok.token_symbol AS output_token_symbol,\n    \n    -- Rescaled amounts (human-readable)\n    \n    c.input_amount_raw / POWER(10, COALESCE(input_tok.decimals, 18))\n AS input_amount,\n    \n    CASE \n        WHEN c.output_token_address = '0x0000000000000000000000000000000000000000' \n        THEN c.output_amount_raw / POWER(10, COALESCE(input_tok.decimals, 18))\n        ELSE c.output_amount_raw / POWER(10, COALESCE(output_tok.decimals, 18))\n    END\n AS output_amount,\n    \n    -- Raw amounts (preserved for auditing)\n    c.input_amount_raw,\n    c.output_amount_raw,\n    \n    -- User info\n    c.recipient_address\n    \nFROM cleaned_deposits c\n\n-- Join with token metadata for INPUT token (origin chain)\nLEFT JOIN input_token_meta AS input_tok\n    ON c.input_token_address = input_tok.token_address\n\n-- Join with token metadata for OUTPUT token (destination chain)\nLEFT JOIN output_token_meta AS output_tok\n    ON c.output_token_address = output_tok.token_address\n    AND c.destination_chain_id = output_tok.chain_id\n\n-- Data quality: Only include rows with essential fields populated\nWHERE c.deposit_id IS NOT NULL\n    AND c.depositor_address IS NOT NULL\n    AND c.input_token_address IS NOT NULL\n    AND c.output_token_address IS NOT NULL\n    AND c.input_amount_raw IS NOT NULL\n    AND c.output_amount_raw IS NOT NULL", "relation_name": "\"across_analytics\".\"dbt_staging\".\"stg_ethereum__deposits\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2025-12-30T08:45:59.122584Z", "completed_at": "2025-12-30T08:45:59.222233Z"}, {"name": "execute", "started_at": "2025-12-30T08:45:59.223304Z", "completed_at": "2025-12-30T08:45:59.575515Z"}], "thread_id": "Thread-3 (worker)", "execution_time": 0.500739574432373, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.stg_arbitrum__refunds", "compiled": true, "compiled_code": "-- Staging model for ExecutedRelayerRefundRoot events from Arbitrum\r\n\r\nWITH raw_refunds AS (\r\n\r\n    SELECT\r\n        -- Core event metadata - these identify WHEN and WHERE the event happened\r\n        timestamp_datetime,              -- When the event occurred (from blockchain)\r\n        transactionHash,                 -- Unique transaction identifier\r\n        blockchain,                      -- Which blockchain (should be 'arbitrum')\r\n        source_file,                     -- Which source file this came from (for lineage)\r\n        \r\n        -- Indexed fields from topics (these are searchable/filterable in blockchain)\r\n        -- Topics are like \"indexed columns\" in SQL - they're stored separately for fast queries\r\n        topic_chain_id,                  -- Chain ID where the refund was executed\r\n        topic_root_bundle_id,            -- Merkle root bundle identifier (groups multiple refunds)\r\n        topic_leaf_id,                   -- Leaf index in the merkle tree (identifies this specific refund batch)\r\n        \r\n        -- Non-indexed fields from the event's data field\r\n        -- These are the actual business data about the refund\r\n        amount_to_return,                -- Total amount of capital returned in this batch\r\n        l2_token_address,                -- Token address being refunded (on Arbitrum L2)\r\n        refund_amounts,                  -- Comma-separated list of individual refund amounts\r\n        refund_addresses,                -- Comma-separated list of relayer addresses receiving refunds\r\n        refund_count                     -- Number of relayers being refunded in this batch\r\n        \r\n    FROM raw.arbitrum_logs_processed\r\n    \r\n    -- Filter: Only include rows where ExecutedRelayerRefundRoot data exists\r\n    -- Topic_0 is the event signature hash that identifies the event type\r\n    WHERE topic_0 = '0xf4ad92585b1bc117fbdd644990adf0827bc4c95baeae8a23322af807b6d0020e'\r\n        AND amount_to_return IS NOT NULL\r\n        AND l2_token_address IS NOT NULL\r\n        AND refund_count IS NOT NULL\r\n),\r\n\r\ncleaned_refunds AS (\r\n    SELECT\r\n        \r\n        -- Timestamp: Convert text to proper timestamp type\r\n        CASE \r\n            WHEN timestamp_datetime ~ '^\\d+$' THEN          -- If it's pure digits, treat as Unix timestamp (seconds since 1970-01-01)\r\n                TO_TIMESTAMP(timestamp_datetime::BIGINT)    -- PostgreSQL's TO_TIMESTAMP converts Unix seconds to timestamp\r\n            ELSE \r\n                timestamp_datetime::TIMESTAMP               -- Otherwise, try to parse as ISO format string (e.g., \"2025-12-03 10:30:00\")\r\n        END AS refund_timestamp,\r\n        \r\n        transactionHash AS transaction_hash,\r\n        blockchain,\r\n        source_file,\r\n        \r\n        -- ============================================================\r\n        -- INDEXED FIELDS (from topics) - Already decoded by ETL\r\n        -- ============================================================\r\n        -- These come from the event's \"topics\" array, which is indexed for fast blockchain queries\r\n        \r\n        -- Chain ID: Which blockchain this refund was executed on\r\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\r\n        -- Example: 42161 = Arbitrum, 1 = Ethereum, 8453 = Base\r\n        -- Cast via NUMERIC first to handle decimal strings like \"42161.0\" from ETL\r\n        (topic_chain_id::NUMERIC)::BIGINT AS chain_id,\r\n        \r\n        -- Root bundle ID: Groups multiple refunds together in a merkle tree\r\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\r\n        -- Multiple refund batches can be grouped into one merkle root for gas efficiency\r\n        -- Cast via NUMERIC first to handle decimal strings like \"12345.0\" from ETL\r\n        (topic_root_bundle_id::NUMERIC)::BIGINT AS root_bundle_id,\r\n        \r\n        -- Leaf ID: Identifies this specific refund batch within the merkle tree\r\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\r\n        -- Each leaf represents one batch of refunds to multiple relayers\r\n        -- Cast via NUMERIC first to handle decimal strings like \"789.0\" from ETL\r\n        (topic_leaf_id::NUMERIC)::BIGINT AS leaf_id,\r\n        \r\n        -- ============================================================\r\n        -- TOKEN INFORMATION (what was refunded)\r\n        -- ============================================================\r\n        \r\n        -- L2 token address: The token being refunded on Arbitrum\r\n        -- Already decoded by ETL to proper address format (0x...)\r\n        -- Example: 0xaf88d065aC88dCc5619a6eeFdD463aAbdE3eE2c3 = USDC on Arbitrum\r\n        l2_token_address AS refund_token_address,\r\n        \r\n        -- ============================================================\r\n        -- AMOUNT INFORMATION (how much was refunded)\r\n        -- ============================================================\r\n        \r\n        -- Amount to return: Total capital returned in this batch\r\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\r\n        -- This is the sum of all individual refund amounts in the batch\r\n        -- Stored as NUMERIC to handle large token amounts (18 decimals)\r\n        amount_to_return::NUMERIC AS total_refund_amount,\r\n        \r\n        -- Refund amounts: Individual refund amounts as comma-separated string\r\n        -- Format: \"1000000000000000000,2000000000000000000,3000000000000000000\"\r\n        -- Each value is in token's base units (e.g., wei for ETH, 6 decimals for USDC)\r\n        -- This is a comma-separated string because the ETL processes dynamic arrays\r\n        refund_amounts AS refund_amounts_string,\r\n        \r\n        -- Refund addresses: Relayer addresses receiving refunds as comma-separated string\r\n        -- Format: \"0xAAA...,0xBBB...,0xCCC...\"\r\n        -- Matches 1:1 with refund_amounts (first address gets first amount, etc.)\r\n        -- This is a comma-separated string because the ETL processes dynamic arrays\r\n        refund_addresses AS refund_addresses_string,\r\n        \r\n        -- Refund count: Number of relayers being refunded in this batch\r\n        -- Already converted by ETL to integer\r\n        -- This tells us how many individual refunds are in the arrays above\r\n        refund_count::INTEGER AS refund_count\r\n        \r\n    FROM raw_refunds\r\n)\r\n\r\n-- Final SELECT: Add any computed fields and ensure data quality\r\nSELECT\r\n    -- Event identity\r\n    refund_timestamp,\r\n    transaction_hash,\r\n    blockchain,\r\n    source_file,\r\n    \r\n    -- Indexed fields (merkle tree identifiers)\r\n    chain_id,\r\n    root_bundle_id,\r\n    leaf_id,\r\n    \r\n    -- Token info\r\n    refund_token_address,\r\n    \r\n    -- Amounts\r\n    total_refund_amount,\r\n    refund_amounts_string,\r\n    refund_addresses_string,\r\n    refund_count\r\n    \r\nFROM cleaned_refunds\r\n\r\n-- Data quality: Only include rows with essential fields populated\r\nWHERE chain_id IS NOT NULL\r\n    AND refund_token_address IS NOT NULL\r\n    AND total_refund_amount IS NOT NULL\r\n    AND refund_count IS NOT NULL", "relation_name": "\"across_analytics\".\"dbt_staging\".\"stg_arbitrum__refunds\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2025-12-30T08:45:59.130368Z", "completed_at": "2025-12-30T08:45:59.208979Z"}, {"name": "execute", "started_at": "2025-12-30T08:45:59.210228Z", "completed_at": "2025-12-30T08:45:59.599055Z"}], "thread_id": "Thread-2 (worker)", "execution_time": 0.5231523513793945, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.stg_arbitrum__fills", "compiled": true, "compiled_code": "-- Staging model for FilledRelay events from Arbitrum\n-- This model extracts fill events where relayers complete cross-chain bridge transactions\n\nWITH raw_fills AS (\n\n    SELECT\n        -- Core event metadata - these identify WHEN and WHERE the event happened\n        timestamp_datetime,              -- When the event occurred (from blockchain)\n        transactionHash,                 -- Unique transaction identifier\n        blockchain,                      -- Which blockchain (should be 'arbitrum')\n        source_file,                     -- Which source file this came from (for lineage)\n        \n        -- Indexed fields from topics (these are searchable/filterable in blockchain)\n        -- Topics are like \"indexed columns\" in SQL - they're stored separately for fast queries\n        topic_origin_chain_id,           -- Chain ID where the original deposit happened\n        topic_deposit_id,                -- Unique deposit identifier (links to deposit event)\n        topic_relayer,                   -- Address of the relayer who provided liquidity\n        \n        -- Non-indexed fields from the event's data field\n        -- These are the actual business data about the fill\n        filled_relay_data_input_token,          -- Token address on origin chain\n        filled_relay_data_output_token,          -- Token address on destination chain\n        filled_relay_data_input_amount,          -- Amount sent (in origin token units)\n        filled_relay_data_output_amount,         -- Amount received (in destination token units)\n        filled_relay_data_repayment_chain_id,     -- Where relayer gets reimbursed\n        filled_relay_data_exclusive_relayer,     -- Address with exclusive fill rights (if any)\n        filled_relay_data_depositor,             -- Original user who initiated the bridge\n        filled_relay_data_recipient,              -- Final recipient of the bridged funds\n        \n        -- Gas data (for relayer cost analysis)\n        gas_price_wei,\n        gas_used\n        \n    FROM raw.arbitrum_logs_processed\n    \n    -- Filter: Only include rows where FilledRelay data exists\n    WHERE topic_0 = '0x44b559f101f8fbcc8a0ea43fa91a05a729a5ea6e14a7c75aa750374690137208'\n        AND filled_relay_data_input_amount IS NOT NULL\n        AND filled_relay_data_output_amount IS NOT NULL\n),\n\n-- INPUT token metadata: includes chain_id for matching with origin_chain_id\n-- This allows us to find the correct token on whichever chain the deposit came from\ninput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals,\n        chain_id\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n\n),\n\n-- OUTPUT token metadata: filtered to destination chain (Arbitrum)\n-- This is correct because on fills, output_token is always on the destination chain\noutput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n    WHERE chain = 'arbitrum'\n\n),\n\ncleaned_fills AS (\n    SELECT\n        \n        -- Timestamp: Convert text to proper timestamp type\n        \n        CASE \n            WHEN timestamp_datetime ~ '^\\d+$' THEN          -- If it's pure digits, treat as Unix timestamp (seconds since 1970-01-01)\n                TO_TIMESTAMP(timestamp_datetime::BIGINT)    -- PostgreSQL's TO_TIMESTAMP can parse various formats, but we use the simplest approach\n            ELSE \n                timestamp_datetime::TIMESTAMP               -- Otherwise, try to parse as ISO format string\n        END AS fill_timestamp,\n        \n        transactionHash AS transaction_hash,\n        blockchain,\n        source_file,\n        \n        -- ============================================================\n        -- INDEXED FIELDS (from topics) - Already decoded by ETL\n        -- ============================================================\n        -- These come from the event's \"topics\" array, which is indexed for fast blockchain queries\n        \n        -- Origin chain ID: Which blockchain the funds came FROM\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\n        -- Example: 1 = Ethereum, 42161 = Arbitrum, 8453 = Base\n        -- Cast via NUMERIC first to handle decimal strings like \"8453.0\" from ETL\n        (topic_origin_chain_id::NUMERIC)::BIGINT AS origin_chain_id,\n        \n        -- Deposit ID: Unique identifier linking this fill to its original deposit\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\n        -- This is the KEY that connects deposits \u2194 fills across chains\n        -- Cast via NUMERIC first to handle decimal strings like \"12345.0\" from ETL\n        (topic_deposit_id::NUMERIC)::BIGINT AS deposit_id,\n        \n        -- Relayer address: Who provided the liquidity\n        -- Already decoded by ETL to proper address format (0x...)\n        topic_relayer AS relayer_address,\n        \n        -- ============================================================\n        -- TOKEN INFORMATION (what was bridged)\n        -- ============================================================\n        \n        -- Input token: The token address on the origin chain\n        -- Already decoded by ETL to proper address format\n        -- Example: 0xA0b86991c6218b36c1d19D4a2e9Eb0cE3606eB48 = USDC on Ethereum\n        -- Normalize to lowercase for joining with token metadata\n        LOWER(filled_relay_data_input_token) AS input_token_address,\n        \n        -- Output token: The token address on the destination chain (Arbitrum in this case)\n        -- Already decoded by ETL to proper address format\n        -- Normalize to lowercase for joining with token metadata\n        LOWER(filled_relay_data_output_token) AS output_token_address,\n        \n        -- ============================================================\n        -- AMOUNT INFORMATION (how much was bridged)\n        -- ============================================================\n        \n        -- Input amount: How much was sent from origin chain\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\n        -- Store as RAW amount first (before rescaling by decimals)\n        filled_relay_data_input_amount::NUMERIC AS input_amount_raw,\n        \n        -- Output amount: How much was received on destination chain\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\n        -- Usually slightly less than input due to fees/spread\n        -- Store as RAW amount first (before rescaling by decimals)\n        filled_relay_data_output_amount::NUMERIC AS output_amount_raw,\n        \n        -- ============================================================\n        -- RELAYER & ROUTING INFORMATION\n        -- ============================================================\n        \n        -- Repayment chain ID: Where the relayer gets reimbursed\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\n        -- Relayers front capital, then get paid back (often on a different chain)\n        -- Cast via NUMERIC first to handle decimal strings like \"1.0\" from ETL\n        (filled_relay_data_repayment_chain_id::NUMERIC)::BIGINT AS repayment_chain_id,\n        \n        -- Exclusive relayer: If set, only this address can fill this deposit\n        -- Already decoded by ETL to proper address format\n        -- Used for priority/guaranteed fills (NULL if no exclusive relayer)\n        filled_relay_data_exclusive_relayer AS exclusive_relayer_address,\n        \n        -- ============================================================\n        -- USER INFORMATION\n        -- ============================================================\n        \n        -- Depositor: The original user who initiated the bridge\n        -- Already decoded by ETL to proper address format\n        filled_relay_data_depositor AS depositor_address,\n        \n        -- Recipient: Who receives the funds on the destination chain\n        -- Already decoded by ETL to proper address format\n        -- Usually the same as depositor, but can be different (gift/transfer)\n        filled_relay_data_recipient AS recipient_address,\n        \n        -- ============================================================\n        -- GAS DATA (for relayer cost analysis)\n        -- ============================================================\n        gas_price_wei::BIGINT AS gas_price_wei,\n        gas_used::BIGINT AS gas_used\n        \n    FROM raw_fills\n)\n\n-- ============================================================\n-- FINAL SELECT: Join with token metadata and rescale amounts\n-- ============================================================\nSELECT\n    -- Event identity\n    c.fill_timestamp,\n    c.transaction_hash,\n    c.blockchain,\n    c.source_file,\n    \n    -- Indexed fields\n    c.origin_chain_id,\n    c.deposit_id,\n    c.relayer_address,\n    \n    -- Token info WITH NAMES\n    -- Input token (what was deposited on origin chain)\n    c.input_token_address,\n    input_tok.token_symbol AS input_token_symbol,        -- e.g., 'USDC', 'WETH'\n    \n    -- Output token (what was received on destination chain - Arbitrum)\n    c.output_token_address,\n    output_tok.token_symbol AS output_token_symbol,      -- e.g., 'USDC', 'WETH'\n    \n    -- Rescaled amounts (human-readable)\n    -- These use the rescale_amount macro which divides raw amount by 10^decimals\n    -- Example: 5000000 raw USDC (6 decimals) \u2192 5.0 USDC\n    \n    c.input_amount_raw / POWER(10, COALESCE(input_tok.decimals, 18))\n AS input_amount,\n    \n    c.output_amount_raw / POWER(10, COALESCE(output_tok.decimals, 18))\n AS output_amount,\n    \n    -- Raw amounts (preserved for auditing)\n    -- These are the original blockchain values before rescaling\n    -- Example: 5.0 USDC is stored as 5000000 on blockchain\n    c.input_amount_raw,\n    c.output_amount_raw,\n    \n    -- Relayer routing\n    c.repayment_chain_id,\n    c.exclusive_relayer_address,\n    \n    -- User info\n    c.depositor_address,\n    c.recipient_address,\n    \n    -- Gas data (for relayer cost analysis)\n    c.gas_price_wei,\n    c.gas_used,\n    (c.gas_price_wei * c.gas_used) AS gas_cost_wei\n    \nFROM cleaned_fills c\n\n-- Join with token metadata to get decimals and symbols for INPUT token\n-- Join on BOTH address AND chain_id to match the correct origin chain\nLEFT JOIN input_token_meta AS input_tok\n    ON c.input_token_address = input_tok.token_address\n    AND c.origin_chain_id = input_tok.chain_id\n\n-- Join with token metadata to get decimals and symbols for OUTPUT token\n-- Join on address only - we already filtered to destination chain (Arbitrum)\nLEFT JOIN output_token_meta AS output_tok\n    ON c.output_token_address = output_tok.token_address\n\n-- Data quality: Only include rows with essential fields populated\nWHERE c.deposit_id IS NOT NULL\n    AND c.relayer_address IS NOT NULL\n    AND c.input_token_address IS NOT NULL\n    AND c.output_token_address IS NOT NULL\n    AND c.input_amount_raw IS NOT NULL\n    AND c.output_amount_raw IS NOT NULL", "relation_name": "\"across_analytics\".\"dbt_staging\".\"stg_arbitrum__fills\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2025-12-30T08:45:59.586032Z", "completed_at": "2025-12-30T08:45:59.614679Z"}, {"name": "execute", "started_at": "2025-12-30T08:45:59.618508Z", "completed_at": "2025-12-30T08:45:59.884797Z"}], "thread_id": "Thread-1 (worker)", "execution_time": 0.30926513671875, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.stg_ethereum__fills", "compiled": true, "compiled_code": "-- Staging model for FilledRelay events from Ethereum\n-- This model extracts fill events where relayers complete cross-chain bridge transactions\n\nWITH raw_fills AS (\n\n    SELECT\n        -- Core event metadata - these identify WHEN and WHERE the event happened\n        timestamp_datetime,              -- When the event occurred (from blockchain)\n        transactionHash,                 -- Unique transaction identifier\n        blockchain,                      -- Which blockchain (should be 'ethereum')\n        source_file,                     -- Which source file this came from (for lineage)\n        \n        -- Indexed fields from topics (these are searchable/filterable in blockchain)\n        -- Topics are like \"indexed columns\" in SQL - they're stored separately for fast queries\n        topic_origin_chain_id,           -- Chain ID where the original deposit happened\n        topic_deposit_id,                -- Unique deposit identifier (links to deposit event)\n        topic_relayer,                   -- Address of the relayer who provided liquidity\n        \n        -- Non-indexed fields from the event's data field\n        -- These are the actual business data about the fill\n        filled_relay_data_input_token,          -- Token address on origin chain\n        filled_relay_data_output_token,          -- Token address on destination chain\n        filled_relay_data_input_amount,          -- Amount sent (in origin token units)\n        filled_relay_data_output_amount,         -- Amount received (in destination token units)\n        filled_relay_data_repayment_chain_id,     -- Where relayer gets reimbursed\n        filled_relay_data_exclusive_relayer,     -- Address with exclusive fill rights (if any)\n        filled_relay_data_depositor,             -- Original user who initiated the bridge\n        filled_relay_data_recipient,              -- Final recipient of the bridged funds\n        \n        -- Gas data (for relayer cost analysis)\n        gas_price_wei,\n        gas_used\n        \n    FROM raw.ethereum_logs_processed\n    \n    -- Filter: Only include rows where FilledRelay data exists\n    WHERE topic_0 = '0x44b559f101f8fbcc8a0ea43fa91a05a729a5ea6e14a7c75aa750374690137208'\n        AND filled_relay_data_input_amount IS NOT NULL\n        AND filled_relay_data_output_amount IS NOT NULL\n),\n\n-- INPUT token metadata: includes chain_id for matching with origin_chain_id\n-- This allows us to find the correct token on whichever chain the deposit came from\ninput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals,\n        chain_id\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n\n),\n\n-- OUTPUT token metadata: filtered to destination chain (Ethereum)\n-- This is correct because on fills, output_token is always on the destination chain\noutput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n    WHERE chain = 'ethereum'\n\n),\n\ncleaned_fills AS (\n    SELECT\n        \n        -- Timestamp: Convert text to proper timestamp type\n        \n        CASE \n            WHEN timestamp_datetime ~ '^\\d+$' THEN          -- If it's pure digits, treat as Unix timestamp (seconds since 1970-01-01)\n                TO_TIMESTAMP(timestamp_datetime::BIGINT)    -- PostgreSQL's TO_TIMESTAMP can parse various formats, but we use the simplest approach\n            ELSE \n                timestamp_datetime::TIMESTAMP               -- Otherwise, try to parse as ISO format string\n        END AS fill_timestamp,\n        \n        transactionHash AS transaction_hash,\n        blockchain,\n        source_file,\n        \n        -- ============================================================\n        -- INDEXED FIELDS (from topics) - Already decoded by ETL\n        -- ============================================================\n        -- These come from the event's \"topics\" array, which is indexed for fast blockchain queries\n        \n        -- Origin chain ID: Which blockchain the funds came FROM\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\n        -- Example: 1 = Ethereum, 42161 = Arbitrum, 8453 = Base\n        -- Cast via NUMERIC first to handle decimal strings like \"8453.0\" from ETL\n        (topic_origin_chain_id::NUMERIC)::BIGINT AS origin_chain_id,\n        \n        -- Deposit ID: Unique identifier linking this fill to its original deposit\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\n        -- This is the KEY that connects deposits \u2194 fills across chains\n        -- Cast via NUMERIC first to handle decimal strings like \"12345.0\" from ETL\n        (topic_deposit_id::NUMERIC)::BIGINT AS deposit_id,\n        \n        -- Relayer address: Who provided the liquidity\n        -- Already decoded by ETL to proper address format (0x...)\n        topic_relayer AS relayer_address,\n        \n        -- ============================================================\n        -- TOKEN INFORMATION (what was bridged)\n        -- ============================================================\n        \n        -- Input token: The token address on the origin chain\n        -- Already decoded by ETL to proper address format\n        -- Example: 0xA0b86991c6218b36c1d19D4a2e9Eb0cE3606eB48 = USDC on Ethereum\n        -- Normalize to lowercase for joining with token metadata\n        LOWER(filled_relay_data_input_token) AS input_token_address,\n        \n        -- Output token: The token address on the destination chain (Ethereum in this case)\n        -- Already decoded by ETL to proper address format\n        -- Normalize to lowercase for joining with token metadata\n        LOWER(filled_relay_data_output_token) AS output_token_address,\n        \n        -- ============================================================\n        -- AMOUNT INFORMATION (how much was bridged)\n        -- ============================================================\n        \n        -- Input amount: How much was sent from origin chain\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\n        -- Store as RAW amount first (before rescaling by decimals)\n        filled_relay_data_input_amount::NUMERIC AS input_amount_raw,\n        \n        -- Output amount: How much was received on destination chain\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\n        -- Usually slightly less than input due to fees/spread\n        -- Store as RAW amount first (before rescaling by decimals)\n        filled_relay_data_output_amount::NUMERIC AS output_amount_raw,\n        \n        -- ============================================================\n        -- RELAYER & ROUTING INFORMATION\n        -- ============================================================\n        \n        -- Repayment chain ID: Where the relayer gets reimbursed\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\n        -- Relayers front capital, then get paid back (often on a different chain)\n        -- Cast via NUMERIC first to handle decimal strings like \"1.0\" from ETL\n        (filled_relay_data_repayment_chain_id::NUMERIC)::BIGINT AS repayment_chain_id,\n        \n        -- Exclusive relayer: If set, only this address can fill this deposit\n        -- Already decoded by ETL to proper address format\n        -- Used for priority/guaranteed fills (NULL if no exclusive relayer)\n        filled_relay_data_exclusive_relayer AS exclusive_relayer_address,\n        \n        -- ============================================================\n        -- USER INFORMATION\n        -- ============================================================\n        \n        -- Depositor: The original user who initiated the bridge\n        -- Already decoded by ETL to proper address format\n        filled_relay_data_depositor AS depositor_address,\n        \n        -- Recipient: Who receives the funds on the destination chain\n        -- Already decoded by ETL to proper address format\n        -- Usually the same as depositor, but can be different (gift/transfer)\n        filled_relay_data_recipient AS recipient_address,\n        \n        -- ============================================================\n        -- GAS DATA (for relayer cost analysis)\n        -- ============================================================\n        gas_price_wei::BIGINT AS gas_price_wei,\n        gas_used::BIGINT AS gas_used\n        \n    FROM raw_fills\n)\n\n-- ============================================================\n-- FINAL SELECT: Join with token metadata and rescale amounts\n-- ============================================================\nSELECT\n    -- Event identity\n    c.fill_timestamp,\n    c.transaction_hash,\n    c.blockchain,\n    c.source_file,\n    \n    -- Indexed fields\n    c.origin_chain_id,\n    c.deposit_id,\n    c.relayer_address,\n    \n    -- Token info WITH NAMES\n    -- Input token (what was deposited on origin chain)\n    c.input_token_address,\n    input_tok.token_symbol AS input_token_symbol,        -- e.g., 'USDC', 'WETH'\n    \n    -- Output token (what was received on destination chain - Ethereum)\n    c.output_token_address,\n    output_tok.token_symbol AS output_token_symbol,      -- e.g., 'USDC', 'WETH'\n    \n    -- Rescaled amounts (human-readable)\n    -- These use the rescale_amount macro which divides raw amount by 10^decimals\n    -- Example: 5000000 raw USDC (6 decimals) \u2192 5.0 USDC\n    \n    c.input_amount_raw / POWER(10, COALESCE(input_tok.decimals, 18))\n AS input_amount,\n    \n    c.output_amount_raw / POWER(10, COALESCE(output_tok.decimals, 18))\n AS output_amount,\n    \n    -- Raw amounts (preserved for auditing)\n    -- These are the original blockchain values before rescaling\n    -- Example: 5.0 USDC is stored as 5000000 on blockchain\n    c.input_amount_raw,\n    c.output_amount_raw,\n    \n    -- Relayer routing\n    c.repayment_chain_id,\n    c.exclusive_relayer_address,\n    \n    -- User info\n    c.depositor_address,\n    c.recipient_address,\n    \n    -- Gas data (for relayer cost analysis)\n    c.gas_price_wei,\n    c.gas_used,\n    (c.gas_price_wei * c.gas_used) AS gas_cost_wei\n    \nFROM cleaned_fills c\n\n-- Join with token metadata to get decimals and symbols for INPUT token\n-- Join on BOTH address AND chain_id to match the correct origin chain\nLEFT JOIN input_token_meta AS input_tok\n    ON c.input_token_address = input_tok.token_address\n    AND c.origin_chain_id = input_tok.chain_id\n\n-- Join with token metadata to get decimals and symbols for OUTPUT token\n-- Join on address only - we already filtered to destination chain (Ethereum)\nLEFT JOIN output_token_meta AS output_tok\n    ON c.output_token_address = output_tok.token_address\n\n-- Data quality: Only include rows with essential fields populated\nWHERE c.deposit_id IS NOT NULL\n    AND c.relayer_address IS NOT NULL\n    AND c.input_token_address IS NOT NULL\n    AND c.output_token_address IS NOT NULL\n    AND c.input_amount_raw IS NOT NULL\n    AND c.output_amount_raw IS NOT NULL", "relation_name": "\"across_analytics\".\"dbt_staging\".\"stg_ethereum__fills\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2025-12-30T08:45:59.628124Z", "completed_at": "2025-12-30T08:45:59.652612Z"}, {"name": "execute", "started_at": "2025-12-30T08:45:59.659055Z", "completed_at": "2025-12-30T08:46:00.002084Z"}], "thread_id": "Thread-4 (worker)", "execution_time": 0.3968472480773926, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.stg_ethereum__refunds", "compiled": true, "compiled_code": "-- Staging model for ExecutedRelayerRefundRoot events from Ethereum\r\n\r\nWITH raw_refunds AS (\r\n\r\n    SELECT\r\n        -- Core event metadata - these identify WHEN and WHERE the event happened\r\n        timestamp_datetime,              -- When the event occurred (from blockchain)\r\n        transactionHash,                 -- Unique transaction identifier\r\n        blockchain,                      -- Which blockchain (should be 'ethereum')\r\n        source_file,                     -- Which source file this came from (for lineage)\r\n        \r\n        -- Indexed fields from topics (these are searchable/filterable in blockchain)\r\n        -- Topics are like \"indexed columns\" in SQL - they're stored separately for fast queries\r\n        topic_chain_id,                  -- Chain ID where the refund was executed\r\n        topic_root_bundle_id,            -- Merkle root bundle identifier (groups multiple refunds)\r\n        topic_leaf_id,                   -- Leaf index in the merkle tree (identifies this specific refund batch)\r\n        \r\n        -- Non-indexed fields from the event's data field\r\n        -- These are the actual business data about the refund\r\n        amount_to_return,                -- Total amount of capital returned in this batch\r\n        l2_token_address,                -- Token address being refunded (on Ethereum L1)\r\n        refund_amounts,                  -- Comma-separated list of individual refund amounts\r\n        refund_addresses,                -- Comma-separated list of relayer addresses receiving refunds\r\n        refund_count                     -- Number of relayers being refunded in this batch\r\n        \r\n    FROM raw.ethereum_logs_processed\r\n    \r\n    -- Filter: Only include rows where ExecutedRelayerRefundRoot data exists\r\n    -- Topic_0 is the event signature hash that identifies the event type\r\n    WHERE topic_0 = '0xf4ad92585b1bc117fbdd644990adf0827bc4c95baeae8a23322af807b6d0020e'\r\n        AND amount_to_return IS NOT NULL\r\n        AND l2_token_address IS NOT NULL\r\n        AND refund_count IS NOT NULL\r\n),\r\n\r\ncleaned_refunds AS (\r\n    SELECT\r\n        \r\n        -- Timestamp: Convert text to proper timestamp type\r\n        CASE \r\n            WHEN timestamp_datetime ~ '^\\d+$' THEN          -- If it's pure digits, treat as Unix timestamp (seconds since 1970-01-01)\r\n                TO_TIMESTAMP(timestamp_datetime::BIGINT)    -- PostgreSQL's TO_TIMESTAMP converts Unix seconds to timestamp\r\n            ELSE \r\n                timestamp_datetime::TIMESTAMP               -- Otherwise, try to parse as ISO format string (e.g., \"2025-12-03 10:30:00\")\r\n        END AS refund_timestamp,\r\n        \r\n        transactionHash AS transaction_hash,\r\n        blockchain,\r\n        source_file,\r\n        \r\n        -- ============================================================\r\n        -- INDEXED FIELDS (from topics) - Already decoded by ETL\r\n        -- ============================================================\r\n        -- These come from the event's \"topics\" array, which is indexed for fast blockchain queries\r\n        \r\n        -- Chain ID: Which blockchain this refund was executed on\r\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\r\n        -- Example: 42161 = Arbitrum, 1 = Ethereum, 8453 = Base\r\n        -- Cast via NUMERIC first to handle decimal strings like \"42161.0\" from ETL\r\n        (topic_chain_id::NUMERIC)::BIGINT AS chain_id,\r\n        \r\n        -- Root bundle ID: Groups multiple refunds together in a merkle tree\r\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\r\n        -- Multiple refund batches can be grouped into one merkle root for gas efficiency\r\n        -- Cast via NUMERIC first to handle decimal strings like \"12345.0\" from ETL\r\n        (topic_root_bundle_id::NUMERIC)::BIGINT AS root_bundle_id,\r\n        \r\n        -- Leaf ID: Identifies this specific refund batch within the merkle tree\r\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\r\n        -- Each leaf represents one batch of refunds to multiple relayers\r\n        -- Cast via NUMERIC first to handle decimal strings like \"789.0\" from ETL\r\n        (topic_leaf_id::NUMERIC)::BIGINT AS leaf_id,\r\n        \r\n        -- ============================================================\r\n        -- TOKEN INFORMATION (what was refunded)\r\n        -- ============================================================\r\n        \r\n        -- L2 token address: The token being refunded on Ethereum\r\n        -- Already decoded by ETL to proper address format (0x...)\r\n        -- Example: 0xA0b86991c6218b36c1d19D4a2e9Eb0cE3606eB48 = USDC on Ethereum\r\n        l2_token_address AS refund_token_address,\r\n        \r\n        -- ============================================================\r\n        -- AMOUNT INFORMATION (how much was refunded)\r\n        -- ============================================================\r\n        \r\n        -- Amount to return: Total capital returned in this batch\r\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\r\n        -- This is the sum of all individual refund amounts in the batch\r\n        -- Stored as NUMERIC to handle large token amounts (18 decimals)\r\n        amount_to_return::NUMERIC AS total_refund_amount,\r\n        \r\n        -- Refund amounts: Individual refund amounts as comma-separated string\r\n        -- Format: \"1000000000000000000,2000000000000000000,3000000000000000000\"\r\n        -- Each value is in token's base units (e.g., wei for ETH, 6 decimals for USDC)\r\n        -- This is a comma-separated string because the ETL processes dynamic arrays\r\n        refund_amounts AS refund_amounts_string,\r\n        \r\n        -- Refund addresses: Relayer addresses receiving refunds as comma-separated string\r\n        -- Format: \"0xAAA...,0xBBB...,0xCCC...\"\r\n        -- Matches 1:1 with refund_amounts (first address gets first amount, etc.)\r\n        -- This is a comma-separated string because the ETL processes dynamic arrays\r\n        refund_addresses AS refund_addresses_string,\r\n        \r\n        -- Refund count: Number of relayers being refunded in this batch\r\n        -- Already converted by ETL to integer\r\n        -- This tells us how many individual refunds are in the arrays above\r\n        refund_count::INTEGER AS refund_count\r\n        \r\n    FROM raw_refunds\r\n)\r\n\r\n-- Final SELECT: Add any computed fields and ensure data quality\r\nSELECT\r\n    -- Event identity\r\n    refund_timestamp,\r\n    transaction_hash,\r\n    blockchain,\r\n    source_file,\r\n    \r\n    -- Indexed fields (merkle tree identifiers)\r\n    chain_id,\r\n    root_bundle_id,\r\n    leaf_id,\r\n    \r\n    -- Token info\r\n    refund_token_address,\r\n    \r\n    -- Amounts\r\n    total_refund_amount,\r\n    refund_amounts_string,\r\n    refund_addresses_string,\r\n    refund_count\r\n    \r\nFROM cleaned_refunds\r\n\r\n-- Data quality: Only include rows with essential fields populated\r\nWHERE chain_id IS NOT NULL\r\n    AND refund_token_address IS NOT NULL\r\n    AND total_refund_amount IS NOT NULL\r\n    AND refund_count IS NOT NULL", "relation_name": "\"across_analytics\".\"dbt_staging\".\"stg_ethereum__refunds\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2025-12-30T08:45:59.641356Z", "completed_at": "2025-12-30T08:45:59.669954Z"}, {"name": "execute", "started_at": "2025-12-30T08:45:59.681704Z", "completed_at": "2025-12-30T08:46:00.031265Z"}], "thread_id": "Thread-3 (worker)", "execution_time": 0.40732288360595703, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.stg_hyperevm__deposits", "compiled": true, "compiled_code": "-- Staging model for FundsDeposited events from HyperEVM\n-- This model extracts deposit events where users initiate cross-chain bridge transactions\n\nWITH raw_deposits AS (\n\n    SELECT\n        timestamp_datetime,\n        transactionHash,\n        blockchain,\n        source_file,\n        topic_destination_chain_id,\n        topic_deposit_id,\n        topic_depositor,\n        funds_deposited_data_input_token,\n        funds_deposited_data_output_token,\n        funds_deposited_data_input_amount,\n        funds_deposited_data_output_amount,\n        funds_deposited_data_recipient\n        \n    FROM raw.hyperevm_logs_processed\n    \n    WHERE topic_0 = '0x32ed1a409ef04c7b0227189c3a103dc5ac10e775a15b785dcc510201f7c25ad3'\n        AND funds_deposited_data_input_amount IS NOT NULL\n        AND funds_deposited_data_output_amount IS NOT NULL\n),\n\n-- INPUT token metadata: filtered to origin chain (HyperEVM)\n-- Note: hyperevm may not have tokens in token_metadata yet\ninput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n    WHERE chain = 'hyperevm'\n\n),\n\n-- OUTPUT token metadata: includes chain_id for matching with destination_chain_id\noutput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals,\n        chain_id\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n\n),\n\ncleaned_deposits AS (\n    SELECT\n        CASE \n            WHEN timestamp_datetime ~ '^\\d+$' THEN\n                TO_TIMESTAMP(timestamp_datetime::BIGINT)\n            ELSE \n                timestamp_datetime::TIMESTAMP\n        END AS deposit_timestamp,\n        \n        transactionHash AS transaction_hash,\n        blockchain,\n        source_file,\n        (topic_destination_chain_id::NUMERIC)::BIGINT AS destination_chain_id,\n        (topic_deposit_id::NUMERIC)::BIGINT AS deposit_id,\n        topic_depositor AS depositor_address,\n        LOWER(funds_deposited_data_input_token) AS input_token_address,\n        LOWER(funds_deposited_data_output_token) AS output_token_address,\n        funds_deposited_data_input_amount::NUMERIC AS input_amount_raw,\n        funds_deposited_data_output_amount::NUMERIC AS output_amount_raw,\n        funds_deposited_data_recipient AS recipient_address\n        \n    FROM raw_deposits\n)\n\nSELECT\n    c.deposit_timestamp,\n    c.transaction_hash,\n    c.blockchain,\n    c.source_file,\n    c.destination_chain_id,\n    c.deposit_id,\n    c.depositor_address,\n    c.input_token_address,\n    input_tok.token_symbol AS input_token_symbol,\n    c.output_token_address,\n    output_tok.token_symbol AS output_token_symbol,\n    \n    c.input_amount_raw / POWER(10, COALESCE(input_tok.decimals, 18))\n AS input_amount,\n    \n    CASE \n        WHEN c.output_token_address = '0x0000000000000000000000000000000000000000' \n        THEN c.output_amount_raw / POWER(10, COALESCE(input_tok.decimals, 18))\n        ELSE c.output_amount_raw / POWER(10, COALESCE(output_tok.decimals, 18))\n    END\n AS output_amount,\n    c.input_amount_raw,\n    c.output_amount_raw,\n    c.recipient_address\n    \nFROM cleaned_deposits c\n\nLEFT JOIN input_token_meta AS input_tok\n    ON c.input_token_address = input_tok.token_address\n\nLEFT JOIN output_token_meta AS output_tok\n    ON c.output_token_address = output_tok.token_address\n    AND c.destination_chain_id = output_tok.chain_id\n\nWHERE c.deposit_id IS NOT NULL\n    AND c.depositor_address IS NOT NULL\n    AND c.input_token_address IS NOT NULL\n    AND c.output_token_address IS NOT NULL\n    AND c.input_amount_raw IS NOT NULL\n    AND c.output_amount_raw IS NOT NULL", "relation_name": "\"across_analytics\".\"dbt_staging\".\"stg_hyperevm__deposits\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2025-12-30T08:45:59.670686Z", "completed_at": "2025-12-30T08:45:59.688747Z"}, {"name": "execute", "started_at": "2025-12-30T08:45:59.692504Z", "completed_at": "2025-12-30T08:46:00.062968Z"}], "thread_id": "Thread-2 (worker)", "execution_time": 0.41209840774536133, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.stg_hyperevm__fills", "compiled": true, "compiled_code": "-- Staging model for FilledRelay events from HyperEVM\n\nWITH raw_fills AS (\n    SELECT\n        timestamp_datetime, transactionHash, blockchain, source_file,\n        topic_origin_chain_id, topic_deposit_id, topic_relayer,\n        filled_relay_data_input_token, filled_relay_data_output_token,\n        filled_relay_data_input_amount, filled_relay_data_output_amount,\n        filled_relay_data_repayment_chain_id, filled_relay_data_exclusive_relayer,\n        filled_relay_data_depositor, filled_relay_data_recipient,\n        gas_price_wei, gas_used\n    FROM raw.hyperevm_logs_processed\n    WHERE topic_0 = '0x44b559f101f8fbcc8a0ea43fa91a05a729a5ea6e14a7c75aa750374690137208'\n        AND filled_relay_data_input_amount IS NOT NULL\n        AND filled_relay_data_output_amount IS NOT NULL\n),\n\ninput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals,\n        chain_id\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n\n),\n\noutput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n    WHERE chain = 'hyperevm'\n\n),\n\ncleaned_fills AS (\n    SELECT\n        CASE \n            WHEN timestamp_datetime ~ '^\\d+$' THEN TO_TIMESTAMP(timestamp_datetime::BIGINT)\n            ELSE timestamp_datetime::TIMESTAMP\n        END AS fill_timestamp,\n        transactionHash AS transaction_hash,\n        blockchain, source_file,\n        (topic_origin_chain_id::NUMERIC)::BIGINT AS origin_chain_id,\n        (topic_deposit_id::NUMERIC)::BIGINT AS deposit_id,\n        topic_relayer AS relayer_address,\n        LOWER(filled_relay_data_input_token) AS input_token_address,\n        LOWER(filled_relay_data_output_token) AS output_token_address,\n        filled_relay_data_input_amount::NUMERIC AS input_amount_raw,\n        filled_relay_data_output_amount::NUMERIC AS output_amount_raw,\n        (filled_relay_data_repayment_chain_id::NUMERIC)::BIGINT AS repayment_chain_id,\n        filled_relay_data_exclusive_relayer AS exclusive_relayer_address,\n        filled_relay_data_depositor AS depositor_address,\n        filled_relay_data_recipient AS recipient_address,\n        gas_price_wei::BIGINT AS gas_price_wei,\n        gas_used::BIGINT AS gas_used\n    FROM raw_fills\n)\n\nSELECT\n    c.fill_timestamp, c.transaction_hash, c.blockchain, c.source_file,\n    c.origin_chain_id, c.deposit_id, c.relayer_address,\n    c.input_token_address, input_tok.token_symbol AS input_token_symbol,\n    c.output_token_address, output_tok.token_symbol AS output_token_symbol,\n    \n    c.input_amount_raw / POWER(10, COALESCE(input_tok.decimals, 18))\n AS input_amount,\n    \n    c.output_amount_raw / POWER(10, COALESCE(output_tok.decimals, 18))\n AS output_amount,\n    c.input_amount_raw, c.output_amount_raw,\n    c.repayment_chain_id, c.exclusive_relayer_address,\n    c.depositor_address, c.recipient_address,\n    c.gas_price_wei, c.gas_used, (c.gas_price_wei * c.gas_used) AS gas_cost_wei\nFROM cleaned_fills c\nLEFT JOIN input_token_meta AS input_tok\n    ON c.input_token_address = input_tok.token_address AND c.origin_chain_id = input_tok.chain_id\nLEFT JOIN output_token_meta AS output_tok\n    ON c.output_token_address = output_tok.token_address\nWHERE c.deposit_id IS NOT NULL AND c.relayer_address IS NOT NULL\n    AND c.input_token_address IS NOT NULL AND c.output_token_address IS NOT NULL\n    AND c.input_amount_raw IS NOT NULL AND c.output_amount_raw IS NOT NULL", "relation_name": "\"across_analytics\".\"dbt_staging\".\"stg_hyperevm__fills\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2025-12-30T08:45:59.981438Z", "completed_at": "2025-12-30T08:46:00.003136Z"}, {"name": "execute", "started_at": "2025-12-30T08:46:00.015249Z", "completed_at": "2025-12-30T08:46:00.368113Z"}], "thread_id": "Thread-1 (worker)", "execution_time": 0.4131464958190918, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.stg_hyperevm__refunds", "compiled": true, "compiled_code": "-- Staging model for ExecutedRelayerRefundRoot events from HyperEVM\r\n\r\nWITH raw_refunds AS (\r\n\r\n    SELECT\r\n        -- Core event metadata - these identify WHEN and WHERE the event happened\r\n        timestamp_datetime,              -- When the event occurred (from blockchain)\r\n        transactionHash,                 -- Unique transaction identifier\r\n        blockchain,                      -- Which blockchain (should be 'hyperevm')\r\n        source_file,                     -- Which source file this came from (for lineage)\r\n        \r\n        -- Indexed fields from topics (these are searchable/filterable in blockchain)\r\n        -- Topics are like \"indexed columns\" in SQL - they're stored separately for fast queries\r\n        topic_chain_id,                  -- Chain ID where the refund was executed\r\n        topic_root_bundle_id,            -- Merkle root bundle identifier (groups multiple refunds)\r\n        topic_leaf_id,                   -- Leaf index in the merkle tree (identifies this specific refund batch)\r\n        \r\n        -- Non-indexed fields from the event's data field\r\n        -- These are the actual business data about the refund\r\n        amount_to_return,                -- Total amount of capital returned in this batch\r\n        l2_token_address,                -- Token address being refunded (on HyperEVM)\r\n        refund_amounts,                  -- Comma-separated list of individual refund amounts\r\n        refund_addresses,                -- Comma-separated list of relayer addresses receiving refunds\r\n        refund_count                     -- Number of relayers being refunded in this batch\r\n        \r\n    FROM raw.hyperevm_logs_processed\r\n    \r\n    -- Filter: Only include rows where ExecutedRelayerRefundRoot data exists\r\n    -- Topic_0 is the event signature hash that identifies the event type\r\n    WHERE topic_0 = '0xf4ad92585b1bc117fbdd644990adf0827bc4c95baeae8a23322af807b6d0020e'\r\n        AND amount_to_return IS NOT NULL\r\n        AND l2_token_address IS NOT NULL\r\n        AND refund_count IS NOT NULL\r\n),\r\n\r\ncleaned_refunds AS (\r\n    SELECT\r\n        \r\n        -- Timestamp: Convert text to proper timestamp type\r\n        CASE \r\n            WHEN timestamp_datetime ~ '^\\d+$' THEN          -- If it's pure digits, treat as Unix timestamp (seconds since 1970-01-01)\r\n                TO_TIMESTAMP(timestamp_datetime::BIGINT)    -- PostgreSQL's TO_TIMESTAMP converts Unix seconds to timestamp\r\n            ELSE \r\n                timestamp_datetime::TIMESTAMP               -- Otherwise, try to parse as ISO format string (e.g., \"2025-12-03 10:30:00\")\r\n        END AS refund_timestamp,\r\n        \r\n        transactionHash AS transaction_hash,\r\n        blockchain,\r\n        source_file,\r\n        \r\n        -- ============================================================\r\n        -- INDEXED FIELDS (from topics) - Already decoded by ETL\r\n        -- ============================================================\r\n        -- These come from the event's \"topics\" array, which is indexed for fast blockchain queries\r\n        \r\n        -- Chain ID: Which blockchain this refund was executed on\r\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\r\n        -- Example: 42161 = Arbitrum, 1 = Ethereum, 999 = HyperEVM\r\n        -- Cast via NUMERIC first to handle decimal strings like \"42161.0\" from ETL\r\n        (topic_chain_id::NUMERIC)::BIGINT AS chain_id,\r\n        \r\n        -- Root bundle ID: Groups multiple refunds together in a merkle tree\r\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\r\n        -- Multiple refund batches can be grouped into one merkle root for gas efficiency\r\n        -- Cast via NUMERIC first to handle decimal strings like \"12345.0\" from ETL\r\n        (topic_root_bundle_id::NUMERIC)::BIGINT AS root_bundle_id,\r\n        \r\n        -- Leaf ID: Identifies this specific refund batch within the merkle tree\r\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\r\n        -- Each leaf represents one batch of refunds to multiple relayers\r\n        -- Cast via NUMERIC first to handle decimal strings like \"789.0\" from ETL\r\n        (topic_leaf_id::NUMERIC)::BIGINT AS leaf_id,\r\n        \r\n        -- ============================================================\r\n        -- TOKEN INFORMATION (what was refunded)\r\n        -- ============================================================\r\n        \r\n        -- L2 token address: The token being refunded on HyperEVM\r\n        -- Already decoded by ETL to proper address format (0x...)\r\n        -- This is the token address on the HyperEVM chain\r\n        l2_token_address AS refund_token_address,\r\n        \r\n        -- ============================================================\r\n        -- AMOUNT INFORMATION (how much was refunded)\r\n        -- ============================================================\r\n        \r\n        -- Amount to return: Total capital returned in this batch\r\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\r\n        -- This is the sum of all individual refund amounts in the batch\r\n        -- Stored as NUMERIC to handle large token amounts (18 decimals)\r\n        amount_to_return::NUMERIC AS total_refund_amount,\r\n        \r\n        -- Refund amounts: Individual refund amounts as comma-separated string\r\n        -- Format: \"1000000000000000000,2000000000000000000,3000000000000000000\"\r\n        -- Each value is in token's base units (e.g., wei for ETH, 6 decimals for USDC)\r\n        -- This is a comma-separated string because the ETL processes dynamic arrays\r\n        refund_amounts AS refund_amounts_string,\r\n        \r\n        -- Refund addresses: Relayer addresses receiving refunds as comma-separated string\r\n        -- Format: \"0xAAA...,0xBBB...,0xCCC...\"\r\n        -- Matches 1:1 with refund_amounts (first address gets first amount, etc.)\r\n        -- This is a comma-separated string because the ETL processes dynamic arrays\r\n        refund_addresses AS refund_addresses_string,\r\n        \r\n        -- Refund count: Number of relayers being refunded in this batch\r\n        -- Already converted by ETL to integer\r\n        -- This tells us how many individual refunds are in the arrays above\r\n        refund_count::INTEGER AS refund_count\r\n        \r\n    FROM raw_refunds\r\n)\r\n\r\n-- Final SELECT: Add any computed fields and ensure data quality\r\nSELECT\r\n    -- Event identity\r\n    refund_timestamp,\r\n    transaction_hash,\r\n    blockchain,\r\n    source_file,\r\n    \r\n    -- Indexed fields (merkle tree identifiers)\r\n    chain_id,\r\n    root_bundle_id,\r\n    leaf_id,\r\n    \r\n    -- Token info\r\n    refund_token_address,\r\n    \r\n    -- Amounts\r\n    total_refund_amount,\r\n    refund_amounts_string,\r\n    refund_addresses_string,\r\n    refund_count\r\n    \r\nFROM cleaned_refunds\r\n\r\n-- Data quality: Only include rows with essential fields populated\r\nWHERE chain_id IS NOT NULL\r\n    AND refund_token_address IS NOT NULL\r\n    AND total_refund_amount IS NOT NULL\r\n    AND refund_count IS NOT NULL", "relation_name": "\"across_analytics\".\"dbt_staging\".\"stg_hyperevm__refunds\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2025-12-30T08:46:00.248855Z", "completed_at": "2025-12-30T08:46:00.286030Z"}, {"name": "execute", "started_at": "2025-12-30T08:46:00.287574Z", "completed_at": "2025-12-30T08:46:00.591326Z"}], "thread_id": "Thread-2 (worker)", "execution_time": 0.34912991523742676, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.stg_linea__refunds", "compiled": true, "compiled_code": "-- Staging model for ExecutedRelayerRefundRoot events from Linea\r\n\r\nWITH raw_refunds AS (\r\n\r\n    SELECT\r\n        -- Core event metadata - these identify WHEN and WHERE the event happened\r\n        timestamp_datetime,              -- When the event occurred (from blockchain)\r\n        transactionHash,                 -- Unique transaction identifier\r\n        blockchain,                      -- Which blockchain (should be 'linea')\r\n        source_file,                     -- Which source file this came from (for lineage)\r\n        \r\n        -- Indexed fields from topics (these are searchable/filterable in blockchain)\r\n        -- Topics are like \"indexed columns\" in SQL - they're stored separately for fast queries\r\n        topic_chain_id,                  -- Chain ID where the refund was executed\r\n        topic_root_bundle_id,            -- Merkle root bundle identifier (groups multiple refunds)\r\n        topic_leaf_id,                   -- Leaf index in the merkle tree (identifies this specific refund batch)\r\n        \r\n        -- Non-indexed fields from the event's data field\r\n        -- These are the actual business data about the refund\r\n        amount_to_return,                -- Total amount of capital returned in this batch\r\n        l2_token_address,                -- Token address being refunded (on Linea)\r\n        refund_amounts,                  -- Comma-separated list of individual refund amounts\r\n        refund_addresses,                -- Comma-separated list of relayer addresses receiving refunds\r\n        refund_count                     -- Number of relayers being refunded in this batch\r\n        \r\n    FROM raw.linea_logs_processed\r\n    \r\n    -- Filter: Only include rows where ExecutedRelayerRefundRoot data exists\r\n    -- Topic_0 is the event signature hash that identifies the event type\r\n    WHERE topic_0 = '0xf4ad92585b1bc117fbdd644990adf0827bc4c95baeae8a23322af807b6d0020e'\r\n        AND amount_to_return IS NOT NULL\r\n        AND l2_token_address IS NOT NULL\r\n        AND refund_count IS NOT NULL\r\n),\r\n\r\ncleaned_refunds AS (\r\n    SELECT\r\n        \r\n        -- Timestamp: Convert text to proper timestamp type\r\n        CASE \r\n            WHEN timestamp_datetime ~ '^\\d+$' THEN          -- If it's pure digits, treat as Unix timestamp (seconds since 1970-01-01)\r\n                TO_TIMESTAMP(timestamp_datetime::BIGINT)    -- PostgreSQL's TO_TIMESTAMP converts Unix seconds to timestamp\r\n            ELSE \r\n                timestamp_datetime::TIMESTAMP               -- Otherwise, try to parse as ISO format string (e.g., \"2025-12-03 10:30:00\")\r\n        END AS refund_timestamp,\r\n        \r\n        transactionHash AS transaction_hash,\r\n        blockchain,\r\n        source_file,\r\n        \r\n        -- ============================================================\r\n        -- INDEXED FIELDS (from topics) - Already decoded by ETL\r\n        -- ============================================================\r\n        -- These come from the event's \"topics\" array, which is indexed for fast blockchain queries\r\n        \r\n        -- Chain ID: Which blockchain this refund was executed on\r\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\r\n        -- Example: 42161 = Arbitrum, 1 = Ethereum, 59144 = Linea\r\n        -- Cast via NUMERIC first to handle decimal strings like \"42161.0\" from ETL\r\n        (topic_chain_id::NUMERIC)::BIGINT AS chain_id,\r\n        \r\n        -- Root bundle ID: Groups multiple refunds together in a merkle tree\r\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\r\n        -- Multiple refund batches can be grouped into one merkle root for gas efficiency\r\n        -- Cast via NUMERIC first to handle decimal strings like \"12345.0\" from ETL\r\n        (topic_root_bundle_id::NUMERIC)::BIGINT AS root_bundle_id,\r\n        \r\n        -- Leaf ID: Identifies this specific refund batch within the merkle tree\r\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\r\n        -- Each leaf represents one batch of refunds to multiple relayers\r\n        -- Cast via NUMERIC first to handle decimal strings like \"789.0\" from ETL\r\n        (topic_leaf_id::NUMERIC)::BIGINT AS leaf_id,\r\n        \r\n        -- ============================================================\r\n        -- TOKEN INFORMATION (what was refunded)\r\n        -- ============================================================\r\n        \r\n        -- L2 token address: The token being refunded on Linea\r\n        -- Already decoded by ETL to proper address format (0x...)\r\n        -- Example: 0x176211869cA2b568f2A7D4EE941E07aA25fee00b = USDC on Linea\r\n        l2_token_address AS refund_token_address,\r\n        \r\n        -- ============================================================\r\n        -- AMOUNT INFORMATION (how much was refunded)\r\n        -- ============================================================\r\n        \r\n        -- Amount to return: Total capital returned in this batch\r\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\r\n        -- This is the sum of all individual refund amounts in the batch\r\n        -- Stored as NUMERIC to handle large token amounts (18 decimals)\r\n        amount_to_return::NUMERIC AS total_refund_amount,\r\n        \r\n        -- Refund amounts: Individual refund amounts as comma-separated string\r\n        -- Format: \"1000000000000000000,2000000000000000000,3000000000000000000\"\r\n        -- Each value is in token's base units (e.g., wei for ETH, 6 decimals for USDC)\r\n        -- This is a comma-separated string because the ETL processes dynamic arrays\r\n        refund_amounts AS refund_amounts_string,\r\n        \r\n        -- Refund addresses: Relayer addresses receiving refunds as comma-separated string\r\n        -- Format: \"0xAAA...,0xBBB...,0xCCC...\"\r\n        -- Matches 1:1 with refund_amounts (first address gets first amount, etc.)\r\n        -- This is a comma-separated string because the ETL processes dynamic arrays\r\n        refund_addresses AS refund_addresses_string,\r\n        \r\n        -- Refund count: Number of relayers being refunded in this batch\r\n        -- Already converted by ETL to integer\r\n        -- This tells us how many individual refunds are in the arrays above\r\n        refund_count::INTEGER AS refund_count\r\n        \r\n    FROM raw_refunds\r\n)\r\n\r\n-- Final SELECT: Add any computed fields and ensure data quality\r\nSELECT\r\n    -- Event identity\r\n    refund_timestamp,\r\n    transaction_hash,\r\n    blockchain,\r\n    source_file,\r\n    \r\n    -- Indexed fields (merkle tree identifiers)\r\n    chain_id,\r\n    root_bundle_id,\r\n    leaf_id,\r\n    \r\n    -- Token info\r\n    refund_token_address,\r\n    \r\n    -- Amounts\r\n    total_refund_amount,\r\n    refund_amounts_string,\r\n    refund_addresses_string,\r\n    refund_count\r\n    \r\nFROM cleaned_refunds\r\n\r\n-- Data quality: Only include rows with essential fields populated\r\nWHERE chain_id IS NOT NULL\r\n    AND refund_token_address IS NOT NULL\r\n    AND total_refund_amount IS NOT NULL\r\n    AND refund_count IS NOT NULL", "relation_name": "\"across_analytics\".\"dbt_staging\".\"stg_linea__refunds\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2025-12-30T08:46:00.142528Z", "completed_at": "2025-12-30T08:46:00.182556Z"}, {"name": "execute", "started_at": "2025-12-30T08:46:00.183881Z", "completed_at": "2025-12-30T08:46:00.597556Z"}], "thread_id": "Thread-4 (worker)", "execution_time": 0.46318507194519043, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.stg_linea__deposits", "compiled": true, "compiled_code": "-- Staging model for FundsDeposited events from Linea\n-- This model extracts deposit events where users initiate cross-chain bridge transactions\n\nWITH raw_deposits AS (\n\n    SELECT\n        timestamp_datetime,\n        transactionHash,\n        blockchain,\n        source_file,\n        topic_destination_chain_id,\n        topic_deposit_id,\n        topic_depositor,\n        funds_deposited_data_input_token,\n        funds_deposited_data_output_token,\n        funds_deposited_data_input_amount,\n        funds_deposited_data_output_amount,\n        funds_deposited_data_recipient\n        \n    FROM raw.linea_logs_processed\n    \n    WHERE topic_0 = '0x32ed1a409ef04c7b0227189c3a103dc5ac10e775a15b785dcc510201f7c25ad3'\n        AND funds_deposited_data_input_amount IS NOT NULL\n        AND funds_deposited_data_output_amount IS NOT NULL\n),\n\n-- INPUT token metadata: filtered to origin chain (Linea)\ninput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n    WHERE chain = 'linea'\n\n),\n\n-- OUTPUT token metadata: includes chain_id for matching with destination_chain_id\noutput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals,\n        chain_id\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n\n),\n\ncleaned_deposits AS (\n    SELECT\n        CASE \n            WHEN timestamp_datetime ~ '^\\d+$' THEN\n                TO_TIMESTAMP(timestamp_datetime::BIGINT)\n            ELSE \n                timestamp_datetime::TIMESTAMP\n        END AS deposit_timestamp,\n        \n        transactionHash AS transaction_hash,\n        blockchain,\n        source_file,\n        (topic_destination_chain_id::NUMERIC)::BIGINT AS destination_chain_id,\n        (topic_deposit_id::NUMERIC)::BIGINT AS deposit_id,\n        topic_depositor AS depositor_address,\n        LOWER(funds_deposited_data_input_token) AS input_token_address,\n        LOWER(funds_deposited_data_output_token) AS output_token_address,\n        funds_deposited_data_input_amount::NUMERIC AS input_amount_raw,\n        funds_deposited_data_output_amount::NUMERIC AS output_amount_raw,\n        funds_deposited_data_recipient AS recipient_address\n        \n    FROM raw_deposits\n)\n\nSELECT\n    c.deposit_timestamp,\n    c.transaction_hash,\n    c.blockchain,\n    c.source_file,\n    c.destination_chain_id,\n    c.deposit_id,\n    c.depositor_address,\n    c.input_token_address,\n    input_tok.token_symbol AS input_token_symbol,\n    c.output_token_address,\n    output_tok.token_symbol AS output_token_symbol,\n    \n    c.input_amount_raw / POWER(10, COALESCE(input_tok.decimals, 18))\n AS input_amount,\n    \n    CASE \n        WHEN c.output_token_address = '0x0000000000000000000000000000000000000000' \n        THEN c.output_amount_raw / POWER(10, COALESCE(input_tok.decimals, 18))\n        ELSE c.output_amount_raw / POWER(10, COALESCE(output_tok.decimals, 18))\n    END\n AS output_amount,\n    c.input_amount_raw,\n    c.output_amount_raw,\n    c.recipient_address\n    \nFROM cleaned_deposits c\n\nLEFT JOIN input_token_meta AS input_tok\n    ON c.input_token_address = input_tok.token_address\n\nLEFT JOIN output_token_meta AS output_tok\n    ON c.output_token_address = output_tok.token_address\n    AND c.destination_chain_id = output_tok.chain_id\n\nWHERE c.deposit_id IS NOT NULL\n    AND c.depositor_address IS NOT NULL\n    AND c.input_token_address IS NOT NULL\n    AND c.output_token_address IS NOT NULL\n    AND c.input_amount_raw IS NOT NULL\n    AND c.output_amount_raw IS NOT NULL", "relation_name": "\"across_analytics\".\"dbt_staging\".\"stg_linea__deposits\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2025-12-30T08:46:00.216027Z", "completed_at": "2025-12-30T08:46:00.230646Z"}, {"name": "execute", "started_at": "2025-12-30T08:46:00.231843Z", "completed_at": "2025-12-30T08:46:00.704193Z"}], "thread_id": "Thread-3 (worker)", "execution_time": 0.5006763935089111, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.stg_linea__fills", "compiled": true, "compiled_code": "-- Staging model for FilledRelay events from Linea\n-- This model extracts fill events where relayers complete cross-chain bridge transactions\n\nWITH raw_fills AS (\n\n    SELECT\n        -- Core event metadata - these identify WHEN and WHERE the event happened\n        timestamp_datetime,              -- When the event occurred (from blockchain)\n        transactionHash,                 -- Unique transaction identifier\n        blockchain,                      -- Which blockchain (should be 'linea')\n        source_file,                     -- Which source file this came from (for lineage)\n        \n        -- Indexed fields from topics (these are searchable/filterable in blockchain)\n        -- Topics are like \"indexed columns\" in SQL - they're stored separately for fast queries\n        topic_origin_chain_id,           -- Chain ID where the original deposit happened\n        topic_deposit_id,                -- Unique deposit identifier (links to deposit event)\n        topic_relayer,                   -- Address of the relayer who provided liquidity\n        \n        -- Non-indexed fields from the event's data field\n        -- These are the actual business data about the fill\n        filled_relay_data_input_token,          -- Token address on origin chain\n        filled_relay_data_output_token,          -- Token address on destination chain\n        filled_relay_data_input_amount,          -- Amount sent (in origin token units)\n        filled_relay_data_output_amount,         -- Amount received (in destination token units)\n        filled_relay_data_repayment_chain_id,     -- Where relayer gets reimbursed\n        filled_relay_data_exclusive_relayer,     -- Address with exclusive fill rights (if any)\n        filled_relay_data_depositor,             -- Original user who initiated the bridge\n        filled_relay_data_recipient,              -- Final recipient of the bridged funds\n        \n        -- Gas data (for relayer cost analysis)\n        gas_price_wei,\n        gas_used\n        \n    FROM raw.linea_logs_processed\n    \n    -- Filter: Only include rows where FilledRelay data exists\n    WHERE topic_0 = '0x44b559f101f8fbcc8a0ea43fa91a05a729a5ea6e14a7c75aa750374690137208'\n        AND filled_relay_data_input_amount IS NOT NULL\n        AND filled_relay_data_output_amount IS NOT NULL\n),\n\n-- INPUT token metadata: includes chain_id for matching with origin_chain_id\n-- This allows us to find the correct token on whichever chain the deposit came from\ninput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals,\n        chain_id\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n\n),\n\n-- OUTPUT token metadata: filtered to destination chain (Linea)\n-- This is correct because on fills, output_token is always on the destination chain\noutput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n    WHERE chain = 'linea'\n\n),\n\ncleaned_fills AS (\n    SELECT\n        \n        -- Timestamp: Convert text to proper timestamp type\n        \n        CASE \n            WHEN timestamp_datetime ~ '^\\d+$' THEN          -- If it's pure digits, treat as Unix timestamp (seconds since 1970-01-01)\n                TO_TIMESTAMP(timestamp_datetime::BIGINT)    -- PostgreSQL's TO_TIMESTAMP can parse various formats, but we use the simplest approach\n            ELSE \n                timestamp_datetime::TIMESTAMP               -- Otherwise, try to parse as ISO format string\n        END AS fill_timestamp,\n        \n        transactionHash AS transaction_hash,\n        blockchain,\n        source_file,\n        \n        -- ============================================================\n        -- INDEXED FIELDS (from topics) - Already decoded by ETL\n        -- ============================================================\n        -- These come from the event's \"topics\" array, which is indexed for fast blockchain queries\n        \n        -- Origin chain ID: Which blockchain the funds came FROM\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\n        -- Example: 1 = Ethereum, 42161 = Arbitrum, 8453 = Base\n        -- Cast via NUMERIC first to handle decimal strings like \"8453.0\" from ETL\n        (topic_origin_chain_id::NUMERIC)::BIGINT AS origin_chain_id,\n        \n        -- Deposit ID: Unique identifier linking this fill to its original deposit\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\n        -- This is the KEY that connects deposits \u2194 fills across chains\n        -- Cast via NUMERIC first to handle decimal strings like \"12345.0\" from ETL\n        (topic_deposit_id::NUMERIC)::BIGINT AS deposit_id,\n        \n        -- Relayer address: Who provided the liquidity\n        -- Already decoded by ETL to proper address format (0x...)\n        topic_relayer AS relayer_address,\n        \n        -- ============================================================\n        -- TOKEN INFORMATION (what was bridged)\n        -- ============================================================\n        \n        -- Input token: The token address on the origin chain\n        -- Already decoded by ETL to proper address format\n        -- Example: 0xA0b86991c6218b36c1d19D4a2e9Eb0cE3606eB48 = USDC on Ethereum\n        -- Normalize to lowercase for joining with token metadata\n        LOWER(filled_relay_data_input_token) AS input_token_address,\n        \n        -- Output token: The token address on the destination chain (Linea in this case)\n        -- Already decoded by ETL to proper address format\n        -- Normalize to lowercase for joining with token metadata\n        LOWER(filled_relay_data_output_token) AS output_token_address,\n        \n        -- ============================================================\n        -- AMOUNT INFORMATION (how much was bridged)\n        -- ============================================================\n        \n        -- Input amount: How much was sent from origin chain\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\n        -- Store as RAW amount first (before rescaling by decimals)\n        filled_relay_data_input_amount::NUMERIC AS input_amount_raw,\n        \n        -- Output amount: How much was received on destination chain\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\n        -- Usually slightly less than input due to fees/spread\n        -- Store as RAW amount first (before rescaling by decimals)\n        filled_relay_data_output_amount::NUMERIC AS output_amount_raw,\n        \n        -- ============================================================\n        -- RELAYER & ROUTING INFORMATION\n        -- ============================================================\n        \n        -- Repayment chain ID: Where the relayer gets reimbursed\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\n        -- Relayers front capital, then get paid back (often on a different chain)\n        -- Cast via NUMERIC first to handle decimal strings like \"1.0\" from ETL\n        (filled_relay_data_repayment_chain_id::NUMERIC)::BIGINT AS repayment_chain_id,\n        \n        -- Exclusive relayer: If set, only this address can fill this deposit\n        -- Already decoded by ETL to proper address format\n        -- Used for priority/guaranteed fills (NULL if no exclusive relayer)\n        filled_relay_data_exclusive_relayer AS exclusive_relayer_address,\n        \n        -- ============================================================\n        -- USER INFORMATION\n        -- ============================================================\n        \n        -- Depositor: The original user who initiated the bridge\n        -- Already decoded by ETL to proper address format\n        filled_relay_data_depositor AS depositor_address,\n        \n        -- Recipient: Who receives the funds on the destination chain\n        -- Already decoded by ETL to proper address format\n        -- Usually the same as depositor, but can be different (gift/transfer)\n        filled_relay_data_recipient AS recipient_address,\n        \n        -- ============================================================\n        -- GAS DATA (for relayer cost analysis)\n        -- ============================================================\n        gas_price_wei::BIGINT AS gas_price_wei,\n        gas_used::BIGINT AS gas_used\n        \n    FROM raw_fills\n)\n\n-- ============================================================\n-- FINAL SELECT: Join with token metadata and rescale amounts\n-- ============================================================\nSELECT\n    -- Event identity\n    c.fill_timestamp,\n    c.transaction_hash,\n    c.blockchain,\n    c.source_file,\n    \n    -- Indexed fields\n    c.origin_chain_id,\n    c.deposit_id,\n    c.relayer_address,\n    \n    -- Token info WITH NAMES\n    -- Input token (what was deposited on origin chain)\n    c.input_token_address,\n    input_tok.token_symbol AS input_token_symbol,        -- e.g., 'USDC', 'WETH'\n    \n    -- Output token (what was received on destination chain - Linea)\n    c.output_token_address,\n    output_tok.token_symbol AS output_token_symbol,      -- e.g., 'USDC', 'WETH'\n    \n    -- Rescaled amounts (human-readable)\n    -- These use the rescale_amount macro which divides raw amount by 10^decimals\n    -- Example: 5000000 raw USDC (6 decimals) \u2192 5.0 USDC\n    \n    c.input_amount_raw / POWER(10, COALESCE(input_tok.decimals, 18))\n AS input_amount,\n    \n    c.output_amount_raw / POWER(10, COALESCE(output_tok.decimals, 18))\n AS output_amount,\n    \n    -- Raw amounts (preserved for auditing)\n    -- These are the original blockchain values before rescaling\n    -- Example: 5.0 USDC is stored as 5000000 on blockchain\n    c.input_amount_raw,\n    c.output_amount_raw,\n    \n    -- Relayer routing\n    c.repayment_chain_id,\n    c.exclusive_relayer_address,\n    \n    -- User info\n    c.depositor_address,\n    c.recipient_address,\n    \n    -- Gas data (for relayer cost analysis)\n    c.gas_price_wei,\n    c.gas_used,\n    (c.gas_price_wei * c.gas_used) AS gas_cost_wei\n    \nFROM cleaned_fills c\n\n-- Join with token metadata to get decimals and symbols for INPUT token\n-- Join on BOTH address AND chain_id to match the correct origin chain\nLEFT JOIN input_token_meta AS input_tok\n    ON c.input_token_address = input_tok.token_address\n    AND c.origin_chain_id = input_tok.chain_id\n\n-- Join with token metadata to get decimals and symbols for OUTPUT token\n-- Join on address only - we already filtered to destination chain (Linea)\nLEFT JOIN output_token_meta AS output_tok\n    ON c.output_token_address = output_tok.token_address\n\n-- Data quality: Only include rows with essential fields populated\nWHERE c.deposit_id IS NOT NULL\n    AND c.relayer_address IS NOT NULL\n    AND c.input_token_address IS NOT NULL\n    AND c.output_token_address IS NOT NULL\n    AND c.input_amount_raw IS NOT NULL\n    AND c.output_amount_raw IS NOT NULL", "relation_name": "\"across_analytics\".\"dbt_staging\".\"stg_linea__fills\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2025-12-30T08:46:00.394712Z", "completed_at": "2025-12-30T08:46:00.408324Z"}, {"name": "execute", "started_at": "2025-12-30T08:46:00.409923Z", "completed_at": "2025-12-30T08:46:00.722937Z"}], "thread_id": "Thread-1 (worker)", "execution_time": 0.334362268447876, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.stg_monad__deposits", "compiled": true, "compiled_code": "-- Staging model for FundsDeposited events from Monad\n-- This model extracts deposit events where users initiate cross-chain bridge transactions\n\nWITH raw_deposits AS (\n\n    SELECT\n        timestamp_datetime,\n        transactionHash,\n        blockchain,\n        source_file,\n        topic_destination_chain_id,\n        topic_deposit_id,\n        topic_depositor,\n        funds_deposited_data_input_token,\n        funds_deposited_data_output_token,\n        funds_deposited_data_input_amount,\n        funds_deposited_data_output_amount,\n        funds_deposited_data_recipient\n        \n    FROM raw.monad_logs_processed\n    \n    WHERE topic_0 = '0x32ed1a409ef04c7b0227189c3a103dc5ac10e775a15b785dcc510201f7c25ad3'\n        AND funds_deposited_data_input_amount IS NOT NULL\n        AND funds_deposited_data_output_amount IS NOT NULL\n),\n\n-- INPUT token metadata: filtered to origin chain (Monad)\n-- Note: monad may not have tokens in token_metadata yet\ninput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n    WHERE chain = 'monad'\n\n),\n\n-- OUTPUT token metadata: includes chain_id for matching with destination_chain_id\noutput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals,\n        chain_id\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n\n),\n\ncleaned_deposits AS (\n    SELECT\n        CASE \n            WHEN timestamp_datetime ~ '^\\d+$' THEN\n                TO_TIMESTAMP(timestamp_datetime::BIGINT)\n            ELSE \n                timestamp_datetime::TIMESTAMP\n        END AS deposit_timestamp,\n        \n        transactionHash AS transaction_hash,\n        blockchain,\n        source_file,\n        (topic_destination_chain_id::NUMERIC)::BIGINT AS destination_chain_id,\n        (topic_deposit_id::NUMERIC)::BIGINT AS deposit_id,\n        topic_depositor AS depositor_address,\n        LOWER(funds_deposited_data_input_token) AS input_token_address,\n        LOWER(funds_deposited_data_output_token) AS output_token_address,\n        funds_deposited_data_input_amount::NUMERIC AS input_amount_raw,\n        funds_deposited_data_output_amount::NUMERIC AS output_amount_raw,\n        funds_deposited_data_recipient AS recipient_address\n        \n    FROM raw_deposits\n)\n\nSELECT\n    c.deposit_timestamp,\n    c.transaction_hash,\n    c.blockchain,\n    c.source_file,\n    c.destination_chain_id,\n    c.deposit_id,\n    c.depositor_address,\n    c.input_token_address,\n    input_tok.token_symbol AS input_token_symbol,\n    c.output_token_address,\n    output_tok.token_symbol AS output_token_symbol,\n    \n    c.input_amount_raw / POWER(10, COALESCE(input_tok.decimals, 18))\n AS input_amount,\n    \n    CASE \n        WHEN c.output_token_address = '0x0000000000000000000000000000000000000000' \n        THEN c.output_amount_raw / POWER(10, COALESCE(input_tok.decimals, 18))\n        ELSE c.output_amount_raw / POWER(10, COALESCE(output_tok.decimals, 18))\n    END\n AS output_amount,\n    c.input_amount_raw,\n    c.output_amount_raw,\n    c.recipient_address\n    \nFROM cleaned_deposits c\n\nLEFT JOIN input_token_meta AS input_tok\n    ON c.input_token_address = input_tok.token_address\n\nLEFT JOIN output_token_meta AS output_tok\n    ON c.output_token_address = output_tok.token_address\n    AND c.destination_chain_id = output_tok.chain_id\n\nWHERE c.deposit_id IS NOT NULL\n    AND c.depositor_address IS NOT NULL\n    AND c.input_token_address IS NOT NULL\n    AND c.output_token_address IS NOT NULL\n    AND c.input_amount_raw IS NOT NULL\n    AND c.output_amount_raw IS NOT NULL", "relation_name": "\"across_analytics\".\"dbt_staging\".\"stg_monad__deposits\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2025-12-30T08:46:00.638244Z", "completed_at": "2025-12-30T08:46:00.663205Z"}, {"name": "execute", "started_at": "2025-12-30T08:46:00.665258Z", "completed_at": "2025-12-30T08:46:00.977659Z"}], "thread_id": "Thread-2 (worker)", "execution_time": 0.35203003883361816, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.stg_monad__fills", "compiled": true, "compiled_code": "-- Staging model for FilledRelay events from Monad\n\nWITH raw_fills AS (\n    SELECT\n        timestamp_datetime, transactionHash, blockchain, source_file,\n        topic_origin_chain_id, topic_deposit_id, topic_relayer,\n        filled_relay_data_input_token, filled_relay_data_output_token,\n        filled_relay_data_input_amount, filled_relay_data_output_amount,\n        filled_relay_data_repayment_chain_id, filled_relay_data_exclusive_relayer,\n        filled_relay_data_depositor, filled_relay_data_recipient,\n        gas_price_wei, gas_used\n    FROM raw.monad_logs_processed\n    WHERE topic_0 = '0x44b559f101f8fbcc8a0ea43fa91a05a729a5ea6e14a7c75aa750374690137208'\n        AND filled_relay_data_input_amount IS NOT NULL\n        AND filled_relay_data_output_amount IS NOT NULL\n),\n\ninput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals,\n        chain_id\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n\n),\n\noutput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n    WHERE chain = 'monad'\n\n),\n\ncleaned_fills AS (\n    SELECT\n        CASE \n            WHEN timestamp_datetime ~ '^\\d+$' THEN TO_TIMESTAMP(timestamp_datetime::BIGINT)\n            ELSE timestamp_datetime::TIMESTAMP\n        END AS fill_timestamp,\n        transactionHash AS transaction_hash,\n        blockchain, source_file,\n        (topic_origin_chain_id::NUMERIC)::BIGINT AS origin_chain_id,\n        (topic_deposit_id::NUMERIC)::BIGINT AS deposit_id,\n        topic_relayer AS relayer_address,\n        LOWER(filled_relay_data_input_token) AS input_token_address,\n        LOWER(filled_relay_data_output_token) AS output_token_address,\n        filled_relay_data_input_amount::NUMERIC AS input_amount_raw,\n        filled_relay_data_output_amount::NUMERIC AS output_amount_raw,\n        (filled_relay_data_repayment_chain_id::NUMERIC)::BIGINT AS repayment_chain_id,\n        filled_relay_data_exclusive_relayer AS exclusive_relayer_address,\n        filled_relay_data_depositor AS depositor_address,\n        filled_relay_data_recipient AS recipient_address,\n        gas_price_wei::BIGINT AS gas_price_wei,\n        gas_used::BIGINT AS gas_used\n    FROM raw_fills\n)\n\nSELECT\n    c.fill_timestamp, c.transaction_hash, c.blockchain, c.source_file,\n    c.origin_chain_id, c.deposit_id, c.relayer_address,\n    c.input_token_address, input_tok.token_symbol AS input_token_symbol,\n    c.output_token_address, output_tok.token_symbol AS output_token_symbol,\n    \n    c.input_amount_raw / POWER(10, COALESCE(input_tok.decimals, 18))\n AS input_amount,\n    \n    c.output_amount_raw / POWER(10, COALESCE(output_tok.decimals, 18))\n AS output_amount,\n    c.input_amount_raw, c.output_amount_raw,\n    c.repayment_chain_id, c.exclusive_relayer_address,\n    c.depositor_address, c.recipient_address,\n    c.gas_price_wei, c.gas_used, (c.gas_price_wei * c.gas_used) AS gas_cost_wei\nFROM cleaned_fills c\nLEFT JOIN input_token_meta AS input_tok\n    ON c.input_token_address = input_tok.token_address AND c.origin_chain_id = input_tok.chain_id\nLEFT JOIN output_token_meta AS output_tok\n    ON c.output_token_address = output_tok.token_address\nWHERE c.deposit_id IS NOT NULL AND c.relayer_address IS NOT NULL\n    AND c.input_token_address IS NOT NULL AND c.output_token_address IS NOT NULL\n    AND c.input_amount_raw IS NOT NULL AND c.output_amount_raw IS NOT NULL", "relation_name": "\"across_analytics\".\"dbt_staging\".\"stg_monad__fills\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2025-12-30T08:46:00.659614Z", "completed_at": "2025-12-30T08:46:00.672788Z"}, {"name": "execute", "started_at": "2025-12-30T08:46:00.676620Z", "completed_at": "2025-12-30T08:46:00.998427Z"}], "thread_id": "Thread-4 (worker)", "execution_time": 0.37165093421936035, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.stg_monad__refunds", "compiled": true, "compiled_code": "-- Staging model for ExecutedRelayerRefundRoot events from Monad\r\n\r\nWITH raw_refunds AS (\r\n\r\n    SELECT\r\n        -- Core event metadata - these identify WHEN and WHERE the event happened\r\n        timestamp_datetime,              -- When the event occurred (from blockchain)\r\n        transactionHash,                 -- Unique transaction identifier\r\n        blockchain,                      -- Which blockchain (should be 'monad')\r\n        source_file,                     -- Which source file this came from (for lineage)\r\n        \r\n        -- Indexed fields from topics (these are searchable/filterable in blockchain)\r\n        -- Topics are like \"indexed columns\" in SQL - they're stored separately for fast queries\r\n        topic_chain_id,                  -- Chain ID where the refund was executed\r\n        topic_root_bundle_id,            -- Merkle root bundle identifier (groups multiple refunds)\r\n        topic_leaf_id,                   -- Leaf index in the merkle tree (identifies this specific refund batch)\r\n        \r\n        -- Non-indexed fields from the event's data field\r\n        -- These are the actual business data about the refund\r\n        amount_to_return,                -- Total amount of capital returned in this batch\r\n        l2_token_address,                -- Token address being refunded (on Monad)\r\n        refund_amounts,                  -- Comma-separated list of individual refund amounts\r\n        refund_addresses,                -- Comma-separated list of relayer addresses receiving refunds\r\n        refund_count                     -- Number of relayers being refunded in this batch\r\n        \r\n    FROM raw.monad_logs_processed\r\n    \r\n    -- Filter: Only include rows where ExecutedRelayerRefundRoot data exists\r\n    -- Topic_0 is the event signature hash that identifies the event type\r\n    WHERE topic_0 = '0xf4ad92585b1bc117fbdd644990adf0827bc4c95baeae8a23322af807b6d0020e'\r\n        AND amount_to_return IS NOT NULL\r\n        AND l2_token_address IS NOT NULL\r\n        AND refund_count IS NOT NULL\r\n),\r\n\r\ncleaned_refunds AS (\r\n    SELECT\r\n        \r\n        -- Timestamp: Convert text to proper timestamp type\r\n        CASE \r\n            WHEN timestamp_datetime ~ '^\\d+$' THEN          -- If it's pure digits, treat as Unix timestamp (seconds since 1970-01-01)\r\n                TO_TIMESTAMP(timestamp_datetime::BIGINT)    -- PostgreSQL's TO_TIMESTAMP converts Unix seconds to timestamp\r\n            ELSE \r\n                timestamp_datetime::TIMESTAMP               -- Otherwise, try to parse as ISO format string (e.g., \"2025-12-03 10:30:00\")\r\n        END AS refund_timestamp,\r\n        \r\n        transactionHash AS transaction_hash,\r\n        blockchain,\r\n        source_file,\r\n        \r\n        -- ============================================================\r\n        -- INDEXED FIELDS (from topics) - Already decoded by ETL\r\n        -- ============================================================\r\n        -- These come from the event's \"topics\" array, which is indexed for fast blockchain queries\r\n        \r\n        -- Chain ID: Which blockchain this refund was executed on\r\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\r\n        -- Example: 42161 = Arbitrum, 1 = Ethereum, 143 = Monad\r\n        -- Cast via NUMERIC first to handle decimal strings like \"42161.0\" from ETL\r\n        (topic_chain_id::NUMERIC)::BIGINT AS chain_id,\r\n        \r\n        -- Root bundle ID: Groups multiple refunds together in a merkle tree\r\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\r\n        -- Multiple refund batches can be grouped into one merkle root for gas efficiency\r\n        -- Cast via NUMERIC first to handle decimal strings like \"12345.0\" from ETL\r\n        (topic_root_bundle_id::NUMERIC)::BIGINT AS root_bundle_id,\r\n        \r\n        -- Leaf ID: Identifies this specific refund batch within the merkle tree\r\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\r\n        -- Each leaf represents one batch of refunds to multiple relayers\r\n        -- Cast via NUMERIC first to handle decimal strings like \"789.0\" from ETL\r\n        (topic_leaf_id::NUMERIC)::BIGINT AS leaf_id,\r\n        \r\n        -- ============================================================\r\n        -- TOKEN INFORMATION (what was refunded)\r\n        -- ============================================================\r\n        \r\n        -- L2 token address: The token being refunded on Monad\r\n        -- Already decoded by ETL to proper address format (0x...)\r\n        -- Example: 0x754704Bc059F8C67012fEd69BC8A327a5aafb603 = USDC on Monad\r\n        l2_token_address AS refund_token_address,\r\n        \r\n        -- ============================================================\r\n        -- AMOUNT INFORMATION (how much was refunded)\r\n        -- ============================================================\r\n        \r\n        -- Amount to return: Total capital returned in this batch\r\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\r\n        -- This is the sum of all individual refund amounts in the batch\r\n        -- Stored as NUMERIC to handle large token amounts (18 decimals)\r\n        amount_to_return::NUMERIC AS total_refund_amount,\r\n        \r\n        -- Refund amounts: Individual refund amounts as comma-separated string\r\n        -- Format: \"1000000000000000000,2000000000000000000,3000000000000000000\"\r\n        -- Each value is in token's base units (e.g., wei for ETH, 6 decimals for USDC)\r\n        -- This is a comma-separated string because the ETL processes dynamic arrays\r\n        refund_amounts AS refund_amounts_string,\r\n        \r\n        -- Refund addresses: Relayer addresses receiving refunds as comma-separated string\r\n        -- Format: \"0xAAA...,0xBBB...,0xCCC...\"\r\n        -- Matches 1:1 with refund_amounts (first address gets first amount, etc.)\r\n        -- This is a comma-separated string because the ETL processes dynamic arrays\r\n        refund_addresses AS refund_addresses_string,\r\n        \r\n        -- Refund count: Number of relayers being refunded in this batch\r\n        -- Already converted by ETL to integer\r\n        -- This tells us how many individual refunds are in the arrays above\r\n        refund_count::INTEGER AS refund_count\r\n        \r\n    FROM raw_refunds\r\n)\r\n\r\n-- Final SELECT: Add any computed fields and ensure data quality\r\nSELECT\r\n    -- Event identity\r\n    refund_timestamp,\r\n    transaction_hash,\r\n    blockchain,\r\n    source_file,\r\n    \r\n    -- Indexed fields (merkle tree identifiers)\r\n    chain_id,\r\n    root_bundle_id,\r\n    leaf_id,\r\n    \r\n    -- Token info\r\n    refund_token_address,\r\n    \r\n    -- Amounts\r\n    total_refund_amount,\r\n    refund_amounts_string,\r\n    refund_addresses_string,\r\n    refund_count\r\n    \r\nFROM cleaned_refunds\r\n\r\n-- Data quality: Only include rows with essential fields populated\r\nWHERE chain_id IS NOT NULL\r\n    AND refund_token_address IS NOT NULL\r\n    AND total_refund_amount IS NOT NULL\r\n    AND refund_count IS NOT NULL", "relation_name": "\"across_analytics\".\"dbt_staging\".\"stg_monad__refunds\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2025-12-30T08:46:00.739948Z", "completed_at": "2025-12-30T08:46:00.756444Z"}, {"name": "execute", "started_at": "2025-12-30T08:46:00.759309Z", "completed_at": "2025-12-30T08:46:01.013081Z"}], "thread_id": "Thread-3 (worker)", "execution_time": 0.2796354293823242, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.stg_polygon__deposits", "compiled": true, "compiled_code": "-- Staging model for FundsDeposited events from Polygon\n-- This model extracts deposit events where users initiate cross-chain bridge transactions\n\nWITH raw_deposits AS (\n\n    SELECT\n        -- Core event metadata\n        timestamp_datetime,\n        transactionHash,\n        blockchain,\n        source_file,\n        \n        -- Indexed fields from topics\n        topic_destination_chain_id,\n        topic_deposit_id,\n        topic_depositor,\n        \n        -- Non-indexed fields from the event's data field\n        funds_deposited_data_input_token,\n        funds_deposited_data_output_token,\n        funds_deposited_data_input_amount,\n        funds_deposited_data_output_amount,\n        funds_deposited_data_recipient\n        \n    FROM raw.polygon_logs_processed\n    \n    WHERE topic_0 = '0x32ed1a409ef04c7b0227189c3a103dc5ac10e775a15b785dcc510201f7c25ad3'\n        AND funds_deposited_data_input_amount IS NOT NULL\n        AND funds_deposited_data_output_amount IS NOT NULL\n),\n\n-- INPUT token metadata: filtered to origin chain (Polygon)\ninput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n    WHERE chain = 'polygon'\n\n),\n\n-- OUTPUT token metadata: includes chain_id for matching with destination_chain_id\noutput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals,\n        chain_id\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n\n),\n\ncleaned_deposits AS (\n    SELECT\n        CASE \n            WHEN timestamp_datetime ~ '^\\d+$' THEN\n                TO_TIMESTAMP(timestamp_datetime::BIGINT)\n            ELSE \n                timestamp_datetime::TIMESTAMP\n        END AS deposit_timestamp,\n        \n        transactionHash AS transaction_hash,\n        blockchain,\n        source_file,\n        (topic_destination_chain_id::NUMERIC)::BIGINT AS destination_chain_id,\n        (topic_deposit_id::NUMERIC)::BIGINT AS deposit_id,\n        topic_depositor AS depositor_address,\n        LOWER(funds_deposited_data_input_token) AS input_token_address,\n        LOWER(funds_deposited_data_output_token) AS output_token_address,\n        funds_deposited_data_input_amount::NUMERIC AS input_amount_raw,\n        funds_deposited_data_output_amount::NUMERIC AS output_amount_raw,\n        funds_deposited_data_recipient AS recipient_address\n        \n    FROM raw_deposits\n)\n\nSELECT\n    c.deposit_timestamp,\n    c.transaction_hash,\n    c.blockchain,\n    c.source_file,\n    c.destination_chain_id,\n    c.deposit_id,\n    c.depositor_address,\n    c.input_token_address,\n    input_tok.token_symbol AS input_token_symbol,\n    c.output_token_address,\n    output_tok.token_symbol AS output_token_symbol,\n    \n    c.input_amount_raw / POWER(10, COALESCE(input_tok.decimals, 18))\n AS input_amount,\n    \n    CASE \n        WHEN c.output_token_address = '0x0000000000000000000000000000000000000000' \n        THEN c.output_amount_raw / POWER(10, COALESCE(input_tok.decimals, 18))\n        ELSE c.output_amount_raw / POWER(10, COALESCE(output_tok.decimals, 18))\n    END\n AS output_amount,\n    c.input_amount_raw,\n    c.output_amount_raw,\n    c.recipient_address\n    \nFROM cleaned_deposits c\n\nLEFT JOIN input_token_meta AS input_tok\n    ON c.input_token_address = input_tok.token_address\n\nLEFT JOIN output_token_meta AS output_tok\n    ON c.output_token_address = output_tok.token_address\n    AND c.destination_chain_id = output_tok.chain_id\n\nWHERE c.deposit_id IS NOT NULL\n    AND c.depositor_address IS NOT NULL\n    AND c.input_token_address IS NOT NULL\n    AND c.output_token_address IS NOT NULL\n    AND c.input_amount_raw IS NOT NULL\n    AND c.output_amount_raw IS NOT NULL", "relation_name": "\"across_analytics\".\"dbt_staging\".\"stg_polygon__deposits\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2025-12-30T08:46:00.777381Z", "completed_at": "2025-12-30T08:46:00.793024Z"}, {"name": "execute", "started_at": "2025-12-30T08:46:00.794412Z", "completed_at": "2025-12-30T08:46:01.088297Z"}], "thread_id": "Thread-1 (worker)", "execution_time": 0.3235514163970947, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.stg_polygon__fills", "compiled": true, "compiled_code": "-- Staging model for FilledRelay events from Polygon\n-- This model extracts fill events where relayers complete cross-chain bridge transactions\n\nWITH raw_fills AS (\n\n    SELECT\n        -- Core event metadata - these identify WHEN and WHERE the event happened\n        timestamp_datetime,              -- When the event occurred (from blockchain)\n        transactionHash,                 -- Unique transaction identifier\n        blockchain,                      -- Which blockchain (should be 'polygon')\n        source_file,                     -- Which source file this came from (for lineage)\n        \n        -- Indexed fields from topics (these are searchable/filterable in blockchain)\n        -- Topics are like \"indexed columns\" in SQL - they're stored separately for fast queries\n        topic_origin_chain_id,           -- Chain ID where the original deposit happened\n        topic_deposit_id,                -- Unique deposit identifier (links to deposit event)\n        topic_relayer,                   -- Address of the relayer who provided liquidity\n        \n        -- Non-indexed fields from the event's data field\n        -- These are the actual business data about the fill\n        filled_relay_data_input_token,          -- Token address on origin chain\n        filled_relay_data_output_token,          -- Token address on destination chain\n        filled_relay_data_input_amount,          -- Amount sent (in origin token units)\n        filled_relay_data_output_amount,         -- Amount received (in destination token units)\n        filled_relay_data_repayment_chain_id,     -- Where relayer gets reimbursed\n        filled_relay_data_exclusive_relayer,     -- Address with exclusive fill rights (if any)\n        filled_relay_data_depositor,             -- Original user who initiated the bridge\n        filled_relay_data_recipient,              -- Final recipient of the bridged funds\n        \n        -- Gas data (for relayer cost analysis)\n        gas_price_wei,\n        gas_used\n        \n    FROM raw.polygon_logs_processed\n    \n    -- Filter: Only include rows where FilledRelay data exists\n    WHERE topic_0 = '0x44b559f101f8fbcc8a0ea43fa91a05a729a5ea6e14a7c75aa750374690137208'\n        AND filled_relay_data_input_amount IS NOT NULL\n        AND filled_relay_data_output_amount IS NOT NULL\n),\n\n-- INPUT token metadata: includes chain_id for matching with origin_chain_id\n-- This allows us to find the correct token on whichever chain the deposit came from\ninput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals,\n        chain_id\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n\n),\n\n-- OUTPUT token metadata: filtered to destination chain (Polygon)\n-- This is correct because on fills, output_token is always on the destination chain\noutput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n    WHERE chain = 'polygon'\n\n),\n\ncleaned_fills AS (\n    SELECT\n        \n        -- Timestamp: Convert text to proper timestamp type\n        \n        CASE \n            WHEN timestamp_datetime ~ '^\\d+$' THEN          -- If it's pure digits, treat as Unix timestamp (seconds since 1970-01-01)\n                TO_TIMESTAMP(timestamp_datetime::BIGINT)    -- PostgreSQL's TO_TIMESTAMP can parse various formats, but we use the simplest approach\n            ELSE \n                timestamp_datetime::TIMESTAMP               -- Otherwise, try to parse as ISO format string\n        END AS fill_timestamp,\n        \n        transactionHash AS transaction_hash,\n        blockchain,\n        source_file,\n        \n        -- ============================================================\n        -- INDEXED FIELDS (from topics) - Already decoded by ETL\n        -- ============================================================\n        -- These come from the event's \"topics\" array, which is indexed for fast blockchain queries\n        \n        -- Origin chain ID: Which blockchain the funds came FROM\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\n        -- Example: 1 = Ethereum, 42161 = Arbitrum, 8453 = Base\n        -- Cast via NUMERIC first to handle decimal strings like \"8453.0\" from ETL\n        (topic_origin_chain_id::NUMERIC)::BIGINT AS origin_chain_id,\n        \n        -- Deposit ID: Unique identifier linking this fill to its original deposit\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\n        -- This is the KEY that connects deposits \u2194 fills across chains\n        -- Cast via NUMERIC first to handle decimal strings like \"12345.0\" from ETL\n        (topic_deposit_id::NUMERIC)::BIGINT AS deposit_id,\n        \n        -- Relayer address: Who provided the liquidity\n        -- Already decoded by ETL to proper address format (0x...)\n        topic_relayer AS relayer_address,\n        \n        -- ============================================================\n        -- TOKEN INFORMATION (what was bridged)\n        -- ============================================================\n        \n        -- Input token: The token address on the origin chain\n        -- Already decoded by ETL to proper address format\n        -- Example: 0xA0b86991c6218b36c1d19D4a2e9Eb0cE3606eB48 = USDC on Ethereum\n        -- Normalize to lowercase for joining with token metadata\n        LOWER(filled_relay_data_input_token) AS input_token_address,\n        \n        -- Output token: The token address on the destination chain (Polygon in this case)\n        -- Already decoded by ETL to proper address format\n        -- Normalize to lowercase for joining with token metadata\n        LOWER(filled_relay_data_output_token) AS output_token_address,\n        \n        -- ============================================================\n        -- AMOUNT INFORMATION (how much was bridged)\n        -- ============================================================\n        \n        -- Input amount: How much was sent from origin chain\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\n        -- Store as RAW amount first (before rescaling by decimals)\n        filled_relay_data_input_amount::NUMERIC AS input_amount_raw,\n        \n        -- Output amount: How much was received on destination chain\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\n        -- Usually slightly less than input due to fees/spread\n        -- Store as RAW amount first (before rescaling by decimals)\n        filled_relay_data_output_amount::NUMERIC AS output_amount_raw,\n        \n        -- ============================================================\n        -- RELAYER & ROUTING INFORMATION\n        -- ============================================================\n        \n        -- Repayment chain ID: Where the relayer gets reimbursed\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\n        -- Relayers front capital, then get paid back (often on a different chain)\n        -- Cast via NUMERIC first to handle decimal strings like \"1.0\" from ETL\n        (filled_relay_data_repayment_chain_id::NUMERIC)::BIGINT AS repayment_chain_id,\n        \n        -- Exclusive relayer: If set, only this address can fill this deposit\n        -- Already decoded by ETL to proper address format\n        -- Used for priority/guaranteed fills (NULL if no exclusive relayer)\n        filled_relay_data_exclusive_relayer AS exclusive_relayer_address,\n        \n        -- ============================================================\n        -- USER INFORMATION\n        -- ============================================================\n        \n        -- Depositor: The original user who initiated the bridge\n        -- Already decoded by ETL to proper address format\n        filled_relay_data_depositor AS depositor_address,\n        \n        -- Recipient: Who receives the funds on the destination chain\n        -- Already decoded by ETL to proper address format\n        -- Usually the same as depositor, but can be different (gift/transfer)\n        filled_relay_data_recipient AS recipient_address,\n        \n        -- ============================================================\n        -- GAS DATA (for relayer cost analysis)\n        -- ============================================================\n        gas_price_wei::BIGINT AS gas_price_wei,\n        gas_used::BIGINT AS gas_used\n        \n    FROM raw_fills\n)\n\n-- ============================================================\n-- FINAL SELECT: Join with token metadata and rescale amounts\n-- ============================================================\nSELECT\n    -- Event identity\n    c.fill_timestamp,\n    c.transaction_hash,\n    c.blockchain,\n    c.source_file,\n    \n    -- Indexed fields\n    c.origin_chain_id,\n    c.deposit_id,\n    c.relayer_address,\n    \n    -- Token info WITH NAMES\n    -- Input token (what was deposited on origin chain)\n    c.input_token_address,\n    input_tok.token_symbol AS input_token_symbol,        -- e.g., 'USDC', 'WETH'\n    \n    -- Output token (what was received on destination chain - Polygon)\n    c.output_token_address,\n    output_tok.token_symbol AS output_token_symbol,      -- e.g., 'USDC', 'WETH'\n    \n    -- Rescaled amounts (human-readable)\n    -- These use the rescale_amount macro which divides raw amount by 10^decimals\n    -- Example: 5000000 raw USDC (6 decimals) \u2192 5.0 USDC\n    \n    c.input_amount_raw / POWER(10, COALESCE(input_tok.decimals, 18))\n AS input_amount,\n    \n    c.output_amount_raw / POWER(10, COALESCE(output_tok.decimals, 18))\n AS output_amount,\n    \n    -- Raw amounts (preserved for auditing)\n    -- These are the original blockchain values before rescaling\n    -- Example: 5.0 USDC is stored as 5000000 on blockchain\n    c.input_amount_raw,\n    c.output_amount_raw,\n    \n    -- Relayer routing\n    c.repayment_chain_id,\n    c.exclusive_relayer_address,\n    \n    -- User info\n    c.depositor_address,\n    c.recipient_address,\n    \n    -- Gas data (for relayer cost analysis)\n    c.gas_price_wei,\n    c.gas_used,\n    (c.gas_price_wei * c.gas_used) AS gas_cost_wei\n    \nFROM cleaned_fills c\n\n-- Join with token metadata to get decimals and symbols for INPUT token\n-- Join on BOTH address AND chain_id to match the correct origin chain\nLEFT JOIN input_token_meta AS input_tok\n    ON c.input_token_address = input_tok.token_address\n    AND c.origin_chain_id = input_tok.chain_id\n\n-- Join with token metadata to get decimals and symbols for OUTPUT token\n-- Join on address only - we already filtered to destination chain (Polygon)\nLEFT JOIN output_token_meta AS output_tok\n    ON c.output_token_address = output_tok.token_address\n\n-- Data quality: Only include rows with essential fields populated\nWHERE c.deposit_id IS NOT NULL\n    AND c.relayer_address IS NOT NULL\n    AND c.input_token_address IS NOT NULL\n    AND c.output_token_address IS NOT NULL\n    AND c.input_amount_raw IS NOT NULL\n    AND c.output_amount_raw IS NOT NULL", "relation_name": "\"across_analytics\".\"dbt_staging\".\"stg_polygon__fills\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2025-12-30T08:46:01.028290Z", "completed_at": "2025-12-30T08:46:01.043248Z"}, {"name": "execute", "started_at": "2025-12-30T08:46:01.060004Z", "completed_at": "2025-12-30T08:46:01.345558Z"}], "thread_id": "Thread-2 (worker)", "execution_time": 0.33325815200805664, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.stg_polygon__refunds", "compiled": true, "compiled_code": "-- Staging model for ExecutedRelayerRefundRoot events from Polygon\r\n\r\nWITH raw_refunds AS (\r\n\r\n    SELECT\r\n        -- Core event metadata - these identify WHEN and WHERE the event happened\r\n        timestamp_datetime,              -- When the event occurred (from blockchain)\r\n        transactionHash,                 -- Unique transaction identifier\r\n        blockchain,                      -- Which blockchain (should be 'polygon')\r\n        source_file,                     -- Which source file this came from (for lineage)\r\n        \r\n        -- Indexed fields from topics (these are searchable/filterable in blockchain)\r\n        -- Topics are like \"indexed columns\" in SQL - they're stored separately for fast queries\r\n        topic_chain_id,                  -- Chain ID where the refund was executed\r\n        topic_root_bundle_id,            -- Merkle root bundle identifier (groups multiple refunds)\r\n        topic_leaf_id,                   -- Leaf index in the merkle tree (identifies this specific refund batch)\r\n        \r\n        -- Non-indexed fields from the event's data field\r\n        -- These are the actual business data about the refund\r\n        amount_to_return,                -- Total amount of capital returned in this batch\r\n        l2_token_address,                -- Token address being refunded (on Polygon)\r\n        refund_amounts,                  -- Comma-separated list of individual refund amounts\r\n        refund_addresses,                -- Comma-separated list of relayer addresses receiving refunds\r\n        refund_count                     -- Number of relayers being refunded in this batch\r\n        \r\n    FROM raw.polygon_logs_processed\r\n    \r\n    -- Filter: Only include rows where ExecutedRelayerRefundRoot data exists\r\n    -- Topic_0 is the event signature hash that identifies the event type\r\n    WHERE topic_0 = '0xf4ad92585b1bc117fbdd644990adf0827bc4c95baeae8a23322af807b6d0020e'\r\n        AND amount_to_return IS NOT NULL\r\n        AND l2_token_address IS NOT NULL\r\n        AND refund_count IS NOT NULL\r\n),\r\n\r\ncleaned_refunds AS (\r\n    SELECT\r\n        \r\n        -- Timestamp: Convert text to proper timestamp type\r\n        CASE \r\n            WHEN timestamp_datetime ~ '^\\d+$' THEN          -- If it's pure digits, treat as Unix timestamp (seconds since 1970-01-01)\r\n                TO_TIMESTAMP(timestamp_datetime::BIGINT)    -- PostgreSQL's TO_TIMESTAMP converts Unix seconds to timestamp\r\n            ELSE \r\n                timestamp_datetime::TIMESTAMP               -- Otherwise, try to parse as ISO format string (e.g., \"2025-12-03 10:30:00\")\r\n        END AS refund_timestamp,\r\n        \r\n        transactionHash AS transaction_hash,\r\n        blockchain,\r\n        source_file,\r\n        \r\n        -- ============================================================\r\n        -- INDEXED FIELDS (from topics) - Already decoded by ETL\r\n        -- ============================================================\r\n        -- These come from the event's \"topics\" array, which is indexed for fast blockchain queries\r\n        \r\n        -- Chain ID: Which blockchain this refund was executed on\r\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\r\n        -- Example: 42161 = Arbitrum, 1 = Ethereum, 137 = Polygon\r\n        -- Cast via NUMERIC first to handle decimal strings like \"42161.0\" from ETL\r\n        (topic_chain_id::NUMERIC)::BIGINT AS chain_id,\r\n        \r\n        -- Root bundle ID: Groups multiple refunds together in a merkle tree\r\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\r\n        -- Multiple refund batches can be grouped into one merkle root for gas efficiency\r\n        -- Cast via NUMERIC first to handle decimal strings like \"12345.0\" from ETL\r\n        (topic_root_bundle_id::NUMERIC)::BIGINT AS root_bundle_id,\r\n        \r\n        -- Leaf ID: Identifies this specific refund batch within the merkle tree\r\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\r\n        -- Each leaf represents one batch of refunds to multiple relayers\r\n        -- Cast via NUMERIC first to handle decimal strings like \"789.0\" from ETL\r\n        (topic_leaf_id::NUMERIC)::BIGINT AS leaf_id,\r\n        \r\n        -- ============================================================\r\n        -- TOKEN INFORMATION (what was refunded)\r\n        -- ============================================================\r\n        \r\n        -- L2 token address: The token being refunded on Polygon\r\n        -- Already decoded by ETL to proper address format (0x...)\r\n        -- Example: 0x2791Bca1f2de4661ED88A30C99A7a9449Aa84174 = USDC on Polygon\r\n        l2_token_address AS refund_token_address,\r\n        \r\n        -- ============================================================\r\n        -- AMOUNT INFORMATION (how much was refunded)\r\n        -- ============================================================\r\n        \r\n        -- Amount to return: Total capital returned in this batch\r\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\r\n        -- This is the sum of all individual refund amounts in the batch\r\n        -- Stored as NUMERIC to handle large token amounts (18 decimals)\r\n        amount_to_return::NUMERIC AS total_refund_amount,\r\n        \r\n        -- Refund amounts: Individual refund amounts as comma-separated string\r\n        -- Format: \"1000000000000000000,2000000000000000000,3000000000000000000\"\r\n        -- Each value is in token's base units (e.g., wei for ETH, 6 decimals for USDC)\r\n        -- This is a comma-separated string because the ETL processes dynamic arrays\r\n        refund_amounts AS refund_amounts_string,\r\n        \r\n        -- Refund addresses: Relayer addresses receiving refunds as comma-separated string\r\n        -- Format: \"0xAAA...,0xBBB...,0xCCC...\"\r\n        -- Matches 1:1 with refund_amounts (first address gets first amount, etc.)\r\n        -- This is a comma-separated string because the ETL processes dynamic arrays\r\n        refund_addresses AS refund_addresses_string,\r\n        \r\n        -- Refund count: Number of relayers being refunded in this batch\r\n        -- Already converted by ETL to integer\r\n        -- This tells us how many individual refunds are in the arrays above\r\n        refund_count::INTEGER AS refund_count\r\n        \r\n    FROM raw_refunds\r\n)\r\n\r\n-- Final SELECT: Add any computed fields and ensure data quality\r\nSELECT\r\n    -- Event identity\r\n    refund_timestamp,\r\n    transaction_hash,\r\n    blockchain,\r\n    source_file,\r\n    \r\n    -- Indexed fields (merkle tree identifiers)\r\n    chain_id,\r\n    root_bundle_id,\r\n    leaf_id,\r\n    \r\n    -- Token info\r\n    refund_token_address,\r\n    \r\n    -- Amounts\r\n    total_refund_amount,\r\n    refund_amounts_string,\r\n    refund_addresses_string,\r\n    refund_count\r\n    \r\nFROM cleaned_refunds\r\n\r\n-- Data quality: Only include rows with essential fields populated\r\nWHERE chain_id IS NOT NULL\r\n    AND refund_token_address IS NOT NULL\r\n    AND total_refund_amount IS NOT NULL\r\n    AND refund_count IS NOT NULL", "relation_name": "\"across_analytics\".\"dbt_staging\".\"stg_polygon__refunds\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2025-12-30T08:46:01.043989Z", "completed_at": "2025-12-30T08:46:01.084968Z"}, {"name": "execute", "started_at": "2025-12-30T08:46:01.089553Z", "completed_at": "2025-12-30T08:46:01.350031Z"}], "thread_id": "Thread-4 (worker)", "execution_time": 0.3200967311859131, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.stg_unichain__deposits", "compiled": true, "compiled_code": "-- Staging model for FundsDeposited events from Unichain\n-- This model extracts deposit events where users initiate cross-chain bridge transactions\n\nWITH raw_deposits AS (\n\n    SELECT\n        timestamp_datetime,\n        transactionHash,\n        blockchain,\n        source_file,\n        topic_destination_chain_id,\n        topic_deposit_id,\n        topic_depositor,\n        funds_deposited_data_input_token,\n        funds_deposited_data_output_token,\n        funds_deposited_data_input_amount,\n        funds_deposited_data_output_amount,\n        funds_deposited_data_recipient\n        \n    FROM raw.unichain_logs_processed\n    \n    WHERE topic_0 = '0x32ed1a409ef04c7b0227189c3a103dc5ac10e775a15b785dcc510201f7c25ad3'\n        AND funds_deposited_data_input_amount IS NOT NULL\n        AND funds_deposited_data_output_amount IS NOT NULL\n),\n\n-- INPUT token metadata: filtered to origin chain (Unichain)\n-- Note: unichain may not have tokens in token_metadata yet\ninput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n    WHERE chain = 'unichain'\n\n),\n\n-- OUTPUT token metadata: includes chain_id for matching with destination_chain_id\noutput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals,\n        chain_id\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n\n),\n\ncleaned_deposits AS (\n    SELECT\n        CASE \n            WHEN timestamp_datetime ~ '^\\d+$' THEN\n                TO_TIMESTAMP(timestamp_datetime::BIGINT)\n            ELSE \n                timestamp_datetime::TIMESTAMP\n        END AS deposit_timestamp,\n        \n        transactionHash AS transaction_hash,\n        blockchain,\n        source_file,\n        (topic_destination_chain_id::NUMERIC)::BIGINT AS destination_chain_id,\n        (topic_deposit_id::NUMERIC)::BIGINT AS deposit_id,\n        topic_depositor AS depositor_address,\n        LOWER(funds_deposited_data_input_token) AS input_token_address,\n        LOWER(funds_deposited_data_output_token) AS output_token_address,\n        funds_deposited_data_input_amount::NUMERIC AS input_amount_raw,\n        funds_deposited_data_output_amount::NUMERIC AS output_amount_raw,\n        funds_deposited_data_recipient AS recipient_address\n        \n    FROM raw_deposits\n)\n\nSELECT\n    c.deposit_timestamp,\n    c.transaction_hash,\n    c.blockchain,\n    c.source_file,\n    c.destination_chain_id,\n    c.deposit_id,\n    c.depositor_address,\n    c.input_token_address,\n    input_tok.token_symbol AS input_token_symbol,\n    c.output_token_address,\n    output_tok.token_symbol AS output_token_symbol,\n    \n    c.input_amount_raw / POWER(10, COALESCE(input_tok.decimals, 18))\n AS input_amount,\n    \n    CASE \n        WHEN c.output_token_address = '0x0000000000000000000000000000000000000000' \n        THEN c.output_amount_raw / POWER(10, COALESCE(input_tok.decimals, 18))\n        ELSE c.output_amount_raw / POWER(10, COALESCE(output_tok.decimals, 18))\n    END\n AS output_amount,\n    c.input_amount_raw,\n    c.output_amount_raw,\n    c.recipient_address\n    \nFROM cleaned_deposits c\n\nLEFT JOIN input_token_meta AS input_tok\n    ON c.input_token_address = input_tok.token_address\n\nLEFT JOIN output_token_meta AS output_tok\n    ON c.output_token_address = output_tok.token_address\n    AND c.destination_chain_id = output_tok.chain_id\n\nWHERE c.deposit_id IS NOT NULL\n    AND c.depositor_address IS NOT NULL\n    AND c.input_token_address IS NOT NULL\n    AND c.output_token_address IS NOT NULL\n    AND c.input_amount_raw IS NOT NULL\n    AND c.output_amount_raw IS NOT NULL", "relation_name": "\"across_analytics\".\"dbt_staging\".\"stg_unichain__deposits\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2025-12-30T08:46:01.071862Z", "completed_at": "2025-12-30T08:46:01.096236Z"}, {"name": "execute", "started_at": "2025-12-30T08:46:01.099549Z", "completed_at": "2025-12-30T08:46:01.365807Z"}], "thread_id": "Thread-3 (worker)", "execution_time": 0.31772804260253906, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.stg_unichain__fills", "compiled": true, "compiled_code": "-- Staging model for FilledRelay events from Unichain\n\nWITH raw_fills AS (\n    SELECT\n        timestamp_datetime, transactionHash, blockchain, source_file,\n        topic_origin_chain_id, topic_deposit_id, topic_relayer,\n        filled_relay_data_input_token, filled_relay_data_output_token,\n        filled_relay_data_input_amount, filled_relay_data_output_amount,\n        filled_relay_data_repayment_chain_id, filled_relay_data_exclusive_relayer,\n        filled_relay_data_depositor, filled_relay_data_recipient,\n        gas_price_wei, gas_used\n    FROM raw.unichain_logs_processed\n    WHERE topic_0 = '0x44b559f101f8fbcc8a0ea43fa91a05a729a5ea6e14a7c75aa750374690137208'\n        AND filled_relay_data_input_amount IS NOT NULL\n        AND filled_relay_data_output_amount IS NOT NULL\n),\n\ninput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals,\n        chain_id\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n\n),\n\noutput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n    WHERE chain = 'unichain'\n\n),\n\ncleaned_fills AS (\n    SELECT\n        CASE \n            WHEN timestamp_datetime ~ '^\\d+$' THEN TO_TIMESTAMP(timestamp_datetime::BIGINT)\n            ELSE timestamp_datetime::TIMESTAMP\n        END AS fill_timestamp,\n        transactionHash AS transaction_hash,\n        blockchain, source_file,\n        (topic_origin_chain_id::NUMERIC)::BIGINT AS origin_chain_id,\n        (topic_deposit_id::NUMERIC)::BIGINT AS deposit_id,\n        topic_relayer AS relayer_address,\n        LOWER(filled_relay_data_input_token) AS input_token_address,\n        LOWER(filled_relay_data_output_token) AS output_token_address,\n        filled_relay_data_input_amount::NUMERIC AS input_amount_raw,\n        filled_relay_data_output_amount::NUMERIC AS output_amount_raw,\n        (filled_relay_data_repayment_chain_id::NUMERIC)::BIGINT AS repayment_chain_id,\n        filled_relay_data_exclusive_relayer AS exclusive_relayer_address,\n        filled_relay_data_depositor AS depositor_address,\n        filled_relay_data_recipient AS recipient_address,\n        gas_price_wei::BIGINT AS gas_price_wei,\n        gas_used::BIGINT AS gas_used\n    FROM raw_fills\n)\n\nSELECT\n    c.fill_timestamp, c.transaction_hash, c.blockchain, c.source_file,\n    c.origin_chain_id, c.deposit_id, c.relayer_address,\n    c.input_token_address, input_tok.token_symbol AS input_token_symbol,\n    c.output_token_address, output_tok.token_symbol AS output_token_symbol,\n    \n    c.input_amount_raw / POWER(10, COALESCE(input_tok.decimals, 18))\n AS input_amount,\n    \n    c.output_amount_raw / POWER(10, COALESCE(output_tok.decimals, 18))\n AS output_amount,\n    c.input_amount_raw, c.output_amount_raw,\n    c.repayment_chain_id, c.exclusive_relayer_address,\n    c.depositor_address, c.recipient_address,\n    c.gas_price_wei, c.gas_used, (c.gas_price_wei * c.gas_used) AS gas_cost_wei\nFROM cleaned_fills c\nLEFT JOIN input_token_meta AS input_tok\n    ON c.input_token_address = input_tok.token_address AND c.origin_chain_id = input_tok.chain_id\nLEFT JOIN output_token_meta AS output_tok\n    ON c.output_token_address = output_tok.token_address\nWHERE c.deposit_id IS NOT NULL AND c.relayer_address IS NOT NULL\n    AND c.input_token_address IS NOT NULL AND c.output_token_address IS NOT NULL\n    AND c.input_amount_raw IS NOT NULL AND c.output_amount_raw IS NOT NULL", "relation_name": "\"across_analytics\".\"dbt_staging\".\"stg_unichain__fills\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2025-12-30T08:46:01.130470Z", "completed_at": "2025-12-30T08:46:01.144237Z"}, {"name": "execute", "started_at": "2025-12-30T08:46:01.147733Z", "completed_at": "2025-12-30T08:46:01.416962Z"}], "thread_id": "Thread-1 (worker)", "execution_time": 0.2942841053009033, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.stg_unichain__refunds", "compiled": true, "compiled_code": "-- Staging model for ExecutedRelayerRefundRoot events from Unichain\r\n\r\nWITH raw_refunds AS (\r\n\r\n    SELECT\r\n        -- Core event metadata - these identify WHEN and WHERE the event happened\r\n        timestamp_datetime,              -- When the event occurred (from blockchain)\r\n        transactionHash,                 -- Unique transaction identifier\r\n        blockchain,                      -- Which blockchain (should be 'unichain')\r\n        source_file,                     -- Which source file this came from (for lineage)\r\n        \r\n        -- Indexed fields from topics (these are searchable/filterable in blockchain)\r\n        -- Topics are like \"indexed columns\" in SQL - they're stored separately for fast queries\r\n        topic_chain_id,                  -- Chain ID where the refund was executed\r\n        topic_root_bundle_id,            -- Merkle root bundle identifier (groups multiple refunds)\r\n        topic_leaf_id,                   -- Leaf index in the merkle tree (identifies this specific refund batch)\r\n        \r\n        -- Non-indexed fields from the event's data field\r\n        -- These are the actual business data about the refund\r\n        amount_to_return,                -- Total amount of capital returned in this batch\r\n        l2_token_address,                -- Token address being refunded (on Unichain)\r\n        refund_amounts,                  -- Comma-separated list of individual refund amounts\r\n        refund_addresses,                -- Comma-separated list of relayer addresses receiving refunds\r\n        refund_count                     -- Number of relayers being refunded in this batch\r\n        \r\n    FROM raw.unichain_logs_processed\r\n    \r\n    -- Filter: Only include rows where ExecutedRelayerRefundRoot data exists\r\n    -- Topic_0 is the event signature hash that identifies the event type\r\n    WHERE topic_0 = '0xf4ad92585b1bc117fbdd644990adf0827bc4c95baeae8a23322af807b6d0020e'\r\n        AND amount_to_return IS NOT NULL\r\n        AND l2_token_address IS NOT NULL\r\n        AND refund_count IS NOT NULL\r\n),\r\n\r\ncleaned_refunds AS (\r\n    SELECT\r\n        \r\n        -- Timestamp: Convert text to proper timestamp type\r\n        CASE \r\n            WHEN timestamp_datetime ~ '^\\d+$' THEN          -- If it's pure digits, treat as Unix timestamp (seconds since 1970-01-01)\r\n                TO_TIMESTAMP(timestamp_datetime::BIGINT)    -- PostgreSQL's TO_TIMESTAMP converts Unix seconds to timestamp\r\n            ELSE \r\n                timestamp_datetime::TIMESTAMP               -- Otherwise, try to parse as ISO format string (e.g., \"2025-12-03 10:30:00\")\r\n        END AS refund_timestamp,\r\n        \r\n        transactionHash AS transaction_hash,\r\n        blockchain,\r\n        source_file,\r\n        \r\n        -- ============================================================\r\n        -- INDEXED FIELDS (from topics) - Already decoded by ETL\r\n        -- ============================================================\r\n        -- These come from the event's \"topics\" array, which is indexed for fast blockchain queries\r\n        \r\n        -- Chain ID: Which blockchain this refund was executed on\r\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\r\n        -- Example: 42161 = Arbitrum, 1 = Ethereum, 130 = Unichain\r\n        -- Cast via NUMERIC first to handle decimal strings like \"42161.0\" from ETL\r\n        (topic_chain_id::NUMERIC)::BIGINT AS chain_id,\r\n        \r\n        -- Root bundle ID: Groups multiple refunds together in a merkle tree\r\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\r\n        -- Multiple refund batches can be grouped into one merkle root for gas efficiency\r\n        -- Cast via NUMERIC first to handle decimal strings like \"12345.0\" from ETL\r\n        (topic_root_bundle_id::NUMERIC)::BIGINT AS root_bundle_id,\r\n        \r\n        -- Leaf ID: Identifies this specific refund batch within the merkle tree\r\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\r\n        -- Each leaf represents one batch of refunds to multiple relayers\r\n        -- Cast via NUMERIC first to handle decimal strings like \"789.0\" from ETL\r\n        (topic_leaf_id::NUMERIC)::BIGINT AS leaf_id,\r\n        \r\n        -- ============================================================\r\n        -- TOKEN INFORMATION (what was refunded)\r\n        -- ============================================================\r\n        \r\n        -- L2 token address: The token being refunded on Unichain\r\n        -- Already decoded by ETL to proper address format (0x...)\r\n        -- This is the token address on the Unichain\r\n        l2_token_address AS refund_token_address,\r\n        \r\n        -- ============================================================\r\n        -- AMOUNT INFORMATION (how much was refunded)\r\n        -- ============================================================\r\n        \r\n        -- Amount to return: Total capital returned in this batch\r\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\r\n        -- This is the sum of all individual refund amounts in the batch\r\n        -- Stored as NUMERIC to handle large token amounts (18 decimals)\r\n        amount_to_return::NUMERIC AS total_refund_amount,\r\n        \r\n        -- Refund amounts: Individual refund amounts as comma-separated string\r\n        -- Format: \"1000000000000000000,2000000000000000000,3000000000000000000\"\r\n        -- Each value is in token's base units (e.g., wei for ETH, 6 decimals for USDC)\r\n        -- This is a comma-separated string because the ETL processes dynamic arrays\r\n        refund_amounts AS refund_amounts_string,\r\n        \r\n        -- Refund addresses: Relayer addresses receiving refunds as comma-separated string\r\n        -- Format: \"0xAAA...,0xBBB...,0xCCC...\"\r\n        -- Matches 1:1 with refund_amounts (first address gets first amount, etc.)\r\n        -- This is a comma-separated string because the ETL processes dynamic arrays\r\n        refund_addresses AS refund_addresses_string,\r\n        \r\n        -- Refund count: Number of relayers being refunded in this batch\r\n        -- Already converted by ETL to integer\r\n        -- This tells us how many individual refunds are in the arrays above\r\n        refund_count::INTEGER AS refund_count\r\n        \r\n    FROM raw_refunds\r\n)\r\n\r\n-- Final SELECT: Add any computed fields and ensure data quality\r\nSELECT\r\n    -- Event identity\r\n    refund_timestamp,\r\n    transaction_hash,\r\n    blockchain,\r\n    source_file,\r\n    \r\n    -- Indexed fields (merkle tree identifiers)\r\n    chain_id,\r\n    root_bundle_id,\r\n    leaf_id,\r\n    \r\n    -- Token info\r\n    refund_token_address,\r\n    \r\n    -- Amounts\r\n    total_refund_amount,\r\n    refund_amounts_string,\r\n    refund_addresses_string,\r\n    refund_count\r\n    \r\nFROM cleaned_refunds\r\n\r\n-- Data quality: Only include rows with essential fields populated\r\nWHERE chain_id IS NOT NULL\r\n    AND refund_token_address IS NOT NULL\r\n    AND total_refund_amount IS NOT NULL\r\n    AND refund_count IS NOT NULL", "relation_name": "\"across_analytics\".\"dbt_staging\".\"stg_unichain__refunds\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2025-12-30T08:46:01.418253Z", "completed_at": "2025-12-30T08:46:01.467950Z"}, {"name": "execute", "started_at": "2025-12-30T08:46:01.493778Z", "completed_at": "2025-12-30T08:46:01.777607Z"}], "thread_id": "Thread-3 (worker)", "execution_time": 0.379732608795166, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.stg_worldchain__refunds", "compiled": true, "compiled_code": "-- Staging model for ExecutedRelayerRefundRoot events from Worldchain\r\n\r\nWITH raw_refunds AS (\r\n\r\n    SELECT\r\n        -- Core event metadata - these identify WHEN and WHERE the event happened\r\n        timestamp_datetime,              -- When the event occurred (from blockchain)\r\n        transactionHash,                 -- Unique transaction identifier\r\n        blockchain,                      -- Which blockchain (should be 'worldchain')\r\n        source_file,                     -- Which source file this came from (for lineage)\r\n        \r\n        -- Indexed fields from topics (these are searchable/filterable in blockchain)\r\n        -- Topics are like \"indexed columns\" in SQL - they're stored separately for fast queries\r\n        topic_chain_id,                  -- Chain ID where the refund was executed\r\n        topic_root_bundle_id,            -- Merkle root bundle identifier (groups multiple refunds)\r\n        topic_leaf_id,                   -- Leaf index in the merkle tree (identifies this specific refund batch)\r\n        \r\n        -- Non-indexed fields from the event's data field\r\n        -- These are the actual business data about the refund\r\n        amount_to_return,                -- Total amount of capital returned in this batch\r\n        l2_token_address,                -- Token address being refunded (on Worldchain)\r\n        refund_amounts,                  -- Comma-separated list of individual refund amounts\r\n        refund_addresses,                -- Comma-separated list of relayer addresses receiving refunds\r\n        refund_count                     -- Number of relayers being refunded in this batch\r\n        \r\n    FROM raw.worldchain_logs_processed\r\n    \r\n    -- Filter: Only include rows where ExecutedRelayerRefundRoot data exists\r\n    -- Topic_0 is the event signature hash that identifies the event type\r\n    WHERE topic_0 = '0xf4ad92585b1bc117fbdd644990adf0827bc4c95baeae8a23322af807b6d0020e'\r\n        AND amount_to_return IS NOT NULL\r\n        AND l2_token_address IS NOT NULL\r\n        AND refund_count IS NOT NULL\r\n),\r\n\r\ncleaned_refunds AS (\r\n    SELECT\r\n        \r\n        -- Timestamp: Convert text to proper timestamp type\r\n        CASE \r\n            WHEN timestamp_datetime ~ '^\\d+$' THEN          -- If it's pure digits, treat as Unix timestamp (seconds since 1970-01-01)\r\n                TO_TIMESTAMP(timestamp_datetime::BIGINT)    -- PostgreSQL's TO_TIMESTAMP converts Unix seconds to timestamp\r\n            ELSE \r\n                timestamp_datetime::TIMESTAMP               -- Otherwise, try to parse as ISO format string (e.g., \"2025-12-03 10:30:00\")\r\n        END AS refund_timestamp,\r\n        \r\n        transactionHash AS transaction_hash,\r\n        blockchain,\r\n        source_file,\r\n        \r\n        -- ============================================================\r\n        -- INDEXED FIELDS (from topics) - Already decoded by ETL\r\n        -- ============================================================\r\n        -- These come from the event's \"topics\" array, which is indexed for fast blockchain queries\r\n        \r\n        -- Chain ID: Which blockchain this refund was executed on\r\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\r\n        -- Example: 42161 = Arbitrum, 1 = Ethereum, 480 = Worldchain\r\n        -- Cast via NUMERIC first to handle decimal strings like \"42161.0\" from ETL\r\n        (topic_chain_id::NUMERIC)::BIGINT AS chain_id,\r\n        \r\n        -- Root bundle ID: Groups multiple refunds together in a merkle tree\r\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\r\n        -- Multiple refund batches can be grouped into one merkle root for gas efficiency\r\n        -- Cast via NUMERIC first to handle decimal strings like \"12345.0\" from ETL\r\n        (topic_root_bundle_id::NUMERIC)::BIGINT AS root_bundle_id,\r\n        \r\n        -- Leaf ID: Identifies this specific refund batch within the merkle tree\r\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\r\n        -- Each leaf represents one batch of refunds to multiple relayers\r\n        -- Cast via NUMERIC first to handle decimal strings like \"789.0\" from ETL\r\n        (topic_leaf_id::NUMERIC)::BIGINT AS leaf_id,\r\n        \r\n        -- ============================================================\r\n        -- TOKEN INFORMATION (what was refunded)\r\n        -- ============================================================\r\n        \r\n        -- L2 token address: The token being refunded on Worldchain\r\n        -- Already decoded by ETL to proper address format (0x...)\r\n        -- This is the token address on the Worldchain\r\n        l2_token_address AS refund_token_address,\r\n        \r\n        -- ============================================================\r\n        -- AMOUNT INFORMATION (how much was refunded)\r\n        -- ============================================================\r\n        \r\n        -- Amount to return: Total capital returned in this batch\r\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\r\n        -- This is the sum of all individual refund amounts in the batch\r\n        -- Stored as NUMERIC to handle large token amounts (18 decimals)\r\n        amount_to_return::NUMERIC AS total_refund_amount,\r\n        \r\n        -- Refund amounts: Individual refund amounts as comma-separated string\r\n        -- Format: \"1000000000000000000,2000000000000000000,3000000000000000000\"\r\n        -- Each value is in token's base units (e.g., wei for ETH, 6 decimals for USDC)\r\n        -- This is a comma-separated string because the ETL processes dynamic arrays\r\n        refund_amounts AS refund_amounts_string,\r\n        \r\n        -- Refund addresses: Relayer addresses receiving refunds as comma-separated string\r\n        -- Format: \"0xAAA...,0xBBB...,0xCCC...\"\r\n        -- Matches 1:1 with refund_amounts (first address gets first amount, etc.)\r\n        -- This is a comma-separated string because the ETL processes dynamic arrays\r\n        refund_addresses AS refund_addresses_string,\r\n        \r\n        -- Refund count: Number of relayers being refunded in this batch\r\n        -- Already converted by ETL to integer\r\n        -- This tells us how many individual refunds are in the arrays above\r\n        refund_count::INTEGER AS refund_count\r\n        \r\n    FROM raw_refunds\r\n)\r\n\r\n-- Final SELECT: Add any computed fields and ensure data quality\r\nSELECT\r\n    -- Event identity\r\n    refund_timestamp,\r\n    transaction_hash,\r\n    blockchain,\r\n    source_file,\r\n    \r\n    -- Indexed fields (merkle tree identifiers)\r\n    chain_id,\r\n    root_bundle_id,\r\n    leaf_id,\r\n    \r\n    -- Token info\r\n    refund_token_address,\r\n    \r\n    -- Amounts\r\n    total_refund_amount,\r\n    refund_amounts_string,\r\n    refund_addresses_string,\r\n    refund_count\r\n    \r\nFROM cleaned_refunds\r\n\r\n-- Data quality: Only include rows with essential fields populated\r\nWHERE chain_id IS NOT NULL\r\n    AND refund_token_address IS NOT NULL\r\n    AND total_refund_amount IS NOT NULL\r\n    AND refund_count IS NOT NULL", "relation_name": "\"across_analytics\".\"dbt_staging\".\"stg_worldchain__refunds\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2025-12-30T08:46:01.404514Z", "completed_at": "2025-12-30T08:46:01.465271Z"}, {"name": "execute", "started_at": "2025-12-30T08:46:01.483343Z", "completed_at": "2025-12-30T08:46:01.871241Z"}], "thread_id": "Thread-4 (worker)", "execution_time": 0.48047685623168945, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.stg_worldchain__fills", "compiled": true, "compiled_code": "-- Staging model for FilledRelay events from Worldchain\n-- This model extracts fill events where relayers complete cross-chain bridge transactions\n\nWITH raw_fills AS (\n\n    SELECT\n        -- Core event metadata - these identify WHEN and WHERE the event happened\n        timestamp_datetime,              -- When the event occurred (from blockchain)\n        transactionHash,                 -- Unique transaction identifier\n        blockchain,                      -- Which blockchain (should be 'worldchain')\n        source_file,                     -- Which source file this came from (for lineage)\n        \n        -- Indexed fields from topics (these are searchable/filterable in blockchain)\n        -- Topics are like \"indexed columns\" in SQL - they're stored separately for fast queries\n        topic_origin_chain_id,           -- Chain ID where the original deposit happened\n        topic_deposit_id,                -- Unique deposit identifier (links to deposit event)\n        topic_relayer,                   -- Address of the relayer who provided liquidity\n        \n        -- Non-indexed fields from the event's data field\n        -- These are the actual business data about the fill\n        filled_relay_data_input_token,          -- Token address on origin chain\n        filled_relay_data_output_token,          -- Token address on destination chain\n        filled_relay_data_input_amount,          -- Amount sent (in origin token units)\n        filled_relay_data_output_amount,         -- Amount received (in destination token units)\n        filled_relay_data_repayment_chain_id,     -- Where relayer gets reimbursed\n        filled_relay_data_exclusive_relayer,     -- Address with exclusive fill rights (if any)\n        filled_relay_data_depositor,             -- Original user who initiated the bridge\n        filled_relay_data_recipient,              -- Final recipient of the bridged funds\n        \n        -- Gas data (for relayer cost analysis)\n        gas_price_wei,\n        gas_used\n        \n    FROM raw.worldchain_logs_processed\n    \n    -- Filter: Only include rows where FilledRelay data exists\n    WHERE topic_0 = '0x44b559f101f8fbcc8a0ea43fa91a05a729a5ea6e14a7c75aa750374690137208'\n        AND filled_relay_data_input_amount IS NOT NULL\n        AND filled_relay_data_output_amount IS NOT NULL\n),\n\n-- INPUT token metadata: includes chain_id for matching with origin_chain_id\n-- This allows us to find the correct token on whichever chain the deposit came from\ninput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals,\n        chain_id\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n\n),\n\n-- OUTPUT token metadata: filtered to destination chain (Worldchain)\n-- This is correct because on fills, output_token is always on the destination chain\noutput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n    WHERE chain = 'worldchain'\n\n),\n\ncleaned_fills AS (\n    SELECT\n        \n        -- Timestamp: Convert text to proper timestamp type\n        \n        CASE \n            WHEN timestamp_datetime ~ '^\\d+$' THEN          -- If it's pure digits, treat as Unix timestamp (seconds since 1970-01-01)\n                TO_TIMESTAMP(timestamp_datetime::BIGINT)    -- PostgreSQL's TO_TIMESTAMP can parse various formats, but we use the simplest approach\n            ELSE \n                timestamp_datetime::TIMESTAMP               -- Otherwise, try to parse as ISO format string\n        END AS fill_timestamp,\n        \n        transactionHash AS transaction_hash,\n        blockchain,\n        source_file,\n        \n        -- ============================================================\n        -- INDEXED FIELDS (from topics) - Already decoded by ETL\n        -- ============================================================\n        -- These come from the event's \"topics\" array, which is indexed for fast blockchain queries\n        \n        -- Origin chain ID: Which blockchain the funds came FROM\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\n        -- Example: 1 = Ethereum, 42161 = Arbitrum, 8453 = Base\n        -- Cast via NUMERIC first to handle decimal strings like \"8453.0\" from ETL\n        (topic_origin_chain_id::NUMERIC)::BIGINT AS origin_chain_id,\n        \n        -- Deposit ID: Unique identifier linking this fill to its original deposit\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\n        -- This is the KEY that connects deposits \u2194 fills across chains\n        -- Cast via NUMERIC first to handle decimal strings like \"12345.0\" from ETL\n        (topic_deposit_id::NUMERIC)::BIGINT AS deposit_id,\n        \n        -- Relayer address: Who provided the liquidity\n        -- Already decoded by ETL to proper address format (0x...)\n        topic_relayer AS relayer_address,\n        \n        -- ============================================================\n        -- TOKEN INFORMATION (what was bridged)\n        -- ============================================================\n        \n        -- Input token: The token address on the origin chain\n        -- Already decoded by ETL to proper address format\n        -- Example: 0xA0b86991c6218b36c1d19D4a2e9Eb0cE3606eB48 = USDC on Ethereum\n        -- Normalize to lowercase for joining with token metadata\n        LOWER(filled_relay_data_input_token) AS input_token_address,\n        \n        -- Output token: The token address on the destination chain (Worldchain in this case)\n        -- Already decoded by ETL to proper address format\n        -- Normalize to lowercase for joining with token metadata\n        LOWER(filled_relay_data_output_token) AS output_token_address,\n        \n        -- ============================================================\n        -- AMOUNT INFORMATION (how much was bridged)\n        -- ============================================================\n        \n        -- Input amount: How much was sent from origin chain\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\n        -- Store as RAW amount first (before rescaling by decimals)\n        filled_relay_data_input_amount::NUMERIC AS input_amount_raw,\n        \n        -- Output amount: How much was received on destination chain\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\n        -- Usually slightly less than input due to fees/spread\n        -- Store as RAW amount first (before rescaling by decimals)\n        filled_relay_data_output_amount::NUMERIC AS output_amount_raw,\n        \n        -- ============================================================\n        -- RELAYER & ROUTING INFORMATION\n        -- ============================================================\n        \n        -- Repayment chain ID: Where the relayer gets reimbursed\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\n        -- Relayers front capital, then get paid back (often on a different chain)\n        -- Cast via NUMERIC first to handle decimal strings like \"1.0\" from ETL\n        (filled_relay_data_repayment_chain_id::NUMERIC)::BIGINT AS repayment_chain_id,\n        \n        -- Exclusive relayer: If set, only this address can fill this deposit\n        -- Already decoded by ETL to proper address format\n        -- Used for priority/guaranteed fills (NULL if no exclusive relayer)\n        filled_relay_data_exclusive_relayer AS exclusive_relayer_address,\n        \n        -- ============================================================\n        -- USER INFORMATION\n        -- ============================================================\n        \n        -- Depositor: The original user who initiated the bridge\n        -- Already decoded by ETL to proper address format\n        filled_relay_data_depositor AS depositor_address,\n        \n        -- Recipient: Who receives the funds on the destination chain\n        -- Already decoded by ETL to proper address format\n        -- Usually the same as depositor, but can be different (gift/transfer)\n        filled_relay_data_recipient AS recipient_address,\n        \n        -- ============================================================\n        -- GAS DATA (for relayer cost analysis)\n        -- ============================================================\n        gas_price_wei::BIGINT AS gas_price_wei,\n        gas_used::BIGINT AS gas_used\n        \n    FROM raw_fills\n)\n\n-- ============================================================\n-- FINAL SELECT: Join with token metadata and rescale amounts\n-- ============================================================\nSELECT\n    -- Event identity\n    c.fill_timestamp,\n    c.transaction_hash,\n    c.blockchain,\n    c.source_file,\n    \n    -- Indexed fields\n    c.origin_chain_id,\n    c.deposit_id,\n    c.relayer_address,\n    \n    -- Token info WITH NAMES\n    -- Input token (what was deposited on origin chain)\n    c.input_token_address,\n    input_tok.token_symbol AS input_token_symbol,        -- e.g., 'USDC', 'WETH'\n    \n    -- Output token (what was received on destination chain - Worldchain)\n    c.output_token_address,\n    output_tok.token_symbol AS output_token_symbol,      -- e.g., 'USDC', 'WETH'\n    \n    -- Rescaled amounts (human-readable)\n    -- These use the rescale_amount macro which divides raw amount by 10^decimals\n    -- Example: 5000000 raw USDC (6 decimals) \u2192 5.0 USDC\n    \n    c.input_amount_raw / POWER(10, COALESCE(input_tok.decimals, 18))\n AS input_amount,\n    \n    c.output_amount_raw / POWER(10, COALESCE(output_tok.decimals, 18))\n AS output_amount,\n    \n    -- Raw amounts (preserved for auditing)\n    -- These are the original blockchain values before rescaling\n    -- Example: 5.0 USDC is stored as 5000000 on blockchain\n    c.input_amount_raw,\n    c.output_amount_raw,\n    \n    -- Relayer routing\n    c.repayment_chain_id,\n    c.exclusive_relayer_address,\n    \n    -- User info\n    c.depositor_address,\n    c.recipient_address,\n    \n    -- Gas data (for relayer cost analysis)\n    c.gas_price_wei,\n    c.gas_used,\n    (c.gas_price_wei * c.gas_used) AS gas_cost_wei\n    \nFROM cleaned_fills c\n\n-- Join with token metadata to get decimals and symbols for INPUT token\n-- Join on BOTH address AND chain_id to match the correct origin chain\nLEFT JOIN input_token_meta AS input_tok\n    ON c.input_token_address = input_tok.token_address\n    AND c.origin_chain_id = input_tok.chain_id\n\n-- Join with token metadata to get decimals and symbols for OUTPUT token\n-- Join on address only - we already filtered to destination chain (Worldchain)\nLEFT JOIN output_token_meta AS output_tok\n    ON c.output_token_address = output_tok.token_address\n\n-- Data quality: Only include rows with essential fields populated\nWHERE c.deposit_id IS NOT NULL\n    AND c.relayer_address IS NOT NULL\n    AND c.input_token_address IS NOT NULL\n    AND c.output_token_address IS NOT NULL\n    AND c.input_amount_raw IS NOT NULL\n    AND c.output_amount_raw IS NOT NULL", "relation_name": "\"across_analytics\".\"dbt_staging\".\"stg_worldchain__fills\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2025-12-30T08:46:01.884692Z", "completed_at": "2025-12-30T08:46:01.903926Z"}, {"name": "execute", "started_at": "2025-12-30T08:46:01.905717Z", "completed_at": "2025-12-30T08:46:02.551691Z"}], "thread_id": "Thread-1 (worker)", "execution_time": 0.6717069149017334, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.int_unified_refunds", "compiled": true, "compiled_code": "-- int_unified_refunds.sql\n-- PURPOSE: Combine refunds from ALL chains into ONE table with converted amounts\n-- WHY: Refunds = capital returning to relayers. Tracks when relayers get paid back.\n\n\n\n-- Token metadata for amount conversion\nWITH token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals,\n        chain_id\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n\n),\n\n-- Chain ID to Name mapping for chains with parquet data\nchain_names AS (\n    SELECT chain_id, chain_name\n    FROM (\n        VALUES\n        (1, 'Ethereum'), (42161, 'Arbitrum'), (137, 'Polygon'),\n        (59144, 'Linea'), (480, 'Worldchain'), (130, 'Unichain'),\n        (999, 'HyperEVM'), (143, 'Monad')\n    ) AS chains(chain_id, chain_name)\n),\n\n-- Each CTE selects from a chain's staging model\narbitrum_refunds AS (\n    SELECT \n        refund_timestamp,\n        transaction_hash,\n        chain_id,\n        root_bundle_id,\n        leaf_id,\n        LOWER(refund_token_address) AS refund_token_address,\n        total_refund_amount AS total_refund_amount_raw,\n        refund_addresses_string,\n        refund_amounts_string,\n        refund_count,\n        'arbitrum' AS source_blockchain\n    FROM \"across_analytics\".\"dbt_staging\".\"stg_arbitrum__refunds\"\n),\n\nethereum_refunds AS (\n    SELECT \n        refund_timestamp,\n        transaction_hash,\n        chain_id,\n        root_bundle_id,\n        leaf_id,\n        LOWER(refund_token_address) AS refund_token_address,\n        total_refund_amount AS total_refund_amount_raw,\n        refund_addresses_string,\n        refund_amounts_string,\n        refund_count,\n        'ethereum' AS source_blockchain\n    FROM \"across_analytics\".\"dbt_staging\".\"stg_ethereum__refunds\"\n),\n\npolygon_refunds AS (\n    SELECT \n        refund_timestamp,\n        transaction_hash,\n        chain_id,\n        root_bundle_id,\n        leaf_id,\n        LOWER(refund_token_address) AS refund_token_address,\n        total_refund_amount AS total_refund_amount_raw,\n        refund_addresses_string,\n        refund_amounts_string,\n        refund_count,\n        'polygon' AS source_blockchain\n    FROM \"across_analytics\".\"dbt_staging\".\"stg_polygon__refunds\"\n),\n\nlinea_refunds AS (\n    SELECT \n        refund_timestamp,\n        transaction_hash,\n        chain_id,\n        root_bundle_id,\n        leaf_id,\n        LOWER(refund_token_address) AS refund_token_address,\n        total_refund_amount AS total_refund_amount_raw,\n        refund_addresses_string,\n        refund_amounts_string,\n        refund_count,\n        'linea' AS source_blockchain\n    FROM \"across_analytics\".\"dbt_staging\".\"stg_linea__refunds\"\n),\n\nworldchain_refunds AS (\n    SELECT \n        refund_timestamp,\n        transaction_hash,\n        chain_id,\n        root_bundle_id,\n        leaf_id,\n        LOWER(refund_token_address) AS refund_token_address,\n        total_refund_amount AS total_refund_amount_raw,\n        refund_addresses_string,\n        refund_amounts_string,\n        refund_count,\n        'worldchain' AS source_blockchain\n    FROM \"across_analytics\".\"dbt_staging\".\"stg_worldchain__refunds\"\n),\n\nunichain_refunds AS (\n    SELECT \n        refund_timestamp,\n        transaction_hash,\n        chain_id,\n        root_bundle_id,\n        leaf_id,\n        LOWER(refund_token_address) AS refund_token_address,\n        total_refund_amount AS total_refund_amount_raw,\n        refund_addresses_string,\n        refund_amounts_string,\n        refund_count,\n        'unichain' AS source_blockchain\n    FROM \"across_analytics\".\"dbt_staging\".\"stg_unichain__refunds\"\n),\n\nhyperevm_refunds AS (\n    SELECT \n        refund_timestamp,\n        transaction_hash,\n        chain_id,\n        root_bundle_id,\n        leaf_id,\n        LOWER(refund_token_address) AS refund_token_address,\n        total_refund_amount AS total_refund_amount_raw,\n        refund_addresses_string,\n        refund_amounts_string,\n        refund_count,\n        'hyperevm' AS source_blockchain\n    FROM \"across_analytics\".\"dbt_staging\".\"stg_hyperevm__refunds\"\n),\n\nmonad_refunds AS (\n    SELECT \n        refund_timestamp,\n        transaction_hash,\n        chain_id,\n        root_bundle_id,\n        leaf_id,\n        LOWER(refund_token_address) AS refund_token_address,\n        total_refund_amount AS total_refund_amount_raw,\n        refund_addresses_string,\n        refund_amounts_string,\n        refund_count,\n        'monad' AS source_blockchain\n    FROM \"across_analytics\".\"dbt_staging\".\"stg_monad__refunds\"\n),\n\n-- UNION ALL: Stack all refunds from all chains\nall_refunds AS (\n    SELECT * FROM arbitrum_refunds\n    UNION ALL\n    SELECT * FROM ethereum_refunds\n    UNION ALL\n    SELECT * FROM polygon_refunds\n    UNION ALL\n    SELECT * FROM linea_refunds\n    UNION ALL\n    SELECT * FROM worldchain_refunds\n    UNION ALL\n    SELECT * FROM unichain_refunds\n    UNION ALL\n    SELECT * FROM hyperevm_refunds\n    UNION ALL\n    SELECT * FROM monad_refunds\n)\n\n-- Final SELECT: Join with token metadata and convert amounts\nSELECT\n    r.refund_timestamp,\n    r.transaction_hash,\n    r.chain_id,\n    cn.chain_name,\n    r.root_bundle_id,\n    r.leaf_id,\n    r.refund_token_address,\n    tok.token_symbol AS refund_token_symbol,\n    tok.decimals AS token_decimals,\n    -- Rescaled amount (human-readable)\n    \n    r.total_refund_amount_raw / POWER(10, COALESCE(tok.decimals, 18))\n AS total_refund_amount,\n    -- Raw amount (preserved for auditing)\n    r.total_refund_amount_raw,\n    r.refund_addresses_string,\n    r.refund_amounts_string,\n    r.refund_count,\n    r.source_blockchain\n\nFROM all_refunds r\n\n-- Join for chain name\nLEFT JOIN chain_names cn ON r.chain_id = cn.chain_id\n\n-- Join with token metadata on address + chain_id\nLEFT JOIN token_meta AS tok\n    ON r.refund_token_address = tok.token_address\n    AND r.chain_id = tok.chain_id", "relation_name": "\"across_analytics\".\"dbt_intermediate\".\"int_unified_refunds\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2025-12-30T08:46:01.396297Z", "completed_at": "2025-12-30T08:46:01.423435Z"}, {"name": "execute", "started_at": "2025-12-30T08:46:01.428121Z", "completed_at": "2025-12-30T08:46:01.802282Z"}], "thread_id": "Thread-2 (worker)", "execution_time": 0.4153916835784912, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.stg_worldchain__deposits", "compiled": true, "compiled_code": "-- Staging model for FundsDeposited events from Worldchain\n-- This model extracts deposit events where users initiate cross-chain bridge transactions\n\nWITH raw_deposits AS (\n\n    SELECT\n        timestamp_datetime,\n        transactionHash,\n        blockchain,\n        source_file,\n        topic_destination_chain_id,\n        topic_deposit_id,\n        topic_depositor,\n        funds_deposited_data_input_token,\n        funds_deposited_data_output_token,\n        funds_deposited_data_input_amount,\n        funds_deposited_data_output_amount,\n        funds_deposited_data_recipient\n        \n    FROM raw.worldchain_logs_processed\n    \n    WHERE topic_0 = '0x32ed1a409ef04c7b0227189c3a103dc5ac10e775a15b785dcc510201f7c25ad3'\n        AND funds_deposited_data_input_amount IS NOT NULL\n        AND funds_deposited_data_output_amount IS NOT NULL\n),\n\n-- INPUT token metadata: filtered to origin chain (Worldchain)\ninput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n    WHERE chain = 'worldchain'\n\n),\n\n-- OUTPUT token metadata: includes chain_id for matching with destination_chain_id\noutput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals,\n        chain_id\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n\n),\n\ncleaned_deposits AS (\n    SELECT\n        CASE \n            WHEN timestamp_datetime ~ '^\\d+$' THEN\n                TO_TIMESTAMP(timestamp_datetime::BIGINT)\n            ELSE \n                timestamp_datetime::TIMESTAMP\n        END AS deposit_timestamp,\n        \n        transactionHash AS transaction_hash,\n        blockchain,\n        source_file,\n        (topic_destination_chain_id::NUMERIC)::BIGINT AS destination_chain_id,\n        (topic_deposit_id::NUMERIC)::BIGINT AS deposit_id,\n        topic_depositor AS depositor_address,\n        LOWER(funds_deposited_data_input_token) AS input_token_address,\n        LOWER(funds_deposited_data_output_token) AS output_token_address,\n        funds_deposited_data_input_amount::NUMERIC AS input_amount_raw,\n        funds_deposited_data_output_amount::NUMERIC AS output_amount_raw,\n        funds_deposited_data_recipient AS recipient_address\n        \n    FROM raw_deposits\n)\n\nSELECT\n    c.deposit_timestamp,\n    c.transaction_hash,\n    c.blockchain,\n    c.source_file,\n    c.destination_chain_id,\n    c.deposit_id,\n    c.depositor_address,\n    c.input_token_address,\n    input_tok.token_symbol AS input_token_symbol,\n    c.output_token_address,\n    output_tok.token_symbol AS output_token_symbol,\n    \n    c.input_amount_raw / POWER(10, COALESCE(input_tok.decimals, 18))\n AS input_amount,\n    \n    CASE \n        WHEN c.output_token_address = '0x0000000000000000000000000000000000000000' \n        THEN c.output_amount_raw / POWER(10, COALESCE(input_tok.decimals, 18))\n        ELSE c.output_amount_raw / POWER(10, COALESCE(output_tok.decimals, 18))\n    END\n AS output_amount,\n    c.input_amount_raw,\n    c.output_amount_raw,\n    c.recipient_address\n    \nFROM cleaned_deposits c\n\nLEFT JOIN input_token_meta AS input_tok\n    ON c.input_token_address = input_tok.token_address\n\nLEFT JOIN output_token_meta AS output_tok\n    ON c.output_token_address = output_tok.token_address\n    AND c.destination_chain_id = output_tok.chain_id\n\nWHERE c.deposit_id IS NOT NULL\n    AND c.depositor_address IS NOT NULL\n    AND c.input_token_address IS NOT NULL\n    AND c.output_token_address IS NOT NULL\n    AND c.input_amount_raw IS NOT NULL\n    AND c.output_amount_raw IS NOT NULL", "relation_name": "\"across_analytics\".\"dbt_staging\".\"stg_worldchain__deposits\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2025-12-30T08:46:03.601726Z", "completed_at": "2025-12-30T08:46:03.624455Z"}, {"name": "execute", "started_at": "2025-12-30T08:46:03.627124Z", "completed_at": "2025-12-30T08:46:03.848942Z"}], "thread_id": "Thread-4 (worker)", "execution_time": 0.266493558883667, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.int_unified_refunds_expanded", "compiled": true, "compiled_code": "-- int_refunds_expanded.sql\n-- CHALLENGE: some refund_amounts_string are arrays that have to be unnested\n-- PURPOSE: Expand comma-separated refund arrays into individual rows\n-- WHY: Each row = one relayer receiving one refund amount\n-- ENABLES: Easy aggregation, filtering by relayer, joins with relayer metrics\n\n\n\nWITH unified AS (\n    -- Get all refunds from all chains (already unified in int_unified_refunds)\n    SELECT * \n    FROM \"across_analytics\".\"dbt_intermediate\".\"int_unified_refunds\"\n    WHERE refund_count > 0  -- Only process batches with actual refunds\n),\n\n-- Unnest the comma-separated strings into individual rows\n-- Uses CROSS JOIN LATERAL with UNNEST to expand arrays\n-- WITH ORDINALITY gives us the position index for matching amounts to addresses\nexpanded AS (\n    SELECT\n        -- Batch-level identifiers (same for all rows from same batch)\n        refund_timestamp,\n        transaction_hash,\n        chain_id,\n        chain_name,\n        root_bundle_id,\n        leaf_id,\n        refund_token_address,\n        refund_token_symbol,\n        token_decimals,\n        total_refund_amount,\n        total_refund_amount_raw,\n        refund_count,\n        source_blockchain,\n        \n        -- Individual refund data (one row per relayer/amount pair)\n        -- TRIM handles any whitespace that might exist in the CSV-like strings\n        TRIM(amounts.amount)::NUMERIC AS refund_amount_raw,\n        TRIM(addresses.address) AS relayer_address,\n        amounts.idx AS refund_index\n        \n    FROM unified\n    -- Expand refund_amounts_string: \"100,200,300\" \u2192 3 rows with values 100, 200, 300\n    CROSS JOIN LATERAL UNNEST(\n        string_to_array(refund_amounts_string, ',')\n    ) WITH ORDINALITY AS amounts(amount, idx)\n    -- Expand refund_addresses_string: \"0xAAA,0xBBB,0xCCC\" \u2192 3 rows with addresses\n    CROSS JOIN LATERAL UNNEST(\n        string_to_array(refund_addresses_string, ',')\n    ) WITH ORDINALITY AS addresses(address, idx)\n    -- Match by position: 1st amount goes to 1st address, 2nd to 2nd, etc.\n    WHERE amounts.idx = addresses.idx\n)\n\nSELECT\n    -- Event identification\n    refund_timestamp,\n    transaction_hash,\n    source_blockchain,\n    \n    -- Batch identifiers (for grouping back if needed)\n    chain_id,\n    chain_name,\n    root_bundle_id,\n    leaf_id,\n    \n    -- Individual refund details\n    refund_index,\n    relayer_address,\n    refund_token_address,\n    refund_token_symbol,\n    \n    -- Individual refund amount (rescaled using token decimals)\n    \n    refund_amount_raw / POWER(10, COALESCE(token_decimals, 18))\n AS refund_amount,\n    refund_amount_raw,\n    \n    -- Batch context (useful for analysis)\n    total_refund_amount AS batch_total_amount,\n    total_refund_amount_raw AS batch_total_amount_raw,\n    refund_count AS batch_refund_count,\n    \n    -- Unique identifier for each individual refund record\n    transaction_hash || '-' || leaf_id || '-' || refund_index AS refund_id\n    \nFROM expanded", "relation_name": "\"across_analytics\".\"dbt_intermediate\".\"int_unified_refunds_expanded\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2025-12-30T08:46:03.611873Z", "completed_at": "2025-12-30T08:46:03.634363Z"}, {"name": "execute", "started_at": "2025-12-30T08:46:03.636237Z", "completed_at": "2025-12-30T08:46:03.890275Z"}], "thread_id": "Thread-1 (worker)", "execution_time": 0.3056671619415283, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.int_unified_deposits", "compiled": true, "compiled_code": "-- int_unified_deposits.sql\n-- PURPOSE: Combine deposits from ALL chains into ONE table\n-- WHY: Right now deposits are separate per chain. We need them unified to track cross-chain flows.\n\n\n\n-- Each CTE selects from a chain's staging model and adds the origin chain ID\nWITH arbitrum_deposits AS (\n    SELECT \n        deposit_timestamp,\n        transaction_hash,\n        42161 AS origin_chain_id,  -- Arbitrum's chain ID\n        destination_chain_id,\n        deposit_id,\n        depositor_address,\n        recipient_address,\n        input_token_address,\n        input_token_symbol,\n        output_token_address,\n        output_token_symbol,\n        input_amount,\n        output_amount\n    FROM \"across_analytics\".\"dbt_staging\".\"stg_arbitrum__deposits\"\n),\n\nethereum_deposits AS (\n    SELECT \n        deposit_timestamp,\n        transaction_hash,\n        1 AS origin_chain_id,  -- Ethereum's chain ID\n        destination_chain_id,\n        deposit_id,\n        depositor_address,\n        recipient_address,\n        input_token_address,\n        input_token_symbol,\n        output_token_address,\n        output_token_symbol,\n        input_amount,\n        output_amount\n    FROM \"across_analytics\".\"dbt_staging\".\"stg_ethereum__deposits\"\n),\n\npolygon_deposits AS (\n    SELECT \n        deposit_timestamp,\n        transaction_hash,\n        137 AS origin_chain_id,  -- Polygon's chain ID\n        destination_chain_id,\n        deposit_id,\n        depositor_address,\n        recipient_address,\n        input_token_address,\n        input_token_symbol,\n        output_token_address,\n        output_token_symbol,\n        input_amount,\n        output_amount\n    FROM \"across_analytics\".\"dbt_staging\".\"stg_polygon__deposits\"\n),\n\nlinea_deposits AS (\n    SELECT \n        deposit_timestamp,\n        transaction_hash,\n        59144 AS origin_chain_id,  -- Linea's chain ID\n        destination_chain_id,\n        deposit_id,\n        depositor_address,\n        recipient_address,\n        input_token_address,\n        input_token_symbol,\n        output_token_address,\n        output_token_symbol,\n        input_amount,\n        output_amount\n    FROM \"across_analytics\".\"dbt_staging\".\"stg_linea__deposits\"\n),\n\nworldchain_deposits AS (\n    SELECT \n        deposit_timestamp,\n        transaction_hash,\n        480 AS origin_chain_id,  -- WorldChain's chain ID\n        destination_chain_id,\n        deposit_id,\n        depositor_address,\n        recipient_address,\n        input_token_address,\n        input_token_symbol,\n        output_token_address,\n        output_token_symbol,\n        input_amount,\n        output_amount\n    FROM \"across_analytics\".\"dbt_staging\".\"stg_worldchain__deposits\"\n),\n\nunichain_deposits AS (\n    SELECT \n        deposit_timestamp,\n        transaction_hash,\n        130 AS origin_chain_id,  -- Unichain's chain ID\n        destination_chain_id,\n        deposit_id,\n        depositor_address,\n        recipient_address,\n        input_token_address,\n        input_token_symbol,\n        output_token_address,\n        output_token_symbol,\n        input_amount,\n        output_amount\n    FROM \"across_analytics\".\"dbt_staging\".\"stg_unichain__deposits\"\n),\n\nhyperevm_deposits AS (\n    SELECT \n        deposit_timestamp,\n        transaction_hash,\n        999 AS origin_chain_id,  -- HyperEVM's chain ID\n        destination_chain_id,\n        deposit_id,\n        depositor_address,\n        recipient_address,\n        input_token_address,\n        input_token_symbol,\n        output_token_address,\n        output_token_symbol,\n        input_amount,\n        output_amount\n    FROM \"across_analytics\".\"dbt_staging\".\"stg_hyperevm__deposits\"\n),\n\nmonad_deposits AS (\n    SELECT \n        deposit_timestamp,\n        transaction_hash,\n        143 AS origin_chain_id,  -- Monad's chain ID\n        destination_chain_id,\n        deposit_id,\n        depositor_address,\n        recipient_address,\n        input_token_address,\n        input_token_symbol,\n        output_token_address,\n        output_token_symbol,\n        input_amount,\n        output_amount\n    FROM \"across_analytics\".\"dbt_staging\".\"stg_monad__deposits\"\n),\n\n-- Supported chain IDs (chains we have parquet data for)\n-- 42161=Arbitrum, 1=Ethereum, 137=Polygon, 59144=Linea, 480=Worldchain, 130=Unichain, 999=HyperEVM, 143=Monad\n\n-- Chain ID to Name mapping for chains with parquet data\nchain_names AS (\n    SELECT chain_id, chain_name\n    FROM (\n        VALUES\n        (1, 'Ethereum'), (42161, 'Arbitrum'), (137, 'Polygon'),\n        (59144, 'Linea'), (480, 'Worldchain'), (130, 'Unichain'),\n        (999, 'HyperEVM'), (143, 'Monad')\n    ) AS chains(chain_id, chain_name)\n),\n\n-- UNION ALL: Stack all deposits from all chains into one table\n-- Filter: Only include deposits where destination_chain_id is a supported chain\nall_deposits AS (\n    SELECT * FROM arbitrum_deposits WHERE destination_chain_id IN (42161, 1, 137, 59144, 480, 130, 999, 143)\n    UNION ALL\n    SELECT * FROM ethereum_deposits WHERE destination_chain_id IN (42161, 1, 137, 59144, 480, 130, 999, 143)\n    UNION ALL\n    SELECT * FROM polygon_deposits WHERE destination_chain_id IN (42161, 1, 137, 59144, 480, 130, 999, 143)\n    UNION ALL\n    SELECT * FROM linea_deposits WHERE destination_chain_id IN (42161, 1, 137, 59144, 480, 130, 999, 143)\n    UNION ALL\n    SELECT * FROM worldchain_deposits WHERE destination_chain_id IN (42161, 1, 137, 59144, 480, 130, 999, 143)\n    UNION ALL\n    SELECT * FROM unichain_deposits WHERE destination_chain_id IN (42161, 1, 137, 59144, 480, 130, 999, 143)\n    UNION ALL\n    SELECT * FROM hyperevm_deposits WHERE destination_chain_id IN (42161, 1, 137, 59144, 480, 130, 999, 143)\n    UNION ALL\n    SELECT * FROM monad_deposits WHERE destination_chain_id IN (42161, 1, 137, 59144, 480, 130, 999, 143)\n)\n\n-- Final SELECT with descriptive chain names\nSELECT\n    d.deposit_timestamp,\n    d.transaction_hash,\n    d.origin_chain_id,\n    oc.chain_name AS origin_chain_name,\n    d.destination_chain_id,\n    dc.chain_name AS destination_chain_name,\n    d.deposit_id,\n    d.depositor_address,\n    d.recipient_address,\n    d.input_token_address,\n    d.input_token_symbol,\n    d.output_token_address,\n    d.output_token_symbol,\n    d.input_amount,\n    d.output_amount\nFROM all_deposits d\nLEFT JOIN chain_names oc ON d.origin_chain_id = oc.chain_id\nLEFT JOIN chain_names dc ON d.destination_chain_id = dc.chain_id", "relation_name": "\"across_analytics\".\"dbt_intermediate\".\"int_unified_deposits\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2025-12-30T08:46:03.587420Z", "completed_at": "2025-12-30T08:46:03.635523Z"}, {"name": "execute", "started_at": "2025-12-30T08:46:03.640988Z", "completed_at": "2025-12-30T08:46:03.888413Z"}], "thread_id": "Thread-3 (worker)", "execution_time": 0.31009840965270996, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.int_unified_fills", "compiled": true, "compiled_code": "-- int_unified_fills.sql\n-- PURPOSE: Combine fills from ALL chains into ONE table\n-- WHY: Fills happen on the DESTINATION chain. We need to see all fills to match with deposits.\n\n\n\n-- Each CTE selects from a chain's staging model and adds the destination chain ID\nWITH arbitrum_fills AS (\n    SELECT \n        fill_timestamp,\n        transaction_hash,\n        origin_chain_id,\n        42161 AS destination_chain_id,  -- Fill happened ON Arbitrum\n        deposit_id,\n        relayer_address,\n        depositor_address,\n        recipient_address,\n        input_token_address,\n        output_token_address,\n        output_token_symbol,\n        input_amount,\n        output_amount,\n        repayment_chain_id,\n        gas_price_wei,\n        gas_used,\n        gas_cost_wei\n    FROM \"across_analytics\".\"dbt_staging\".\"stg_arbitrum__fills\"\n),\n\nethereum_fills AS (\n    SELECT \n        fill_timestamp,\n        transaction_hash,\n        origin_chain_id,\n        1 AS destination_chain_id,  -- Fill happened ON Ethereum\n        deposit_id,\n        relayer_address,\n        depositor_address,\n        recipient_address,\n        input_token_address,\n        output_token_address,\n        output_token_symbol,\n        input_amount,\n        output_amount,\n        repayment_chain_id,\n        gas_price_wei,\n        gas_used,\n        gas_cost_wei\n    FROM \"across_analytics\".\"dbt_staging\".\"stg_ethereum__fills\"\n),\n\npolygon_fills AS (\n    SELECT \n        fill_timestamp,\n        transaction_hash,\n        origin_chain_id,\n        137 AS destination_chain_id,  -- Fill happened ON Polygon\n        deposit_id,\n        relayer_address,\n        depositor_address,\n        recipient_address,\n        input_token_address,\n        output_token_address,\n        output_token_symbol,\n        input_amount,\n        output_amount,\n        repayment_chain_id,\n        gas_price_wei,\n        gas_used,\n        gas_cost_wei\n    FROM \"across_analytics\".\"dbt_staging\".\"stg_polygon__fills\"\n),\n\nlinea_fills AS (\n    SELECT \n        fill_timestamp,\n        transaction_hash,\n        origin_chain_id,\n        59144 AS destination_chain_id,  -- Fill happened ON Linea\n        deposit_id,\n        relayer_address,\n        depositor_address,\n        recipient_address,\n        input_token_address,\n        output_token_address,\n        output_token_symbol,\n        input_amount,\n        output_amount,\n        repayment_chain_id,\n        gas_price_wei,\n        gas_used,\n        gas_cost_wei\n    FROM \"across_analytics\".\"dbt_staging\".\"stg_linea__fills\"\n),\n\nworldchain_fills AS (\n    SELECT \n        fill_timestamp,\n        transaction_hash,\n        origin_chain_id,\n        480 AS destination_chain_id,  -- Fill happened ON WorldChain\n        deposit_id,\n        relayer_address,\n        depositor_address,\n        recipient_address,\n        input_token_address,\n        output_token_address,\n        output_token_symbol,\n        input_amount,\n        output_amount,\n        repayment_chain_id,\n        gas_price_wei,\n        gas_used,\n        gas_cost_wei\n    FROM \"across_analytics\".\"dbt_staging\".\"stg_worldchain__fills\"\n),\n\nunichain_fills AS (\n    SELECT \n        fill_timestamp,\n        transaction_hash,\n        origin_chain_id,\n        130 AS destination_chain_id,  -- Fill happened ON Unichain\n        deposit_id,\n        relayer_address,\n        depositor_address,\n        recipient_address,\n        input_token_address,\n        output_token_address,\n        output_token_symbol,\n        input_amount,\n        output_amount,\n        repayment_chain_id,\n        gas_price_wei,\n        gas_used,\n        gas_cost_wei\n    FROM \"across_analytics\".\"dbt_staging\".\"stg_unichain__fills\"\n),\n\nhyperevm_fills AS (\n    SELECT \n        fill_timestamp,\n        transaction_hash,\n        origin_chain_id,\n        999 AS destination_chain_id,  -- Fill happened ON HyperEVM\n        deposit_id,\n        relayer_address,\n        depositor_address,\n        recipient_address,\n        input_token_address,\n        output_token_address,\n        output_token_symbol,\n        input_amount,\n        output_amount,\n        repayment_chain_id,\n        gas_price_wei,\n        gas_used,\n        gas_cost_wei\n    FROM \"across_analytics\".\"dbt_staging\".\"stg_hyperevm__fills\"\n),\n\nmonad_fills AS (\n    SELECT \n        fill_timestamp,\n        transaction_hash,\n        origin_chain_id,\n        143 AS destination_chain_id,  -- Fill happened ON Monad\n        deposit_id,\n        relayer_address,\n        depositor_address,\n        recipient_address,\n        input_token_address,\n        output_token_address,\n        output_token_symbol,\n        input_amount,\n        output_amount,\n        repayment_chain_id,\n        gas_price_wei,\n        gas_used,\n        gas_cost_wei\n    FROM \"across_analytics\".\"dbt_staging\".\"stg_monad__fills\"\n),\n\n-- Supported chain IDs (chains we have parquet data for)\n-- 42161=Arbitrum, 1=Ethereum, 137=Polygon, 59144=Linea, 480=Worldchain, 130=Unichain, 999=HyperEVM, 143=Monad\n\n-- Chain ID to Name mapping for chains with parquet data\nchain_names AS (\n    SELECT chain_id, chain_name\n    FROM (\n        VALUES\n        (1, 'Ethereum'), (42161, 'Arbitrum'), (137, 'Polygon'),\n        (59144, 'Linea'), (480, 'Worldchain'), (130, 'Unichain'),\n        (999, 'HyperEVM'), (143, 'Monad')\n    ) AS chains(chain_id, chain_name)\n),\n\n-- UNION ALL: Stack all fills from all chains into one table\n-- Filter: Only include fills where origin_chain_id is a supported chain\nall_fills AS (\n    SELECT * FROM arbitrum_fills WHERE origin_chain_id IN (42161, 1, 137, 59144, 480, 130, 999, 143)\n    UNION ALL\n    SELECT * FROM ethereum_fills WHERE origin_chain_id IN (42161, 1, 137, 59144, 480, 130, 999, 143)\n    UNION ALL\n    SELECT * FROM polygon_fills WHERE origin_chain_id IN (42161, 1, 137, 59144, 480, 130, 999, 143)\n    UNION ALL\n    SELECT * FROM linea_fills WHERE origin_chain_id IN (42161, 1, 137, 59144, 480, 130, 999, 143)\n    UNION ALL\n    SELECT * FROM worldchain_fills WHERE origin_chain_id IN (42161, 1, 137, 59144, 480, 130, 999, 143)\n    UNION ALL\n    SELECT * FROM unichain_fills WHERE origin_chain_id IN (42161, 1, 137, 59144, 480, 130, 999, 143)\n    UNION ALL\n    SELECT * FROM hyperevm_fills WHERE origin_chain_id IN (42161, 1, 137, 59144, 480, 130, 999, 143)\n    UNION ALL\n    SELECT * FROM monad_fills WHERE origin_chain_id IN (42161, 1, 137, 59144, 480, 130, 999, 143)\n)\n\n-- Final SELECT with descriptive chain names\nSELECT\n    f.fill_timestamp,\n    f.transaction_hash,\n    f.origin_chain_id,\n    oc.chain_name AS origin_chain_name,\n    f.destination_chain_id,\n    dc.chain_name AS destination_chain_name,\n    f.deposit_id,\n    f.relayer_address,\n    f.depositor_address,\n    f.recipient_address,\n    f.input_token_address,\n    f.output_token_address,\n    f.output_token_symbol AS fill_token_symbol,  -- Token symbol for the filled amount\n    f.input_amount,\n    f.output_amount,\n    f.repayment_chain_id,\n    -- Gas data (for relayer cost analysis)\n    f.gas_price_wei,\n    f.gas_used,\n    f.gas_cost_wei\nFROM all_fills f\nLEFT JOIN chain_names oc ON f.origin_chain_id = oc.chain_id\nLEFT JOIN chain_names dc ON f.destination_chain_id = dc.chain_id", "relation_name": "\"across_analytics\".\"dbt_intermediate\".\"int_unified_fills\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2025-12-30T08:46:03.904628Z", "completed_at": "2025-12-30T08:46:03.917440Z"}, {"name": "execute", "started_at": "2025-12-30T08:46:03.918737Z", "completed_at": "2025-12-30T08:46:04.055448Z"}], "thread_id": "Thread-2 (worker)", "execution_time": 0.15308403968811035, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.int_deposit_fill_matching", "compiled": true, "compiled_code": "-- int_deposit_fill_matching.sql\n-- PURPOSE: Match every deposit to its fill (if it exists)\n-- WHY: This is the CORE model - connects money leaving one chain to money arriving on another.\n-- \n-- HOW IT WORKS:\n-- 1. User deposits on Chain A \u2192 creates deposit_id\n-- 2. Relayer fills on Chain B \u2192 same deposit_id\n-- 3. We JOIN on deposit_id + origin/destination chain match\n-- 4. Unfilled deposits = rows with NULL fill (stuck capital)\n\n\n\n-- Chain ID to Name mapping for chains with parquet data\nWITH chain_names AS (\n    SELECT \n        chain_id,\n        chain_name\n    FROM (\n        VALUES\n        -- Only chains we have parquet data for:\n        (1, 'Ethereum'),\n        (42161, 'Arbitrum'),\n        (137, 'Polygon'),\n        (59144, 'Linea'),\n        (480, 'Worldchain'),\n        (130, 'Unichain'),\n        (999, 'HyperEVM'),\n        (143, 'Monad')\n    ) AS chains(chain_id, chain_name)\n),\n\n-- Token metadata for symbol lookups\ntoken_metadata AS (\n    SELECT * FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n),\n\ndeposits AS (\n    SELECT * FROM \"across_analytics\".\"dbt_intermediate\".\"int_unified_deposits\"\n),\n\nfills AS (\n    SELECT * FROM \"across_analytics\".\"dbt_intermediate\".\"int_unified_fills\"\n),\n\n-- JOIN deposits to fills on deposit_id + chain matching\nmatched AS (\n    SELECT\n        -- === DEPOSIT INFO (Origin side) ===\n        d.deposit_timestamp,\n        d.transaction_hash AS deposit_tx_hash,\n        d.origin_chain_id,\n        d.destination_chain_id,\n        d.deposit_id,\n        d.depositor_address,\n        d.recipient_address AS deposit_recipient,\n        d.input_token_address AS deposit_token,\n        d.input_amount AS deposit_amount,\n        d.output_amount AS expected_output_amount,\n        \n        -- === FILL INFO (Destination side) ===\n        f.fill_timestamp,\n        f.transaction_hash AS fill_tx_hash,\n        f.relayer_address,\n        f.output_token_address AS fill_token,\n        f.output_amount AS actual_output_amount,\n        f.repayment_chain_id,\n        \n        -- === COMPUTED FIELDS ===\n        -- Fill latency: How long did it take to fill? (in seconds)\n        -- Use GREATEST(0, ...) to handle cross-chain timestamp sync issues\n        GREATEST(0, EXTRACT(EPOCH FROM (f.fill_timestamp - d.deposit_timestamp))) AS fill_latency_seconds,\n        \n        -- Is this deposit filled?\n        CASE WHEN f.deposit_id IS NOT NULL THEN TRUE ELSE FALSE END AS is_filled,\n        \n        -- Fee: difference between deposited amount and filled amount\n        CASE \n            WHEN f.output_amount IS NOT NULL\n            THEN ROUND((d.input_amount - f.output_amount)::NUMERIC, 2)\n            ELSE NULL \n        END AS bridge_fee_nominal,\n        \n        CASE \n            WHEN f.output_amount IS NOT NULL\n            THEN ROUND(((d.input_amount - f.output_amount) / d.input_amount * 100)::NUMERIC, 2)\n            ELSE NULL \n        END AS bridge_fee_percent,\n        \n        -- Slippage: Difference between expected and actual output\n        CASE \n            WHEN f.output_amount IS NOT NULL AND d.output_amount > 0 \n            THEN ROUND(((d.output_amount - f.output_amount) / d.output_amount * 100)::NUMERIC, 2)\n            ELSE NULL \n        END AS slippage_percent\n\n    FROM deposits d\n    \n    -- LEFT JOIN: Keep ALL deposits, even unfilled ones\n    LEFT JOIN fills f \n        ON d.deposit_id = f.deposit_id\n        AND d.origin_chain_id = f.origin_chain_id  -- Must match origin\n        AND d.destination_chain_id = f.destination_chain_id  -- Must match destination\n)\n\nSELECT\n    -- Identity\n    m.deposit_timestamp,\n    m.deposit_tx_hash,\n    m.origin_chain_id,\n    oc.chain_name AS origin_chain_name,\n    m.destination_chain_id,\n    dc.chain_name AS destination_chain_name,\n    m.deposit_id,\n    \n    -- Deposit details\n    m.depositor_address,\n    m.deposit_recipient,\n    m.deposit_token,\n    dt.token_symbol AS deposit_token_symbol,\n    m.deposit_amount,\n    m.expected_output_amount,\n    \n    -- Fill details (NULL if unfilled)\n    m.fill_timestamp,\n    m.fill_tx_hash,\n    m.relayer_address,\n    m.fill_token,\n    ft.token_symbol AS fill_token_symbol,\n    m.actual_output_amount,\n    m.repayment_chain_id,\n    \n    -- Metrics\n    m.fill_latency_seconds,\n    m.is_filled,\n    m.bridge_fee_nominal,\n    m.bridge_fee_percent,\n    m.slippage_percent,\n    \n    -- Route identifier (for aggregations)\n    oc.chain_name || ' \u2192 ' || dc.chain_name AS route_name,\n    m.origin_chain_id || '_' || m.destination_chain_id AS route_id\n\nFROM matched m\n\n-- Join for origin chain name\nLEFT JOIN chain_names oc ON m.origin_chain_id = oc.chain_id\n\n-- Join for destination chain name  \nLEFT JOIN chain_names dc ON m.destination_chain_id = dc.chain_id\n\n-- Join for deposit token symbol (origin chain token)\nLEFT JOIN token_metadata dt \n    ON m.origin_chain_id = dt.chain_id \n    AND LOWER(m.deposit_token) = LOWER(dt.token_address)\n\n-- Join for fill token symbol (destination chain token)\nLEFT JOIN token_metadata ft \n    ON m.destination_chain_id = ft.chain_id \n    AND LOWER(m.fill_token) = LOWER(ft.token_address)", "relation_name": "\"across_analytics\".\"dbt_intermediate\".\"int_deposit_fill_matching\"", "batch_results": null}], "elapsed_time": 5.714206218719482, "args": {"log_file_max_bytes": 10485760, "require_generic_test_arguments_property": true, "use_colors_file": true, "log_path": "C:\\Users\\Longin\\Desktop\\Projects\\across_analytics\\logs", "warn_error_options": {"error": [], "warn": [], "silence": []}, "write_json": true, "log_format_file": "debug", "log_format": "default", "use_colors": true, "favor_state": false, "project_dir": "C:\\Users\\Longin\\Desktop\\Projects\\across_analytics", "defer": false, "macro_debugging": false, "quiet": false, "show_all_deprecations": false, "require_yaml_configuration_for_mf_time_spines": false, "show_resource_report": false, "require_nested_cumulative_type_params": false, "print": true, "static_parser": true, "upload_to_artifacts_ingest_api": false, "vars": {}, "skip_nodes_if_on_run_start_fails": false, "exclude": [], "indirect_selection": "eager", "introspect": true, "profiles_dir": "C:\\Users\\Longin\\.dbt", "printer_width": 80, "validate_macro_args": false, "source_freshness_run_project_hooks": true, "state_modified_compare_vars": false, "strict_mode": false, "require_all_warnings_handled_by_warn_error": false, "require_explicit_package_overrides_for_builtin_materializations": true, "log_level": "info", "which": "run", "require_batched_execution_for_custom_microbatch_strategy": false, "populate_cache": true, "empty": false, "invocation_command": "dbt run --select staging intermediate", "require_resource_names_without_spaces": true, "send_anonymous_usage_stats": true, "state_modified_compare_more_unrendered_values": false, "version_check": true, "log_level_file": "debug", "cache_selected_only": false, "partial_parse_file_diff": true, "select": ["staging", "intermediate"], "use_fast_test_edges": false, "partial_parse": true}}