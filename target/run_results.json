{"metadata": {"dbt_schema_version": "https://schemas.getdbt.com/dbt/run-results/v6.json", "dbt_version": "1.10.15", "generated_at": "2026-01-05T13:09:52.501634Z", "invocation_id": "779ad6a3-d923-4d3c-8de2-1a3888df69f5", "invocation_started_at": "2026-01-05T13:09:31.556758Z", "env": {}}, "results": [{"status": "success", "timing": [{"name": "compile", "started_at": "2026-01-05T13:09:37.139282Z", "completed_at": "2026-01-05T13:09:37.227327Z"}, {"name": "execute", "started_at": "2026-01-05T13:09:37.230038Z", "completed_at": "2026-01-05T13:09:37.942257Z"}], "thread_id": "Thread-1 (worker)", "execution_time": 0.8164985179901123, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.stg_arbitrum__deposits", "compiled": true, "compiled_code": "-- Staging model for FundsDeposited events from Arbitrum\n-- This model extracts deposit events where users initiate cross-chain bridge transactions\n\nWITH raw_deposits AS (\n\n    SELECT\n        -- Core event metadata - these identify WHEN and WHERE the event happened\n        timestamp_datetime,              -- When the event occurred (from blockchain)\n        transactionHash,                 -- Unique transaction identifier\n        blockchain,                      -- Which blockchain (should be 'arbitrum')\n        source_file,                     -- Which source file this came from (for lineage)\n        \n        -- Indexed fields from topics (these are searchable/filterable in blockchain)\n        -- Topics are like \"indexed columns\" in SQL - they're stored separately for fast queries\n        topic_destination_chain_id,      -- Chain ID where the funds are being sent TO\n        topic_deposit_id,                -- Unique deposit identifier (links deposits to fills)\n        topic_depositor,                  -- Address of the user who initiated the deposit\n        \n        -- Non-indexed fields from the event's data field\n        -- These are the actual business data about the deposit\n        funds_deposited_data_input_token,          -- Token address being deposited (on origin chain)\n        funds_deposited_data_output_token,         -- Token address to receive (on destination chain)\n        funds_deposited_data_input_amount,          -- Amount deposited (in input token units)\n        funds_deposited_data_output_amount,         -- Expected amount to receive (in output token units)\n        funds_deposited_data_recipient             -- Final recipient of the bridged funds\n        -- Optional fields: Not required for capital flow analysis, commented out in database schema\n        -- funds_deposited_data_quote_timestamp,       -- When the exchange rate quote was generated\n        -- funds_deposited_data_fill_deadline,         -- Deadline by which the deposit must be filled\n        -- funds_deposited_data_exclusivity_deadline,  -- Deadline for exclusive relayer rights\n        -- funds_deposited_data_exclusive_relayer      -- Address with exclusive fill rights (if any)\n        \n    FROM raw.arbitrum_logs_processed\n    \n    -- Filter: Only include rows where FundsDeposited data exists\n    -- Topic_0 is the event signature hash that identifies the event type\n    WHERE topic_0 = '0x32ed1a409ef04c7b0227189c3a103dc5ac10e775a15b785dcc510201f7c25ad3'\n        AND funds_deposited_data_input_amount IS NOT NULL\n        AND funds_deposited_data_output_amount IS NOT NULL\n),\n\n-- INPUT token metadata: filtered to origin chain (Arbitrum)\n-- This is correct because input_token is always on the origin chain\ninput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n    WHERE chain = 'arbitrum'\n\n),\n\n-- OUTPUT token metadata: includes chain_id for matching with destination_chain_id\n-- This allows us to find the correct token on whichever chain the funds are going to\noutput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals,\n        chain_id\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n\n),\n\ncleaned_deposits AS (\n    SELECT\n        \n        -- Timestamp: Convert text to proper timestamp type\n        -- The ETL process may store timestamps as Unix integers or ISO strings\n        CASE \n            WHEN timestamp_datetime ~ '^\\d+$' THEN          -- If it's pure digits, treat as Unix timestamp (seconds since 1970-01-01)\n                TO_TIMESTAMP(timestamp_datetime::BIGINT)    -- PostgreSQL's TO_TIMESTAMP converts Unix seconds to timestamp\n            ELSE \n                timestamp_datetime::TIMESTAMP               -- Otherwise, try to parse as ISO format string (e.g., \"2025-12-03 10:30:00\")\n        END AS deposit_timestamp,\n        \n        transactionHash AS transaction_hash,\n        blockchain,\n        source_file,\n        \n        -- ============================================================\n        -- INDEXED FIELDS (from topics) - Already decoded by ETL\n        -- ============================================================\n        -- These come from the event's \"topics\" array, which is indexed for fast blockchain queries\n        \n        -- Destination chain ID: Which blockchain the funds are being sent TO\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\n        -- Example: 1 = Ethereum, 8453 = Base, 137 = Polygon\n        -- Cast via NUMERIC first to handle decimal strings like \"8453.0\" from ETL\n        (topic_destination_chain_id::NUMERIC)::BIGINT AS destination_chain_id,\n        \n        -- Deposit ID: Unique identifier linking this deposit to its fill(s)\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\n        -- This is the KEY that connects deposits \u2194 fills across chains\n        -- Cast via NUMERIC first to handle decimal strings like \"12345.0\" from ETL\n        (topic_deposit_id::NUMERIC)::BIGINT AS deposit_id,\n        \n        -- Depositor address: Who initiated the bridge transaction\n        -- Already decoded by ETL to proper address format (0x...)\n        topic_depositor AS depositor_address,\n        \n        -- ============================================================\n        -- TOKEN INFORMATION (what was deposited)\n        -- ============================================================\n        \n        -- Input token: The token address being deposited on the origin chain\n        -- Already decoded by ETL to proper address format\n        -- Example: 0xaf88d065aC88dCc5619a6eeFdD463aAbdE3eE2c3 = USDC on Arbitrum\n        -- Normalize to lowercase for joining with token metadata\n        LOWER(funds_deposited_data_input_token) AS input_token_address,\n        \n        -- Output token: The token address to receive on the destination chain\n        -- Already decoded by ETL to proper address format\n        -- Example: 0xA0b86991c6218b36c1d19D4a2e9Eb0cE3606eB48 = USDC on Ethereum\n        -- Normalize to lowercase for joining with token metadata\n        LOWER(funds_deposited_data_output_token) AS output_token_address,\n        \n        -- ============================================================\n        -- AMOUNT INFORMATION (how much was deposited)\n        -- ============================================================\n        \n        -- Input amount: How much was deposited on the origin chain\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\n        -- Store as RAW amount first (before rescaling by decimals)\n        funds_deposited_data_input_amount::NUMERIC AS input_amount_raw,\n        \n        -- Output amount: Expected amount to receive on destination chain\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\n        -- Store as RAW amount first (before rescaling by decimals)\n        funds_deposited_data_output_amount::NUMERIC AS output_amount_raw,\n        \n        -- ============================================================\n        -- TIMING INFORMATION (deadlines and quotes)\n        -- ============================================================\n        -- NOTE: These fields are NOT required for capital flow analysis and are commented out in database schema\n        \n        -- Quote timestamp: When the exchange rate quote was generated\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\n        -- Used to determine which exchange rate was used for the bridge\n        -- Cast via NUMERIC first to handle decimal strings like \"1733234567.0\" from ETL\n        -- (funds_deposited_data_quote_timestamp::NUMERIC)::BIGINT AS quote_timestamp,\n        \n        -- Fill deadline: Latest timestamp by which the deposit must be filled\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\n        -- If not filled by this time, the deposit can be refunded\n        -- Cast via NUMERIC first to handle decimal strings like \"1733234567.0\" from ETL\n        -- (funds_deposited_data_fill_deadline::NUMERIC)::BIGINT AS fill_deadline,\n        \n        -- Exclusivity deadline: Latest timestamp for exclusive relayer rights\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\n        -- Before this deadline, only the exclusive_relayer can fill this deposit\n        -- Cast via NUMERIC first to handle decimal strings like \"1733234567.0\" from ETL\n        -- (funds_deposited_data_exclusivity_deadline::NUMERIC)::BIGINT AS exclusivity_deadline,\n        \n        -- ============================================================\n        -- USER & RELAYER INFORMATION\n        -- ============================================================\n        \n        -- Recipient: Who receives the funds on the destination chain\n        -- Already decoded by ETL to proper address format\n        -- Usually the same as depositor, but can be different (gift/transfer)\n        funds_deposited_data_recipient AS recipient_address\n        \n        -- Exclusive relayer: Address with exclusive fill rights (if any)\n        -- Already decoded by ETL to proper address format\n        -- Used for priority/guaranteed fills (NULL if no exclusive relayer)\n        -- NOTE: Field is NOT required for capital flow analysis and is commented out in database schema\n        -- funds_deposited_data_exclusive_relayer AS exclusive_relayer_address\n        \n    FROM raw_deposits\n)\n\n\n-- ============================================================\n-- FINAL SELECT: Join with token metadata and rescale amounts\n-- ============================================================\nSELECT\n    -- Event identity\n    c.deposit_timestamp,\n    c.transaction_hash,\n    c.blockchain,\n    c.source_file,\n    \n    -- Indexed fields\n    c.destination_chain_id,\n    c.deposit_id,\n    c.depositor_address,\n    \n    -- \u2728 NEW: Token information WITH NAMES\n    -- Input token (what was deposited on origin chain - Arbitrum)\n    c.input_token_address,\n    input_tok.token_symbol AS input_token_symbol,        -- e.g., 'USDC', 'WETH'\n    \n    -- Output token (what will be received on destination chain)\n    c.output_token_address,\n    output_tok.token_symbol AS output_token_symbol,      -- e.g., 'USDC', 'WETH'\n    \n    -- \u2728 NEW: Rescaled amounts (human-readable)\n    -- These use the rescale_amount macro which divides raw amount by 10^decimals\n    -- Example: 5000000 raw USDC (6 decimals) \u2192 5.0 USDC\n    \n    c.input_amount_raw / POWER(10, COALESCE(input_tok.decimals, 18))\n AS input_amount,\n    \n    CASE \n        WHEN c.output_token_address = '0x0000000000000000000000000000000000000000' \n        THEN c.output_amount_raw / POWER(10, COALESCE(input_tok.decimals, 18))\n        ELSE c.output_amount_raw / POWER(10, COALESCE(output_tok.decimals, 18))\n    END\n AS output_amount,\n    \n    -- \u2728 NEW: Raw amounts (preserved for auditing)\n    -- These are the original blockchain values before rescaling\n    -- Example: 5.0 USDC is stored as 5000000 on blockchain\n    c.input_amount_raw,\n    c.output_amount_raw,\n    \n    -- Timing (commented out - available in schema but not included in output)\n    -- quote_timestamp,\n    -- fill_deadline,\n    -- exclusivity_deadline,\n    \n    -- User info\n    c.recipient_address\n    -- exclusive_relayer_address  -- Commented out - available in schema but not included in output\n    \nFROM cleaned_deposits c\n\n-- Join with token metadata to get decimals and symbols for INPUT token\n-- Join on address only - we already filtered to origin chain (Arbitrum)\nLEFT JOIN input_token_meta AS input_tok\n    ON c.input_token_address = input_tok.token_address\n\n-- Join with token metadata to get decimals and symbols for OUTPUT token\n-- Join on BOTH address AND chain_id to match the correct destination chain\nLEFT JOIN output_token_meta AS output_tok\n    ON c.output_token_address = output_tok.token_address\n    AND c.destination_chain_id = output_tok.chain_id\n\n-- Data quality: Only include rows with essential fields populated\nWHERE c.deposit_id IS NOT NULL\n    AND c.depositor_address IS NOT NULL\n    AND c.input_token_address IS NOT NULL\n    AND c.output_token_address IS NOT NULL\n    AND c.input_amount_raw IS NOT NULL\n    AND c.output_amount_raw IS NOT NULL", "relation_name": "\"across_analytics\".\"dbt_staging\".\"stg_arbitrum__deposits\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2026-01-05T13:09:37.204613Z", "completed_at": "2026-01-05T13:09:37.290323Z"}, {"name": "execute", "started_at": "2026-01-05T13:09:37.305892Z", "completed_at": "2026-01-05T13:09:38.010525Z"}], "thread_id": "Thread-4 (worker)", "execution_time": 0.8806860446929932, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.stg_ethereum__deposits", "compiled": true, "compiled_code": "-- Staging model for FundsDeposited events from Ethereum\n-- This model extracts deposit events where users initiate cross-chain bridge transactions\n\nWITH raw_deposits AS (\n\n    SELECT\n        -- Core event metadata - these identify WHEN and WHERE the event happened\n        timestamp_datetime,              -- When the event occurred (from blockchain)\n        transactionHash,                 -- Unique transaction identifier\n        blockchain,                      -- Which blockchain (should be 'ethereum')\n        source_file,                     -- Which source file this came from (for lineage)\n        \n        -- Indexed fields from topics (these are searchable/filterable in blockchain)\n        -- Topics are like \"indexed columns\" in SQL - they're stored separately for fast queries\n        topic_destination_chain_id,      -- Chain ID where the funds are being sent TO\n        topic_deposit_id,                -- Unique deposit identifier (links deposits to fills)\n        topic_depositor,                  -- Address of the user who initiated the deposit\n        \n        -- Non-indexed fields from the event's data field\n        -- These are the actual business data about the deposit\n        funds_deposited_data_input_token,          -- Token address being deposited (on origin chain)\n        funds_deposited_data_output_token,         -- Token address to receive (on destination chain)\n        funds_deposited_data_input_amount,          -- Amount deposited (in input token units)\n        funds_deposited_data_output_amount,         -- Expected amount to receive (in output token units)\n        funds_deposited_data_recipient             -- Final recipient of the bridged funds\n        \n    FROM raw.ethereum_logs_processed\n    \n    -- Filter: Only include rows where FundsDeposited data exists\n    -- Topic_0 is the event signature hash that identifies the event type\n    WHERE topic_0 = '0x32ed1a409ef04c7b0227189c3a103dc5ac10e775a15b785dcc510201f7c25ad3'\n        AND funds_deposited_data_input_amount IS NOT NULL\n        AND funds_deposited_data_output_amount IS NOT NULL\n),\n\n-- INPUT token metadata: filtered to origin chain (Ethereum)\n-- This is correct because input_token is always on the origin chain\ninput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n    WHERE chain = 'ethereum'\n\n),\n\n-- OUTPUT token metadata: includes chain_id for matching with destination_chain_id\n-- This allows us to find the correct token on whichever chain the funds are going to\noutput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals,\n        chain_id\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n\n),\n\ncleaned_deposits AS (\n    SELECT\n        \n        -- Timestamp: Convert text to proper timestamp type\n        CASE \n            WHEN timestamp_datetime ~ '^\\d+$' THEN\n                TO_TIMESTAMP(timestamp_datetime::BIGINT)\n            ELSE \n                timestamp_datetime::TIMESTAMP\n        END AS deposit_timestamp,\n        \n        transactionHash AS transaction_hash,\n        blockchain,\n        source_file,\n        \n        -- Destination chain ID\n        (topic_destination_chain_id::NUMERIC)::BIGINT AS destination_chain_id,\n        \n        -- Deposit ID\n        (topic_deposit_id::NUMERIC)::BIGINT AS deposit_id,\n        \n        -- Depositor address\n        topic_depositor AS depositor_address,\n        \n        -- Token addresses (normalized to lowercase for joining)\n        LOWER(funds_deposited_data_input_token) AS input_token_address,\n        LOWER(funds_deposited_data_output_token) AS output_token_address,\n        \n        -- Raw amounts (before rescaling)\n        funds_deposited_data_input_amount::NUMERIC AS input_amount_raw,\n        funds_deposited_data_output_amount::NUMERIC AS output_amount_raw,\n        \n        -- Recipient\n        funds_deposited_data_recipient AS recipient_address\n        \n    FROM raw_deposits\n)\n\n-- Final SELECT: Join with token metadata and rescale amounts\nSELECT\n    -- Event identity\n    c.deposit_timestamp,\n    c.transaction_hash,\n    c.blockchain,\n    c.source_file,\n    \n    -- Indexed fields\n    c.destination_chain_id,\n    c.deposit_id,\n    c.depositor_address,\n    \n    -- Input token (what was deposited on origin chain - Ethereum)\n    c.input_token_address,\n    input_tok.token_symbol AS input_token_symbol,\n    \n    -- Output token (what will be received on destination chain)\n    c.output_token_address,\n    output_tok.token_symbol AS output_token_symbol,\n    \n    -- Rescaled amounts (human-readable)\n    \n    c.input_amount_raw / POWER(10, COALESCE(input_tok.decimals, 18))\n AS input_amount,\n    \n    CASE \n        WHEN c.output_token_address = '0x0000000000000000000000000000000000000000' \n        THEN c.output_amount_raw / POWER(10, COALESCE(input_tok.decimals, 18))\n        ELSE c.output_amount_raw / POWER(10, COALESCE(output_tok.decimals, 18))\n    END\n AS output_amount,\n    \n    -- Raw amounts (preserved for auditing)\n    c.input_amount_raw,\n    c.output_amount_raw,\n    \n    -- User info\n    c.recipient_address\n    \nFROM cleaned_deposits c\n\n-- Join with token metadata for INPUT token (origin chain)\nLEFT JOIN input_token_meta AS input_tok\n    ON c.input_token_address = input_tok.token_address\n\n-- Join with token metadata for OUTPUT token (destination chain)\nLEFT JOIN output_token_meta AS output_tok\n    ON c.output_token_address = output_tok.token_address\n    AND c.destination_chain_id = output_tok.chain_id\n\n-- Data quality: Only include rows with essential fields populated\nWHERE c.deposit_id IS NOT NULL\n    AND c.depositor_address IS NOT NULL\n    AND c.input_token_address IS NOT NULL\n    AND c.output_token_address IS NOT NULL\n    AND c.input_amount_raw IS NOT NULL\n    AND c.output_amount_raw IS NOT NULL", "relation_name": "\"across_analytics\".\"dbt_staging\".\"stg_ethereum__deposits\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2026-01-05T13:09:37.220546Z", "completed_at": "2026-01-05T13:09:37.416836Z"}, {"name": "execute", "started_at": "2026-01-05T13:09:37.424783Z", "completed_at": "2026-01-05T13:09:38.026900Z"}], "thread_id": "Thread-3 (worker)", "execution_time": 0.8745443820953369, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.stg_arbitrum__refunds", "compiled": true, "compiled_code": "-- Staging model for ExecutedRelayerRefundRoot events from Arbitrum\r\n\r\nWITH raw_refunds AS (\r\n\r\n    SELECT\r\n        -- Core event metadata - these identify WHEN and WHERE the event happened\r\n        timestamp_datetime,              -- When the event occurred (from blockchain)\r\n        transactionHash,                 -- Unique transaction identifier\r\n        blockchain,                      -- Which blockchain (should be 'arbitrum')\r\n        source_file,                     -- Which source file this came from (for lineage)\r\n        \r\n        -- Indexed fields from topics (these are searchable/filterable in blockchain)\r\n        -- Topics are like \"indexed columns\" in SQL - they're stored separately for fast queries\r\n        topic_chain_id,                  -- Chain ID where the refund was executed\r\n        topic_root_bundle_id,            -- Merkle root bundle identifier (groups multiple refunds)\r\n        topic_leaf_id,                   -- Leaf index in the merkle tree (identifies this specific refund batch)\r\n        \r\n        -- Non-indexed fields from the event's data field\r\n        -- These are the actual business data about the refund\r\n        amount_to_return,                -- Total amount of capital returned in this batch\r\n        l2_token_address,                -- Token address being refunded (on Arbitrum L2)\r\n        refund_amounts,                  -- Comma-separated list of individual refund amounts\r\n        refund_addresses,                -- Comma-separated list of relayer addresses receiving refunds\r\n        refund_count                     -- Number of relayers being refunded in this batch\r\n        \r\n    FROM raw.arbitrum_logs_processed\r\n    \r\n    -- Filter: Only include rows where ExecutedRelayerRefundRoot data exists\r\n    -- Topic_0 is the event signature hash that identifies the event type\r\n    WHERE topic_0 = '0xf4ad92585b1bc117fbdd644990adf0827bc4c95baeae8a23322af807b6d0020e'\r\n        AND amount_to_return IS NOT NULL\r\n        AND l2_token_address IS NOT NULL\r\n        AND refund_count IS NOT NULL\r\n),\r\n\r\ncleaned_refunds AS (\r\n    SELECT\r\n        \r\n        -- Timestamp: Convert text to proper timestamp type\r\n        CASE \r\n            WHEN timestamp_datetime ~ '^\\d+$' THEN          -- If it's pure digits, treat as Unix timestamp (seconds since 1970-01-01)\r\n                TO_TIMESTAMP(timestamp_datetime::BIGINT)    -- PostgreSQL's TO_TIMESTAMP converts Unix seconds to timestamp\r\n            ELSE \r\n                timestamp_datetime::TIMESTAMP               -- Otherwise, try to parse as ISO format string (e.g., \"2025-12-03 10:30:00\")\r\n        END AS refund_timestamp,\r\n        \r\n        transactionHash AS transaction_hash,\r\n        blockchain,\r\n        source_file,\r\n        \r\n        -- ============================================================\r\n        -- INDEXED FIELDS (from topics) - Already decoded by ETL\r\n        -- ============================================================\r\n        -- These come from the event's \"topics\" array, which is indexed for fast blockchain queries\r\n        \r\n        -- Chain ID: Which blockchain this refund was executed on\r\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\r\n        -- Example: 42161 = Arbitrum, 1 = Ethereum, 8453 = Base\r\n        -- Cast via NUMERIC first to handle decimal strings like \"42161.0\" from ETL\r\n        (topic_chain_id::NUMERIC)::BIGINT AS chain_id,\r\n        \r\n        -- Root bundle ID: Groups multiple refunds together in a merkle tree\r\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\r\n        -- Multiple refund batches can be grouped into one merkle root for gas efficiency\r\n        -- Cast via NUMERIC first to handle decimal strings like \"12345.0\" from ETL\r\n        (topic_root_bundle_id::NUMERIC)::BIGINT AS root_bundle_id,\r\n        \r\n        -- Leaf ID: Identifies this specific refund batch within the merkle tree\r\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\r\n        -- Each leaf represents one batch of refunds to multiple relayers\r\n        -- Cast via NUMERIC first to handle decimal strings like \"789.0\" from ETL\r\n        (topic_leaf_id::NUMERIC)::BIGINT AS leaf_id,\r\n        \r\n        -- ============================================================\r\n        -- TOKEN INFORMATION (what was refunded)\r\n        -- ============================================================\r\n        \r\n        -- L2 token address: The token being refunded on Arbitrum\r\n        -- Already decoded by ETL to proper address format (0x...)\r\n        -- Example: 0xaf88d065aC88dCc5619a6eeFdD463aAbdE3eE2c3 = USDC on Arbitrum\r\n        l2_token_address AS refund_token_address,\r\n        \r\n        -- ============================================================\r\n        -- AMOUNT INFORMATION (how much was refunded)\r\n        -- ============================================================\r\n        \r\n        -- Amount to return: Total capital returned in this batch\r\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\r\n        -- This is the sum of all individual refund amounts in the batch\r\n        -- Stored as NUMERIC to handle large token amounts (18 decimals)\r\n        amount_to_return::NUMERIC AS total_refund_amount,\r\n        \r\n        -- Refund amounts: Individual refund amounts as comma-separated string\r\n        -- Format: \"1000000000000000000,2000000000000000000,3000000000000000000\"\r\n        -- Each value is in token's base units (e.g., wei for ETH, 6 decimals for USDC)\r\n        -- This is a comma-separated string because the ETL processes dynamic arrays\r\n        refund_amounts AS refund_amounts_string,\r\n        \r\n        -- Refund addresses: Relayer addresses receiving refunds as comma-separated string\r\n        -- Format: \"0xAAA...,0xBBB...,0xCCC...\"\r\n        -- Matches 1:1 with refund_amounts (first address gets first amount, etc.)\r\n        -- This is a comma-separated string because the ETL processes dynamic arrays\r\n        refund_addresses AS refund_addresses_string,\r\n        \r\n        -- Refund count: Number of relayers being refunded in this batch\r\n        -- Already converted by ETL to integer\r\n        -- This tells us how many individual refunds are in the arrays above\r\n        refund_count::INTEGER AS refund_count\r\n        \r\n    FROM raw_refunds\r\n)\r\n\r\n-- Final SELECT: Add any computed fields and ensure data quality\r\nSELECT\r\n    -- Event identity\r\n    refund_timestamp,\r\n    transaction_hash,\r\n    blockchain,\r\n    source_file,\r\n    \r\n    -- Indexed fields (merkle tree identifiers)\r\n    chain_id,\r\n    root_bundle_id,\r\n    leaf_id,\r\n    \r\n    -- Token info\r\n    refund_token_address,\r\n    \r\n    -- Amounts\r\n    total_refund_amount,\r\n    refund_amounts_string,\r\n    refund_addresses_string,\r\n    refund_count\r\n    \r\nFROM cleaned_refunds\r\n\r\n-- Data quality: Only include rows with essential fields populated\r\nWHERE chain_id IS NOT NULL\r\n    AND refund_token_address IS NOT NULL\r\n    AND total_refund_amount IS NOT NULL\r\n    AND refund_count IS NOT NULL", "relation_name": "\"across_analytics\".\"dbt_staging\".\"stg_arbitrum__refunds\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2026-01-05T13:09:37.187069Z", "completed_at": "2026-01-05T13:09:37.260171Z"}, {"name": "execute", "started_at": "2026-01-05T13:09:37.275195Z", "completed_at": "2026-01-05T13:09:38.060929Z"}], "thread_id": "Thread-2 (worker)", "execution_time": 0.9345264434814453, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.stg_arbitrum__fills", "compiled": true, "compiled_code": "-- Staging model for FilledRelay events from Arbitrum\n-- This model extracts fill events where relayers complete cross-chain bridge transactions\n\nWITH raw_fills AS (\n\n    SELECT\n        -- Core event metadata - these identify WHEN and WHERE the event happened\n        timestamp_datetime,              -- When the event occurred (from blockchain)\n        transactionHash,                 -- Unique transaction identifier\n        blockchain,                      -- Which blockchain (should be 'arbitrum')\n        source_file,                     -- Which source file this came from (for lineage)\n        \n        -- Indexed fields from topics (these are searchable/filterable in blockchain)\n        -- Topics are like \"indexed columns\" in SQL - they're stored separately for fast queries\n        topic_origin_chain_id,           -- Chain ID where the original deposit happened\n        topic_deposit_id,                -- Unique deposit identifier (links to deposit event)\n        topic_relayer,                   -- Address of the relayer who provided liquidity\n        \n        -- Non-indexed fields from the event's data field\n        -- These are the actual business data about the fill\n        filled_relay_data_input_token,          -- Token address on origin chain\n        filled_relay_data_output_token,          -- Token address on destination chain\n        filled_relay_data_input_amount,          -- Amount sent (in origin token units)\n        filled_relay_data_output_amount,         -- Amount received (in destination token units)\n        filled_relay_data_repayment_chain_id,     -- Where relayer gets reimbursed\n        filled_relay_data_exclusive_relayer,     -- Address with exclusive fill rights (if any)\n        filled_relay_data_depositor,             -- Original user who initiated the bridge\n        filled_relay_data_recipient,              -- Final recipient of the bridged funds\n        \n        -- Gas data (for relayer cost analysis)\n        gas_price_wei,\n        gas_used\n        \n    FROM raw.arbitrum_logs_processed\n    \n    -- Filter: Only include rows where FilledRelay data exists\n    WHERE topic_0 = '0x44b559f101f8fbcc8a0ea43fa91a05a729a5ea6e14a7c75aa750374690137208'\n        AND filled_relay_data_input_amount IS NOT NULL\n        AND filled_relay_data_output_amount IS NOT NULL\n),\n\n-- INPUT token metadata: includes chain_id for matching with origin_chain_id\n-- This allows us to find the correct token on whichever chain the deposit came from\ninput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals,\n        chain_id\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n\n),\n\n-- OUTPUT token metadata: filtered to destination chain (Arbitrum)\n-- This is correct because on fills, output_token is always on the destination chain\noutput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n    WHERE chain = 'arbitrum'\n\n),\n\ncleaned_fills AS (\n    SELECT\n        \n        -- Timestamp: Convert text to proper timestamp type\n        \n        CASE \n            WHEN timestamp_datetime ~ '^\\d+$' THEN          -- If it's pure digits, treat as Unix timestamp (seconds since 1970-01-01)\n                TO_TIMESTAMP(timestamp_datetime::BIGINT)    -- PostgreSQL's TO_TIMESTAMP can parse various formats, but we use the simplest approach\n            ELSE \n                timestamp_datetime::TIMESTAMP               -- Otherwise, try to parse as ISO format string\n        END AS fill_timestamp,\n        \n        transactionHash AS transaction_hash,\n        blockchain,\n        source_file,\n        \n        -- ============================================================\n        -- INDEXED FIELDS (from topics) - Already decoded by ETL\n        -- ============================================================\n        -- These come from the event's \"topics\" array, which is indexed for fast blockchain queries\n        \n        -- Origin chain ID: Which blockchain the funds came FROM\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\n        -- Example: 1 = Ethereum, 42161 = Arbitrum, 8453 = Base\n        -- Cast via NUMERIC first to handle decimal strings like \"8453.0\" from ETL\n        (topic_origin_chain_id::NUMERIC)::BIGINT AS origin_chain_id,\n        \n        -- Deposit ID: Unique identifier linking this fill to its original deposit\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\n        -- This is the KEY that connects deposits \u2194 fills across chains\n        -- Cast via NUMERIC first to handle decimal strings like \"12345.0\" from ETL\n        (topic_deposit_id::NUMERIC)::BIGINT AS deposit_id,\n        \n        -- Relayer address: Who provided the liquidity\n        -- Already decoded by ETL to proper address format (0x...)\n        topic_relayer AS relayer_address,\n        \n        -- ============================================================\n        -- TOKEN INFORMATION (what was bridged)\n        -- ============================================================\n        \n        -- Input token: The token address on the origin chain\n        -- Already decoded by ETL to proper address format\n        -- Example: 0xA0b86991c6218b36c1d19D4a2e9Eb0cE3606eB48 = USDC on Ethereum\n        -- Normalize to lowercase for joining with token metadata\n        LOWER(filled_relay_data_input_token) AS input_token_address,\n        \n        -- Output token: The token address on the destination chain (Arbitrum in this case)\n        -- Already decoded by ETL to proper address format\n        -- Normalize to lowercase for joining with token metadata\n        LOWER(filled_relay_data_output_token) AS output_token_address,\n        \n        -- ============================================================\n        -- AMOUNT INFORMATION (how much was bridged)\n        -- ============================================================\n        \n        -- Input amount: How much was sent from origin chain\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\n        -- Store as RAW amount first (before rescaling by decimals)\n        filled_relay_data_input_amount::NUMERIC AS input_amount_raw,\n        \n        -- Output amount: How much was received on destination chain\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\n        -- Usually slightly less than input due to fees/spread\n        -- Store as RAW amount first (before rescaling by decimals)\n        filled_relay_data_output_amount::NUMERIC AS output_amount_raw,\n        \n        -- ============================================================\n        -- RELAYER & ROUTING INFORMATION\n        -- ============================================================\n        \n        -- Repayment chain ID: Where the relayer gets reimbursed\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\n        -- Relayers front capital, then get paid back (often on a different chain)\n        -- Cast via NUMERIC first to handle decimal strings like \"1.0\" from ETL\n        (filled_relay_data_repayment_chain_id::NUMERIC)::BIGINT AS repayment_chain_id,\n        \n        -- Exclusive relayer: If set, only this address can fill this deposit\n        -- Already decoded by ETL to proper address format\n        -- Used for priority/guaranteed fills (NULL if no exclusive relayer)\n        filled_relay_data_exclusive_relayer AS exclusive_relayer_address,\n        \n        -- ============================================================\n        -- USER INFORMATION\n        -- ============================================================\n        \n        -- Depositor: The original user who initiated the bridge\n        -- Already decoded by ETL to proper address format\n        filled_relay_data_depositor AS depositor_address,\n        \n        -- Recipient: Who receives the funds on the destination chain\n        -- Already decoded by ETL to proper address format\n        -- Usually the same as depositor, but can be different (gift/transfer)\n        filled_relay_data_recipient AS recipient_address,\n        \n        -- ============================================================\n        -- GAS DATA (for relayer cost analysis)\n        -- ============================================================\n        gas_price_wei::BIGINT AS gas_price_wei,\n        gas_used::BIGINT AS gas_used\n        \n    FROM raw_fills\n)\n\n-- ============================================================\n-- FINAL SELECT: Join with token metadata and rescale amounts\n-- ============================================================\nSELECT\n    -- Event identity\n    c.fill_timestamp,\n    c.transaction_hash,\n    c.blockchain,\n    c.source_file,\n    \n    -- Indexed fields\n    c.origin_chain_id,\n    c.deposit_id,\n    c.relayer_address,\n    \n    -- Token info WITH NAMES\n    -- Input token (what was deposited on origin chain)\n    c.input_token_address,\n    input_tok.token_symbol AS input_token_symbol,        -- e.g., 'USDC', 'WETH'\n    \n    -- Output token (what was received on destination chain - Arbitrum)\n    c.output_token_address,\n    output_tok.token_symbol AS output_token_symbol,      -- e.g., 'USDC', 'WETH'\n    \n    -- Rescaled amounts (human-readable)\n    -- These use the rescale_amount macro which divides raw amount by 10^decimals\n    -- Example: 5000000 raw USDC (6 decimals) \u2192 5.0 USDC\n    \n    c.input_amount_raw / POWER(10, COALESCE(input_tok.decimals, 18))\n AS input_amount,\n    \n    c.output_amount_raw / POWER(10, COALESCE(output_tok.decimals, 18))\n AS output_amount,\n    \n    -- Raw amounts (preserved for auditing)\n    -- These are the original blockchain values before rescaling\n    -- Example: 5.0 USDC is stored as 5000000 on blockchain\n    c.input_amount_raw,\n    c.output_amount_raw,\n    \n    -- Relayer routing\n    c.repayment_chain_id,\n    c.exclusive_relayer_address,\n    \n    -- User info\n    c.depositor_address,\n    c.recipient_address,\n    \n    -- Gas data (for relayer cost analysis)\n    c.gas_price_wei,\n    c.gas_used,\n    (c.gas_price_wei * c.gas_used) AS gas_cost_wei\n    \nFROM cleaned_fills c\n\n-- Join with token metadata to get decimals and symbols for INPUT token\n-- Join on BOTH address AND chain_id to match the correct origin chain\nLEFT JOIN input_token_meta AS input_tok\n    ON c.input_token_address = input_tok.token_address\n    AND c.origin_chain_id = input_tok.chain_id\n\n-- Join with token metadata to get decimals and symbols for OUTPUT token\n-- Join on address only - we already filtered to destination chain (Arbitrum)\nLEFT JOIN output_token_meta AS output_tok\n    ON c.output_token_address = output_tok.token_address\n\n-- Data quality: Only include rows with essential fields populated\nWHERE c.deposit_id IS NOT NULL\n    AND c.relayer_address IS NOT NULL\n    AND c.input_token_address IS NOT NULL\n    AND c.output_token_address IS NOT NULL\n    AND c.input_amount_raw IS NOT NULL\n    AND c.output_amount_raw IS NOT NULL", "relation_name": "\"across_analytics\".\"dbt_staging\".\"stg_arbitrum__fills\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2026-01-05T13:09:38.038737Z", "completed_at": "2026-01-05T13:09:38.111420Z"}, {"name": "execute", "started_at": "2026-01-05T13:09:38.112800Z", "completed_at": "2026-01-05T13:09:38.631399Z"}], "thread_id": "Thread-1 (worker)", "execution_time": 0.6080572605133057, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.stg_ethereum__fills", "compiled": true, "compiled_code": "-- Staging model for FilledRelay events from Ethereum\n-- This model extracts fill events where relayers complete cross-chain bridge transactions\n\nWITH raw_fills AS (\n\n    SELECT\n        -- Core event metadata - these identify WHEN and WHERE the event happened\n        timestamp_datetime,              -- When the event occurred (from blockchain)\n        transactionHash,                 -- Unique transaction identifier\n        blockchain,                      -- Which blockchain (should be 'ethereum')\n        source_file,                     -- Which source file this came from (for lineage)\n        \n        -- Indexed fields from topics (these are searchable/filterable in blockchain)\n        -- Topics are like \"indexed columns\" in SQL - they're stored separately for fast queries\n        topic_origin_chain_id,           -- Chain ID where the original deposit happened\n        topic_deposit_id,                -- Unique deposit identifier (links to deposit event)\n        topic_relayer,                   -- Address of the relayer who provided liquidity\n        \n        -- Non-indexed fields from the event's data field\n        -- These are the actual business data about the fill\n        filled_relay_data_input_token,          -- Token address on origin chain\n        filled_relay_data_output_token,          -- Token address on destination chain\n        filled_relay_data_input_amount,          -- Amount sent (in origin token units)\n        filled_relay_data_output_amount,         -- Amount received (in destination token units)\n        filled_relay_data_repayment_chain_id,     -- Where relayer gets reimbursed\n        filled_relay_data_exclusive_relayer,     -- Address with exclusive fill rights (if any)\n        filled_relay_data_depositor,             -- Original user who initiated the bridge\n        filled_relay_data_recipient,              -- Final recipient of the bridged funds\n        \n        -- Gas data (for relayer cost analysis)\n        gas_price_wei,\n        gas_used\n        \n    FROM raw.ethereum_logs_processed\n    \n    -- Filter: Only include rows where FilledRelay data exists\n    WHERE topic_0 = '0x44b559f101f8fbcc8a0ea43fa91a05a729a5ea6e14a7c75aa750374690137208'\n        AND filled_relay_data_input_amount IS NOT NULL\n        AND filled_relay_data_output_amount IS NOT NULL\n),\n\n-- INPUT token metadata: includes chain_id for matching with origin_chain_id\n-- This allows us to find the correct token on whichever chain the deposit came from\ninput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals,\n        chain_id\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n\n),\n\n-- OUTPUT token metadata: filtered to destination chain (Ethereum)\n-- This is correct because on fills, output_token is always on the destination chain\noutput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n    WHERE chain = 'ethereum'\n\n),\n\ncleaned_fills AS (\n    SELECT\n        \n        -- Timestamp: Convert text to proper timestamp type\n        \n        CASE \n            WHEN timestamp_datetime ~ '^\\d+$' THEN          -- If it's pure digits, treat as Unix timestamp (seconds since 1970-01-01)\n                TO_TIMESTAMP(timestamp_datetime::BIGINT)    -- PostgreSQL's TO_TIMESTAMP can parse various formats, but we use the simplest approach\n            ELSE \n                timestamp_datetime::TIMESTAMP               -- Otherwise, try to parse as ISO format string\n        END AS fill_timestamp,\n        \n        transactionHash AS transaction_hash,\n        blockchain,\n        source_file,\n        \n        -- ============================================================\n        -- INDEXED FIELDS (from topics) - Already decoded by ETL\n        -- ============================================================\n        -- These come from the event's \"topics\" array, which is indexed for fast blockchain queries\n        \n        -- Origin chain ID: Which blockchain the funds came FROM\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\n        -- Example: 1 = Ethereum, 42161 = Arbitrum, 8453 = Base\n        -- Cast via NUMERIC first to handle decimal strings like \"8453.0\" from ETL\n        (topic_origin_chain_id::NUMERIC)::BIGINT AS origin_chain_id,\n        \n        -- Deposit ID: Unique identifier linking this fill to its original deposit\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\n        -- This is the KEY that connects deposits \u2194 fills across chains\n        -- Cast via NUMERIC first to handle decimal strings like \"12345.0\" from ETL\n        (topic_deposit_id::NUMERIC)::BIGINT AS deposit_id,\n        \n        -- Relayer address: Who provided the liquidity\n        -- Already decoded by ETL to proper address format (0x...)\n        topic_relayer AS relayer_address,\n        \n        -- ============================================================\n        -- TOKEN INFORMATION (what was bridged)\n        -- ============================================================\n        \n        -- Input token: The token address on the origin chain\n        -- Already decoded by ETL to proper address format\n        -- Example: 0xA0b86991c6218b36c1d19D4a2e9Eb0cE3606eB48 = USDC on Ethereum\n        -- Normalize to lowercase for joining with token metadata\n        LOWER(filled_relay_data_input_token) AS input_token_address,\n        \n        -- Output token: The token address on the destination chain (Ethereum in this case)\n        -- Already decoded by ETL to proper address format\n        -- Normalize to lowercase for joining with token metadata\n        LOWER(filled_relay_data_output_token) AS output_token_address,\n        \n        -- ============================================================\n        -- AMOUNT INFORMATION (how much was bridged)\n        -- ============================================================\n        \n        -- Input amount: How much was sent from origin chain\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\n        -- Store as RAW amount first (before rescaling by decimals)\n        filled_relay_data_input_amount::NUMERIC AS input_amount_raw,\n        \n        -- Output amount: How much was received on destination chain\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\n        -- Usually slightly less than input due to fees/spread\n        -- Store as RAW amount first (before rescaling by decimals)\n        filled_relay_data_output_amount::NUMERIC AS output_amount_raw,\n        \n        -- ============================================================\n        -- RELAYER & ROUTING INFORMATION\n        -- ============================================================\n        \n        -- Repayment chain ID: Where the relayer gets reimbursed\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\n        -- Relayers front capital, then get paid back (often on a different chain)\n        -- Cast via NUMERIC first to handle decimal strings like \"1.0\" from ETL\n        (filled_relay_data_repayment_chain_id::NUMERIC)::BIGINT AS repayment_chain_id,\n        \n        -- Exclusive relayer: If set, only this address can fill this deposit\n        -- Already decoded by ETL to proper address format\n        -- Used for priority/guaranteed fills (NULL if no exclusive relayer)\n        filled_relay_data_exclusive_relayer AS exclusive_relayer_address,\n        \n        -- ============================================================\n        -- USER INFORMATION\n        -- ============================================================\n        \n        -- Depositor: The original user who initiated the bridge\n        -- Already decoded by ETL to proper address format\n        filled_relay_data_depositor AS depositor_address,\n        \n        -- Recipient: Who receives the funds on the destination chain\n        -- Already decoded by ETL to proper address format\n        -- Usually the same as depositor, but can be different (gift/transfer)\n        filled_relay_data_recipient AS recipient_address,\n        \n        -- ============================================================\n        -- GAS DATA (for relayer cost analysis)\n        -- ============================================================\n        gas_price_wei::BIGINT AS gas_price_wei,\n        gas_used::BIGINT AS gas_used\n        \n    FROM raw_fills\n)\n\n-- ============================================================\n-- FINAL SELECT: Join with token metadata and rescale amounts\n-- ============================================================\nSELECT\n    -- Event identity\n    c.fill_timestamp,\n    c.transaction_hash,\n    c.blockchain,\n    c.source_file,\n    \n    -- Indexed fields\n    c.origin_chain_id,\n    c.deposit_id,\n    c.relayer_address,\n    \n    -- Token info WITH NAMES\n    -- Input token (what was deposited on origin chain)\n    c.input_token_address,\n    input_tok.token_symbol AS input_token_symbol,        -- e.g., 'USDC', 'WETH'\n    \n    -- Output token (what was received on destination chain - Ethereum)\n    c.output_token_address,\n    output_tok.token_symbol AS output_token_symbol,      -- e.g., 'USDC', 'WETH'\n    \n    -- Rescaled amounts (human-readable)\n    -- These use the rescale_amount macro which divides raw amount by 10^decimals\n    -- Example: 5000000 raw USDC (6 decimals) \u2192 5.0 USDC\n    \n    c.input_amount_raw / POWER(10, COALESCE(input_tok.decimals, 18))\n AS input_amount,\n    \n    c.output_amount_raw / POWER(10, COALESCE(output_tok.decimals, 18))\n AS output_amount,\n    \n    -- Raw amounts (preserved for auditing)\n    -- These are the original blockchain values before rescaling\n    -- Example: 5.0 USDC is stored as 5000000 on blockchain\n    c.input_amount_raw,\n    c.output_amount_raw,\n    \n    -- Relayer routing\n    c.repayment_chain_id,\n    c.exclusive_relayer_address,\n    \n    -- User info\n    c.depositor_address,\n    c.recipient_address,\n    \n    -- Gas data (for relayer cost analysis)\n    c.gas_price_wei,\n    c.gas_used,\n    (c.gas_price_wei * c.gas_used) AS gas_cost_wei\n    \nFROM cleaned_fills c\n\n-- Join with token metadata to get decimals and symbols for INPUT token\n-- Join on BOTH address AND chain_id to match the correct origin chain\nLEFT JOIN input_token_meta AS input_tok\n    ON c.input_token_address = input_tok.token_address\n    AND c.origin_chain_id = input_tok.chain_id\n\n-- Join with token metadata to get decimals and symbols for OUTPUT token\n-- Join on address only - we already filtered to destination chain (Ethereum)\nLEFT JOIN output_token_meta AS output_tok\n    ON c.output_token_address = output_tok.token_address\n\n-- Data quality: Only include rows with essential fields populated\nWHERE c.deposit_id IS NOT NULL\n    AND c.relayer_address IS NOT NULL\n    AND c.input_token_address IS NOT NULL\n    AND c.output_token_address IS NOT NULL\n    AND c.input_amount_raw IS NOT NULL\n    AND c.output_amount_raw IS NOT NULL", "relation_name": "\"across_analytics\".\"dbt_staging\".\"stg_ethereum__fills\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2026-01-05T13:09:38.091637Z", "completed_at": "2026-01-05T13:09:38.133096Z"}, {"name": "execute", "started_at": "2026-01-05T13:09:38.147290Z", "completed_at": "2026-01-05T13:09:38.651126Z"}], "thread_id": "Thread-3 (worker)", "execution_time": 0.5711452960968018, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.stg_hyperevm__deposits", "compiled": true, "compiled_code": "-- Staging model for FundsDeposited events from HyperEVM\n-- This model extracts deposit events where users initiate cross-chain bridge transactions\n\nWITH raw_deposits AS (\n\n    SELECT\n        timestamp_datetime,\n        transactionHash,\n        blockchain,\n        source_file,\n        topic_destination_chain_id,\n        topic_deposit_id,\n        topic_depositor,\n        funds_deposited_data_input_token,\n        funds_deposited_data_output_token,\n        funds_deposited_data_input_amount,\n        funds_deposited_data_output_amount,\n        funds_deposited_data_recipient\n        \n    FROM raw.hyperevm_logs_processed\n    \n    WHERE topic_0 = '0x32ed1a409ef04c7b0227189c3a103dc5ac10e775a15b785dcc510201f7c25ad3'\n        AND funds_deposited_data_input_amount IS NOT NULL\n        AND funds_deposited_data_output_amount IS NOT NULL\n),\n\n-- INPUT token metadata: filtered to origin chain (HyperEVM)\n-- Note: hyperevm may not have tokens in token_metadata yet\ninput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n    WHERE chain = 'hyperevm'\n\n),\n\n-- OUTPUT token metadata: includes chain_id for matching with destination_chain_id\noutput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals,\n        chain_id\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n\n),\n\ncleaned_deposits AS (\n    SELECT\n        CASE \n            WHEN timestamp_datetime ~ '^\\d+$' THEN\n                TO_TIMESTAMP(timestamp_datetime::BIGINT)\n            ELSE \n                timestamp_datetime::TIMESTAMP\n        END AS deposit_timestamp,\n        \n        transactionHash AS transaction_hash,\n        blockchain,\n        source_file,\n        (topic_destination_chain_id::NUMERIC)::BIGINT AS destination_chain_id,\n        (topic_deposit_id::NUMERIC)::BIGINT AS deposit_id,\n        topic_depositor AS depositor_address,\n        LOWER(funds_deposited_data_input_token) AS input_token_address,\n        LOWER(funds_deposited_data_output_token) AS output_token_address,\n        funds_deposited_data_input_amount::NUMERIC AS input_amount_raw,\n        funds_deposited_data_output_amount::NUMERIC AS output_amount_raw,\n        funds_deposited_data_recipient AS recipient_address\n        \n    FROM raw_deposits\n)\n\nSELECT\n    c.deposit_timestamp,\n    c.transaction_hash,\n    c.blockchain,\n    c.source_file,\n    c.destination_chain_id,\n    c.deposit_id,\n    c.depositor_address,\n    c.input_token_address,\n    input_tok.token_symbol AS input_token_symbol,\n    c.output_token_address,\n    output_tok.token_symbol AS output_token_symbol,\n    \n    c.input_amount_raw / POWER(10, COALESCE(input_tok.decimals, 18))\n AS input_amount,\n    \n    CASE \n        WHEN c.output_token_address = '0x0000000000000000000000000000000000000000' \n        THEN c.output_amount_raw / POWER(10, COALESCE(input_tok.decimals, 18))\n        ELSE c.output_amount_raw / POWER(10, COALESCE(output_tok.decimals, 18))\n    END\n AS output_amount,\n    c.input_amount_raw,\n    c.output_amount_raw,\n    c.recipient_address\n    \nFROM cleaned_deposits c\n\nLEFT JOIN input_token_meta AS input_tok\n    ON c.input_token_address = input_tok.token_address\n\nLEFT JOIN output_token_meta AS output_tok\n    ON c.output_token_address = output_tok.token_address\n    AND c.destination_chain_id = output_tok.chain_id\n\nWHERE c.deposit_id IS NOT NULL\n    AND c.depositor_address IS NOT NULL\n    AND c.input_token_address IS NOT NULL\n    AND c.output_token_address IS NOT NULL\n    AND c.input_amount_raw IS NOT NULL\n    AND c.output_amount_raw IS NOT NULL", "relation_name": "\"across_analytics\".\"dbt_staging\".\"stg_hyperevm__deposits\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2026-01-05T13:09:38.076696Z", "completed_at": "2026-01-05T13:09:38.159951Z"}, {"name": "execute", "started_at": "2026-01-05T13:09:38.169881Z", "completed_at": "2026-01-05T13:09:38.658177Z"}], "thread_id": "Thread-4 (worker)", "execution_time": 0.5922744274139404, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.stg_ethereum__refunds", "compiled": true, "compiled_code": "-- Staging model for ExecutedRelayerRefundRoot events from Ethereum\r\n\r\nWITH raw_refunds AS (\r\n\r\n    SELECT\r\n        -- Core event metadata - these identify WHEN and WHERE the event happened\r\n        timestamp_datetime,              -- When the event occurred (from blockchain)\r\n        transactionHash,                 -- Unique transaction identifier\r\n        blockchain,                      -- Which blockchain (should be 'ethereum')\r\n        source_file,                     -- Which source file this came from (for lineage)\r\n        \r\n        -- Indexed fields from topics (these are searchable/filterable in blockchain)\r\n        -- Topics are like \"indexed columns\" in SQL - they're stored separately for fast queries\r\n        topic_chain_id,                  -- Chain ID where the refund was executed\r\n        topic_root_bundle_id,            -- Merkle root bundle identifier (groups multiple refunds)\r\n        topic_leaf_id,                   -- Leaf index in the merkle tree (identifies this specific refund batch)\r\n        \r\n        -- Non-indexed fields from the event's data field\r\n        -- These are the actual business data about the refund\r\n        amount_to_return,                -- Total amount of capital returned in this batch\r\n        l2_token_address,                -- Token address being refunded (on Ethereum L1)\r\n        refund_amounts,                  -- Comma-separated list of individual refund amounts\r\n        refund_addresses,                -- Comma-separated list of relayer addresses receiving refunds\r\n        refund_count                     -- Number of relayers being refunded in this batch\r\n        \r\n    FROM raw.ethereum_logs_processed\r\n    \r\n    -- Filter: Only include rows where ExecutedRelayerRefundRoot data exists\r\n    -- Topic_0 is the event signature hash that identifies the event type\r\n    WHERE topic_0 = '0xf4ad92585b1bc117fbdd644990adf0827bc4c95baeae8a23322af807b6d0020e'\r\n        AND amount_to_return IS NOT NULL\r\n        AND l2_token_address IS NOT NULL\r\n        AND refund_count IS NOT NULL\r\n),\r\n\r\ncleaned_refunds AS (\r\n    SELECT\r\n        \r\n        -- Timestamp: Convert text to proper timestamp type\r\n        CASE \r\n            WHEN timestamp_datetime ~ '^\\d+$' THEN          -- If it's pure digits, treat as Unix timestamp (seconds since 1970-01-01)\r\n                TO_TIMESTAMP(timestamp_datetime::BIGINT)    -- PostgreSQL's TO_TIMESTAMP converts Unix seconds to timestamp\r\n            ELSE \r\n                timestamp_datetime::TIMESTAMP               -- Otherwise, try to parse as ISO format string (e.g., \"2025-12-03 10:30:00\")\r\n        END AS refund_timestamp,\r\n        \r\n        transactionHash AS transaction_hash,\r\n        blockchain,\r\n        source_file,\r\n        \r\n        -- ============================================================\r\n        -- INDEXED FIELDS (from topics) - Already decoded by ETL\r\n        -- ============================================================\r\n        -- These come from the event's \"topics\" array, which is indexed for fast blockchain queries\r\n        \r\n        -- Chain ID: Which blockchain this refund was executed on\r\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\r\n        -- Example: 42161 = Arbitrum, 1 = Ethereum, 8453 = Base\r\n        -- Cast via NUMERIC first to handle decimal strings like \"42161.0\" from ETL\r\n        (topic_chain_id::NUMERIC)::BIGINT AS chain_id,\r\n        \r\n        -- Root bundle ID: Groups multiple refunds together in a merkle tree\r\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\r\n        -- Multiple refund batches can be grouped into one merkle root for gas efficiency\r\n        -- Cast via NUMERIC first to handle decimal strings like \"12345.0\" from ETL\r\n        (topic_root_bundle_id::NUMERIC)::BIGINT AS root_bundle_id,\r\n        \r\n        -- Leaf ID: Identifies this specific refund batch within the merkle tree\r\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\r\n        -- Each leaf represents one batch of refunds to multiple relayers\r\n        -- Cast via NUMERIC first to handle decimal strings like \"789.0\" from ETL\r\n        (topic_leaf_id::NUMERIC)::BIGINT AS leaf_id,\r\n        \r\n        -- ============================================================\r\n        -- TOKEN INFORMATION (what was refunded)\r\n        -- ============================================================\r\n        \r\n        -- L2 token address: The token being refunded on Ethereum\r\n        -- Already decoded by ETL to proper address format (0x...)\r\n        -- Example: 0xA0b86991c6218b36c1d19D4a2e9Eb0cE3606eB48 = USDC on Ethereum\r\n        l2_token_address AS refund_token_address,\r\n        \r\n        -- ============================================================\r\n        -- AMOUNT INFORMATION (how much was refunded)\r\n        -- ============================================================\r\n        \r\n        -- Amount to return: Total capital returned in this batch\r\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\r\n        -- This is the sum of all individual refund amounts in the batch\r\n        -- Stored as NUMERIC to handle large token amounts (18 decimals)\r\n        amount_to_return::NUMERIC AS total_refund_amount,\r\n        \r\n        -- Refund amounts: Individual refund amounts as comma-separated string\r\n        -- Format: \"1000000000000000000,2000000000000000000,3000000000000000000\"\r\n        -- Each value is in token's base units (e.g., wei for ETH, 6 decimals for USDC)\r\n        -- This is a comma-separated string because the ETL processes dynamic arrays\r\n        refund_amounts AS refund_amounts_string,\r\n        \r\n        -- Refund addresses: Relayer addresses receiving refunds as comma-separated string\r\n        -- Format: \"0xAAA...,0xBBB...,0xCCC...\"\r\n        -- Matches 1:1 with refund_amounts (first address gets first amount, etc.)\r\n        -- This is a comma-separated string because the ETL processes dynamic arrays\r\n        refund_addresses AS refund_addresses_string,\r\n        \r\n        -- Refund count: Number of relayers being refunded in this batch\r\n        -- Already converted by ETL to integer\r\n        -- This tells us how many individual refunds are in the arrays above\r\n        refund_count::INTEGER AS refund_count\r\n        \r\n    FROM raw_refunds\r\n)\r\n\r\n-- Final SELECT: Add any computed fields and ensure data quality\r\nSELECT\r\n    -- Event identity\r\n    refund_timestamp,\r\n    transaction_hash,\r\n    blockchain,\r\n    source_file,\r\n    \r\n    -- Indexed fields (merkle tree identifiers)\r\n    chain_id,\r\n    root_bundle_id,\r\n    leaf_id,\r\n    \r\n    -- Token info\r\n    refund_token_address,\r\n    \r\n    -- Amounts\r\n    total_refund_amount,\r\n    refund_amounts_string,\r\n    refund_addresses_string,\r\n    refund_count\r\n    \r\nFROM cleaned_refunds\r\n\r\n-- Data quality: Only include rows with essential fields populated\r\nWHERE chain_id IS NOT NULL\r\n    AND refund_token_address IS NOT NULL\r\n    AND total_refund_amount IS NOT NULL\r\n    AND refund_count IS NOT NULL", "relation_name": "\"across_analytics\".\"dbt_staging\".\"stg_ethereum__refunds\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2026-01-05T13:09:38.119243Z", "completed_at": "2026-01-05T13:09:38.167048Z"}, {"name": "execute", "started_at": "2026-01-05T13:09:38.183664Z", "completed_at": "2026-01-05T13:09:38.707428Z"}], "thread_id": "Thread-2 (worker)", "execution_time": 0.5984187126159668, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.stg_hyperevm__fills", "compiled": true, "compiled_code": "-- Staging model for FilledRelay events from HyperEVM\n\nWITH raw_fills AS (\n    SELECT\n        timestamp_datetime, transactionHash, blockchain, source_file,\n        topic_origin_chain_id, topic_deposit_id, topic_relayer,\n        filled_relay_data_input_token, filled_relay_data_output_token,\n        filled_relay_data_input_amount, filled_relay_data_output_amount,\n        filled_relay_data_repayment_chain_id, filled_relay_data_exclusive_relayer,\n        filled_relay_data_depositor, filled_relay_data_recipient,\n        gas_price_wei, gas_used\n    FROM raw.hyperevm_logs_processed\n    WHERE topic_0 = '0x44b559f101f8fbcc8a0ea43fa91a05a729a5ea6e14a7c75aa750374690137208'\n        AND filled_relay_data_input_amount IS NOT NULL\n        AND filled_relay_data_output_amount IS NOT NULL\n),\n\ninput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals,\n        chain_id\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n\n),\n\noutput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n    WHERE chain = 'hyperevm'\n\n),\n\ncleaned_fills AS (\n    SELECT\n        CASE \n            WHEN timestamp_datetime ~ '^\\d+$' THEN TO_TIMESTAMP(timestamp_datetime::BIGINT)\n            ELSE timestamp_datetime::TIMESTAMP\n        END AS fill_timestamp,\n        transactionHash AS transaction_hash,\n        blockchain, source_file,\n        (topic_origin_chain_id::NUMERIC)::BIGINT AS origin_chain_id,\n        (topic_deposit_id::NUMERIC)::BIGINT AS deposit_id,\n        topic_relayer AS relayer_address,\n        LOWER(filled_relay_data_input_token) AS input_token_address,\n        LOWER(filled_relay_data_output_token) AS output_token_address,\n        filled_relay_data_input_amount::NUMERIC AS input_amount_raw,\n        filled_relay_data_output_amount::NUMERIC AS output_amount_raw,\n        (filled_relay_data_repayment_chain_id::NUMERIC)::BIGINT AS repayment_chain_id,\n        filled_relay_data_exclusive_relayer AS exclusive_relayer_address,\n        filled_relay_data_depositor AS depositor_address,\n        filled_relay_data_recipient AS recipient_address,\n        gas_price_wei::BIGINT AS gas_price_wei,\n        gas_used::BIGINT AS gas_used\n    FROM raw_fills\n)\n\nSELECT\n    c.fill_timestamp, c.transaction_hash, c.blockchain, c.source_file,\n    c.origin_chain_id, c.deposit_id, c.relayer_address,\n    c.input_token_address, input_tok.token_symbol AS input_token_symbol,\n    c.output_token_address, output_tok.token_symbol AS output_token_symbol,\n    \n    c.input_amount_raw / POWER(10, COALESCE(input_tok.decimals, 18))\n AS input_amount,\n    \n    c.output_amount_raw / POWER(10, COALESCE(output_tok.decimals, 18))\n AS output_amount,\n    c.input_amount_raw, c.output_amount_raw,\n    c.repayment_chain_id, c.exclusive_relayer_address,\n    c.depositor_address, c.recipient_address,\n    c.gas_price_wei, c.gas_used, (c.gas_price_wei * c.gas_used) AS gas_cost_wei\nFROM cleaned_fills c\nLEFT JOIN input_token_meta AS input_tok\n    ON c.input_token_address = input_tok.token_address AND c.origin_chain_id = input_tok.chain_id\nLEFT JOIN output_token_meta AS output_tok\n    ON c.output_token_address = output_tok.token_address\nWHERE c.deposit_id IS NOT NULL AND c.relayer_address IS NOT NULL\n    AND c.input_token_address IS NOT NULL AND c.output_token_address IS NOT NULL\n    AND c.input_amount_raw IS NOT NULL AND c.output_amount_raw IS NOT NULL", "relation_name": "\"across_analytics\".\"dbt_staging\".\"stg_hyperevm__fills\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2026-01-05T13:09:38.725678Z", "completed_at": "2026-01-05T13:09:38.733322Z"}, {"name": "execute", "started_at": "2026-01-05T13:09:38.734713Z", "completed_at": "2026-01-05T13:09:39.033426Z"}], "thread_id": "Thread-1 (worker)", "execution_time": 0.31783437728881836, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.stg_hyperevm__refunds", "compiled": true, "compiled_code": "-- Staging model for ExecutedRelayerRefundRoot events from HyperEVM\r\n\r\nWITH raw_refunds AS (\r\n\r\n    SELECT\r\n        -- Core event metadata - these identify WHEN and WHERE the event happened\r\n        timestamp_datetime,              -- When the event occurred (from blockchain)\r\n        transactionHash,                 -- Unique transaction identifier\r\n        blockchain,                      -- Which blockchain (should be 'hyperevm')\r\n        source_file,                     -- Which source file this came from (for lineage)\r\n        \r\n        -- Indexed fields from topics (these are searchable/filterable in blockchain)\r\n        -- Topics are like \"indexed columns\" in SQL - they're stored separately for fast queries\r\n        topic_chain_id,                  -- Chain ID where the refund was executed\r\n        topic_root_bundle_id,            -- Merkle root bundle identifier (groups multiple refunds)\r\n        topic_leaf_id,                   -- Leaf index in the merkle tree (identifies this specific refund batch)\r\n        \r\n        -- Non-indexed fields from the event's data field\r\n        -- These are the actual business data about the refund\r\n        amount_to_return,                -- Total amount of capital returned in this batch\r\n        l2_token_address,                -- Token address being refunded (on HyperEVM)\r\n        refund_amounts,                  -- Comma-separated list of individual refund amounts\r\n        refund_addresses,                -- Comma-separated list of relayer addresses receiving refunds\r\n        refund_count                     -- Number of relayers being refunded in this batch\r\n        \r\n    FROM raw.hyperevm_logs_processed\r\n    \r\n    -- Filter: Only include rows where ExecutedRelayerRefundRoot data exists\r\n    -- Topic_0 is the event signature hash that identifies the event type\r\n    WHERE topic_0 = '0xf4ad92585b1bc117fbdd644990adf0827bc4c95baeae8a23322af807b6d0020e'\r\n        AND amount_to_return IS NOT NULL\r\n        AND l2_token_address IS NOT NULL\r\n        AND refund_count IS NOT NULL\r\n),\r\n\r\ncleaned_refunds AS (\r\n    SELECT\r\n        \r\n        -- Timestamp: Convert text to proper timestamp type\r\n        CASE \r\n            WHEN timestamp_datetime ~ '^\\d+$' THEN          -- If it's pure digits, treat as Unix timestamp (seconds since 1970-01-01)\r\n                TO_TIMESTAMP(timestamp_datetime::BIGINT)    -- PostgreSQL's TO_TIMESTAMP converts Unix seconds to timestamp\r\n            ELSE \r\n                timestamp_datetime::TIMESTAMP               -- Otherwise, try to parse as ISO format string (e.g., \"2025-12-03 10:30:00\")\r\n        END AS refund_timestamp,\r\n        \r\n        transactionHash AS transaction_hash,\r\n        blockchain,\r\n        source_file,\r\n        \r\n        -- ============================================================\r\n        -- INDEXED FIELDS (from topics) - Already decoded by ETL\r\n        -- ============================================================\r\n        -- These come from the event's \"topics\" array, which is indexed for fast blockchain queries\r\n        \r\n        -- Chain ID: Which blockchain this refund was executed on\r\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\r\n        -- Example: 42161 = Arbitrum, 1 = Ethereum, 999 = HyperEVM\r\n        -- Cast via NUMERIC first to handle decimal strings like \"42161.0\" from ETL\r\n        (topic_chain_id::NUMERIC)::BIGINT AS chain_id,\r\n        \r\n        -- Root bundle ID: Groups multiple refunds together in a merkle tree\r\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\r\n        -- Multiple refund batches can be grouped into one merkle root for gas efficiency\r\n        -- Cast via NUMERIC first to handle decimal strings like \"12345.0\" from ETL\r\n        (topic_root_bundle_id::NUMERIC)::BIGINT AS root_bundle_id,\r\n        \r\n        -- Leaf ID: Identifies this specific refund batch within the merkle tree\r\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\r\n        -- Each leaf represents one batch of refunds to multiple relayers\r\n        -- Cast via NUMERIC first to handle decimal strings like \"789.0\" from ETL\r\n        (topic_leaf_id::NUMERIC)::BIGINT AS leaf_id,\r\n        \r\n        -- ============================================================\r\n        -- TOKEN INFORMATION (what was refunded)\r\n        -- ============================================================\r\n        \r\n        -- L2 token address: The token being refunded on HyperEVM\r\n        -- Already decoded by ETL to proper address format (0x...)\r\n        -- This is the token address on the HyperEVM chain\r\n        l2_token_address AS refund_token_address,\r\n        \r\n        -- ============================================================\r\n        -- AMOUNT INFORMATION (how much was refunded)\r\n        -- ============================================================\r\n        \r\n        -- Amount to return: Total capital returned in this batch\r\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\r\n        -- This is the sum of all individual refund amounts in the batch\r\n        -- Stored as NUMERIC to handle large token amounts (18 decimals)\r\n        amount_to_return::NUMERIC AS total_refund_amount,\r\n        \r\n        -- Refund amounts: Individual refund amounts as comma-separated string\r\n        -- Format: \"1000000000000000000,2000000000000000000,3000000000000000000\"\r\n        -- Each value is in token's base units (e.g., wei for ETH, 6 decimals for USDC)\r\n        -- This is a comma-separated string because the ETL processes dynamic arrays\r\n        refund_amounts AS refund_amounts_string,\r\n        \r\n        -- Refund addresses: Relayer addresses receiving refunds as comma-separated string\r\n        -- Format: \"0xAAA...,0xBBB...,0xCCC...\"\r\n        -- Matches 1:1 with refund_amounts (first address gets first amount, etc.)\r\n        -- This is a comma-separated string because the ETL processes dynamic arrays\r\n        refund_addresses AS refund_addresses_string,\r\n        \r\n        -- Refund count: Number of relayers being refunded in this batch\r\n        -- Already converted by ETL to integer\r\n        -- This tells us how many individual refunds are in the arrays above\r\n        refund_count::INTEGER AS refund_count\r\n        \r\n    FROM raw_refunds\r\n)\r\n\r\n-- Final SELECT: Add any computed fields and ensure data quality\r\nSELECT\r\n    -- Event identity\r\n    refund_timestamp,\r\n    transaction_hash,\r\n    blockchain,\r\n    source_file,\r\n    \r\n    -- Indexed fields (merkle tree identifiers)\r\n    chain_id,\r\n    root_bundle_id,\r\n    leaf_id,\r\n    \r\n    -- Token info\r\n    refund_token_address,\r\n    \r\n    -- Amounts\r\n    total_refund_amount,\r\n    refund_amounts_string,\r\n    refund_addresses_string,\r\n    refund_count\r\n    \r\nFROM cleaned_refunds\r\n\r\n-- Data quality: Only include rows with essential fields populated\r\nWHERE chain_id IS NOT NULL\r\n    AND refund_token_address IS NOT NULL\r\n    AND total_refund_amount IS NOT NULL\r\n    AND refund_count IS NOT NULL", "relation_name": "\"across_analytics\".\"dbt_staging\".\"stg_hyperevm__refunds\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2026-01-05T13:09:38.752550Z", "completed_at": "2026-01-05T13:09:38.772899Z"}, {"name": "execute", "started_at": "2026-01-05T13:09:38.777758Z", "completed_at": "2026-01-05T13:09:39.315684Z"}], "thread_id": "Thread-3 (worker)", "execution_time": 0.5785195827484131, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.stg_linea__deposits", "compiled": true, "compiled_code": "-- Staging model for FundsDeposited events from Linea\n-- This model extracts deposit events where users initiate cross-chain bridge transactions\n\nWITH raw_deposits AS (\n\n    SELECT\n        timestamp_datetime,\n        transactionHash,\n        blockchain,\n        source_file,\n        topic_destination_chain_id,\n        topic_deposit_id,\n        topic_depositor,\n        funds_deposited_data_input_token,\n        funds_deposited_data_output_token,\n        funds_deposited_data_input_amount,\n        funds_deposited_data_output_amount,\n        funds_deposited_data_recipient\n        \n    FROM raw.linea_logs_processed\n    \n    WHERE topic_0 = '0x32ed1a409ef04c7b0227189c3a103dc5ac10e775a15b785dcc510201f7c25ad3'\n        AND funds_deposited_data_input_amount IS NOT NULL\n        AND funds_deposited_data_output_amount IS NOT NULL\n),\n\n-- INPUT token metadata: filtered to origin chain (Linea)\ninput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n    WHERE chain = 'linea'\n\n),\n\n-- OUTPUT token metadata: includes chain_id for matching with destination_chain_id\noutput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals,\n        chain_id\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n\n),\n\ncleaned_deposits AS (\n    SELECT\n        CASE \n            WHEN timestamp_datetime ~ '^\\d+$' THEN\n                TO_TIMESTAMP(timestamp_datetime::BIGINT)\n            ELSE \n                timestamp_datetime::TIMESTAMP\n        END AS deposit_timestamp,\n        \n        transactionHash AS transaction_hash,\n        blockchain,\n        source_file,\n        (topic_destination_chain_id::NUMERIC)::BIGINT AS destination_chain_id,\n        (topic_deposit_id::NUMERIC)::BIGINT AS deposit_id,\n        topic_depositor AS depositor_address,\n        LOWER(funds_deposited_data_input_token) AS input_token_address,\n        LOWER(funds_deposited_data_output_token) AS output_token_address,\n        funds_deposited_data_input_amount::NUMERIC AS input_amount_raw,\n        funds_deposited_data_output_amount::NUMERIC AS output_amount_raw,\n        funds_deposited_data_recipient AS recipient_address\n        \n    FROM raw_deposits\n)\n\nSELECT\n    c.deposit_timestamp,\n    c.transaction_hash,\n    c.blockchain,\n    c.source_file,\n    c.destination_chain_id,\n    c.deposit_id,\n    c.depositor_address,\n    c.input_token_address,\n    input_tok.token_symbol AS input_token_symbol,\n    c.output_token_address,\n    output_tok.token_symbol AS output_token_symbol,\n    \n    c.input_amount_raw / POWER(10, COALESCE(input_tok.decimals, 18))\n AS input_amount,\n    \n    CASE \n        WHEN c.output_token_address = '0x0000000000000000000000000000000000000000' \n        THEN c.output_amount_raw / POWER(10, COALESCE(input_tok.decimals, 18))\n        ELSE c.output_amount_raw / POWER(10, COALESCE(output_tok.decimals, 18))\n    END\n AS output_amount,\n    c.input_amount_raw,\n    c.output_amount_raw,\n    c.recipient_address\n    \nFROM cleaned_deposits c\n\nLEFT JOIN input_token_meta AS input_tok\n    ON c.input_token_address = input_tok.token_address\n\nLEFT JOIN output_token_meta AS output_tok\n    ON c.output_token_address = output_tok.token_address\n    AND c.destination_chain_id = output_tok.chain_id\n\nWHERE c.deposit_id IS NOT NULL\n    AND c.depositor_address IS NOT NULL\n    AND c.input_token_address IS NOT NULL\n    AND c.output_token_address IS NOT NULL\n    AND c.input_amount_raw IS NOT NULL\n    AND c.output_amount_raw IS NOT NULL", "relation_name": "\"across_analytics\".\"dbt_staging\".\"stg_linea__deposits\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2026-01-05T13:09:38.866969Z", "completed_at": "2026-01-05T13:09:38.878074Z"}, {"name": "execute", "started_at": "2026-01-05T13:09:38.879803Z", "completed_at": "2026-01-05T13:09:39.382726Z"}], "thread_id": "Thread-2 (worker)", "execution_time": 0.5242283344268799, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.stg_linea__refunds", "compiled": true, "compiled_code": "-- Staging model for ExecutedRelayerRefundRoot events from Linea\r\n\r\nWITH raw_refunds AS (\r\n\r\n    SELECT\r\n        -- Core event metadata - these identify WHEN and WHERE the event happened\r\n        timestamp_datetime,              -- When the event occurred (from blockchain)\r\n        transactionHash,                 -- Unique transaction identifier\r\n        blockchain,                      -- Which blockchain (should be 'linea')\r\n        source_file,                     -- Which source file this came from (for lineage)\r\n        \r\n        -- Indexed fields from topics (these are searchable/filterable in blockchain)\r\n        -- Topics are like \"indexed columns\" in SQL - they're stored separately for fast queries\r\n        topic_chain_id,                  -- Chain ID where the refund was executed\r\n        topic_root_bundle_id,            -- Merkle root bundle identifier (groups multiple refunds)\r\n        topic_leaf_id,                   -- Leaf index in the merkle tree (identifies this specific refund batch)\r\n        \r\n        -- Non-indexed fields from the event's data field\r\n        -- These are the actual business data about the refund\r\n        amount_to_return,                -- Total amount of capital returned in this batch\r\n        l2_token_address,                -- Token address being refunded (on Linea)\r\n        refund_amounts,                  -- Comma-separated list of individual refund amounts\r\n        refund_addresses,                -- Comma-separated list of relayer addresses receiving refunds\r\n        refund_count                     -- Number of relayers being refunded in this batch\r\n        \r\n    FROM raw.linea_logs_processed\r\n    \r\n    -- Filter: Only include rows where ExecutedRelayerRefundRoot data exists\r\n    -- Topic_0 is the event signature hash that identifies the event type\r\n    WHERE topic_0 = '0xf4ad92585b1bc117fbdd644990adf0827bc4c95baeae8a23322af807b6d0020e'\r\n        AND amount_to_return IS NOT NULL\r\n        AND l2_token_address IS NOT NULL\r\n        AND refund_count IS NOT NULL\r\n),\r\n\r\ncleaned_refunds AS (\r\n    SELECT\r\n        \r\n        -- Timestamp: Convert text to proper timestamp type\r\n        CASE \r\n            WHEN timestamp_datetime ~ '^\\d+$' THEN          -- If it's pure digits, treat as Unix timestamp (seconds since 1970-01-01)\r\n                TO_TIMESTAMP(timestamp_datetime::BIGINT)    -- PostgreSQL's TO_TIMESTAMP converts Unix seconds to timestamp\r\n            ELSE \r\n                timestamp_datetime::TIMESTAMP               -- Otherwise, try to parse as ISO format string (e.g., \"2025-12-03 10:30:00\")\r\n        END AS refund_timestamp,\r\n        \r\n        transactionHash AS transaction_hash,\r\n        blockchain,\r\n        source_file,\r\n        \r\n        -- ============================================================\r\n        -- INDEXED FIELDS (from topics) - Already decoded by ETL\r\n        -- ============================================================\r\n        -- These come from the event's \"topics\" array, which is indexed for fast blockchain queries\r\n        \r\n        -- Chain ID: Which blockchain this refund was executed on\r\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\r\n        -- Example: 42161 = Arbitrum, 1 = Ethereum, 59144 = Linea\r\n        -- Cast via NUMERIC first to handle decimal strings like \"42161.0\" from ETL\r\n        (topic_chain_id::NUMERIC)::BIGINT AS chain_id,\r\n        \r\n        -- Root bundle ID: Groups multiple refunds together in a merkle tree\r\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\r\n        -- Multiple refund batches can be grouped into one merkle root for gas efficiency\r\n        -- Cast via NUMERIC first to handle decimal strings like \"12345.0\" from ETL\r\n        (topic_root_bundle_id::NUMERIC)::BIGINT AS root_bundle_id,\r\n        \r\n        -- Leaf ID: Identifies this specific refund batch within the merkle tree\r\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\r\n        -- Each leaf represents one batch of refunds to multiple relayers\r\n        -- Cast via NUMERIC first to handle decimal strings like \"789.0\" from ETL\r\n        (topic_leaf_id::NUMERIC)::BIGINT AS leaf_id,\r\n        \r\n        -- ============================================================\r\n        -- TOKEN INFORMATION (what was refunded)\r\n        -- ============================================================\r\n        \r\n        -- L2 token address: The token being refunded on Linea\r\n        -- Already decoded by ETL to proper address format (0x...)\r\n        -- Example: 0x176211869cA2b568f2A7D4EE941E07aA25fee00b = USDC on Linea\r\n        l2_token_address AS refund_token_address,\r\n        \r\n        -- ============================================================\r\n        -- AMOUNT INFORMATION (how much was refunded)\r\n        -- ============================================================\r\n        \r\n        -- Amount to return: Total capital returned in this batch\r\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\r\n        -- This is the sum of all individual refund amounts in the batch\r\n        -- Stored as NUMERIC to handle large token amounts (18 decimals)\r\n        amount_to_return::NUMERIC AS total_refund_amount,\r\n        \r\n        -- Refund amounts: Individual refund amounts as comma-separated string\r\n        -- Format: \"1000000000000000000,2000000000000000000,3000000000000000000\"\r\n        -- Each value is in token's base units (e.g., wei for ETH, 6 decimals for USDC)\r\n        -- This is a comma-separated string because the ETL processes dynamic arrays\r\n        refund_amounts AS refund_amounts_string,\r\n        \r\n        -- Refund addresses: Relayer addresses receiving refunds as comma-separated string\r\n        -- Format: \"0xAAA...,0xBBB...,0xCCC...\"\r\n        -- Matches 1:1 with refund_amounts (first address gets first amount, etc.)\r\n        -- This is a comma-separated string because the ETL processes dynamic arrays\r\n        refund_addresses AS refund_addresses_string,\r\n        \r\n        -- Refund count: Number of relayers being refunded in this batch\r\n        -- Already converted by ETL to integer\r\n        -- This tells us how many individual refunds are in the arrays above\r\n        refund_count::INTEGER AS refund_count\r\n        \r\n    FROM raw_refunds\r\n)\r\n\r\n-- Final SELECT: Add any computed fields and ensure data quality\r\nSELECT\r\n    -- Event identity\r\n    refund_timestamp,\r\n    transaction_hash,\r\n    blockchain,\r\n    source_file,\r\n    \r\n    -- Indexed fields (merkle tree identifiers)\r\n    chain_id,\r\n    root_bundle_id,\r\n    leaf_id,\r\n    \r\n    -- Token info\r\n    refund_token_address,\r\n    \r\n    -- Amounts\r\n    total_refund_amount,\r\n    refund_amounts_string,\r\n    refund_addresses_string,\r\n    refund_count\r\n    \r\nFROM cleaned_refunds\r\n\r\n-- Data quality: Only include rows with essential fields populated\r\nWHERE chain_id IS NOT NULL\r\n    AND refund_token_address IS NOT NULL\r\n    AND total_refund_amount IS NOT NULL\r\n    AND refund_count IS NOT NULL", "relation_name": "\"across_analytics\".\"dbt_staging\".\"stg_linea__refunds\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2026-01-05T13:09:38.799683Z", "completed_at": "2026-01-05T13:09:38.816781Z"}, {"name": "execute", "started_at": "2026-01-05T13:09:38.819333Z", "completed_at": "2026-01-05T13:09:39.397646Z"}], "thread_id": "Thread-4 (worker)", "execution_time": 0.6043386459350586, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.stg_linea__fills", "compiled": true, "compiled_code": "-- Staging model for FilledRelay events from Linea\n-- This model extracts fill events where relayers complete cross-chain bridge transactions\n\nWITH raw_fills AS (\n\n    SELECT\n        -- Core event metadata - these identify WHEN and WHERE the event happened\n        timestamp_datetime,              -- When the event occurred (from blockchain)\n        transactionHash,                 -- Unique transaction identifier\n        blockchain,                      -- Which blockchain (should be 'linea')\n        source_file,                     -- Which source file this came from (for lineage)\n        \n        -- Indexed fields from topics (these are searchable/filterable in blockchain)\n        -- Topics are like \"indexed columns\" in SQL - they're stored separately for fast queries\n        topic_origin_chain_id,           -- Chain ID where the original deposit happened\n        topic_deposit_id,                -- Unique deposit identifier (links to deposit event)\n        topic_relayer,                   -- Address of the relayer who provided liquidity\n        \n        -- Non-indexed fields from the event's data field\n        -- These are the actual business data about the fill\n        filled_relay_data_input_token,          -- Token address on origin chain\n        filled_relay_data_output_token,          -- Token address on destination chain\n        filled_relay_data_input_amount,          -- Amount sent (in origin token units)\n        filled_relay_data_output_amount,         -- Amount received (in destination token units)\n        filled_relay_data_repayment_chain_id,     -- Where relayer gets reimbursed\n        filled_relay_data_exclusive_relayer,     -- Address with exclusive fill rights (if any)\n        filled_relay_data_depositor,             -- Original user who initiated the bridge\n        filled_relay_data_recipient,              -- Final recipient of the bridged funds\n        \n        -- Gas data (for relayer cost analysis)\n        gas_price_wei,\n        gas_used\n        \n    FROM raw.linea_logs_processed\n    \n    -- Filter: Only include rows where FilledRelay data exists\n    WHERE topic_0 = '0x44b559f101f8fbcc8a0ea43fa91a05a729a5ea6e14a7c75aa750374690137208'\n        AND filled_relay_data_input_amount IS NOT NULL\n        AND filled_relay_data_output_amount IS NOT NULL\n),\n\n-- INPUT token metadata: includes chain_id for matching with origin_chain_id\n-- This allows us to find the correct token on whichever chain the deposit came from\ninput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals,\n        chain_id\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n\n),\n\n-- OUTPUT token metadata: filtered to destination chain (Linea)\n-- This is correct because on fills, output_token is always on the destination chain\noutput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n    WHERE chain = 'linea'\n\n),\n\ncleaned_fills AS (\n    SELECT\n        \n        -- Timestamp: Convert text to proper timestamp type\n        \n        CASE \n            WHEN timestamp_datetime ~ '^\\d+$' THEN          -- If it's pure digits, treat as Unix timestamp (seconds since 1970-01-01)\n                TO_TIMESTAMP(timestamp_datetime::BIGINT)    -- PostgreSQL's TO_TIMESTAMP can parse various formats, but we use the simplest approach\n            ELSE \n                timestamp_datetime::TIMESTAMP               -- Otherwise, try to parse as ISO format string\n        END AS fill_timestamp,\n        \n        transactionHash AS transaction_hash,\n        blockchain,\n        source_file,\n        \n        -- ============================================================\n        -- INDEXED FIELDS (from topics) - Already decoded by ETL\n        -- ============================================================\n        -- These come from the event's \"topics\" array, which is indexed for fast blockchain queries\n        \n        -- Origin chain ID: Which blockchain the funds came FROM\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\n        -- Example: 1 = Ethereum, 42161 = Arbitrum, 8453 = Base\n        -- Cast via NUMERIC first to handle decimal strings like \"8453.0\" from ETL\n        (topic_origin_chain_id::NUMERIC)::BIGINT AS origin_chain_id,\n        \n        -- Deposit ID: Unique identifier linking this fill to its original deposit\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\n        -- This is the KEY that connects deposits \u2194 fills across chains\n        -- Cast via NUMERIC first to handle decimal strings like \"12345.0\" from ETL\n        (topic_deposit_id::NUMERIC)::BIGINT AS deposit_id,\n        \n        -- Relayer address: Who provided the liquidity\n        -- Already decoded by ETL to proper address format (0x...)\n        topic_relayer AS relayer_address,\n        \n        -- ============================================================\n        -- TOKEN INFORMATION (what was bridged)\n        -- ============================================================\n        \n        -- Input token: The token address on the origin chain\n        -- Already decoded by ETL to proper address format\n        -- Example: 0xA0b86991c6218b36c1d19D4a2e9Eb0cE3606eB48 = USDC on Ethereum\n        -- Normalize to lowercase for joining with token metadata\n        LOWER(filled_relay_data_input_token) AS input_token_address,\n        \n        -- Output token: The token address on the destination chain (Linea in this case)\n        -- Already decoded by ETL to proper address format\n        -- Normalize to lowercase for joining with token metadata\n        LOWER(filled_relay_data_output_token) AS output_token_address,\n        \n        -- ============================================================\n        -- AMOUNT INFORMATION (how much was bridged)\n        -- ============================================================\n        \n        -- Input amount: How much was sent from origin chain\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\n        -- Store as RAW amount first (before rescaling by decimals)\n        filled_relay_data_input_amount::NUMERIC AS input_amount_raw,\n        \n        -- Output amount: How much was received on destination chain\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\n        -- Usually slightly less than input due to fees/spread\n        -- Store as RAW amount first (before rescaling by decimals)\n        filled_relay_data_output_amount::NUMERIC AS output_amount_raw,\n        \n        -- ============================================================\n        -- RELAYER & ROUTING INFORMATION\n        -- ============================================================\n        \n        -- Repayment chain ID: Where the relayer gets reimbursed\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\n        -- Relayers front capital, then get paid back (often on a different chain)\n        -- Cast via NUMERIC first to handle decimal strings like \"1.0\" from ETL\n        (filled_relay_data_repayment_chain_id::NUMERIC)::BIGINT AS repayment_chain_id,\n        \n        -- Exclusive relayer: If set, only this address can fill this deposit\n        -- Already decoded by ETL to proper address format\n        -- Used for priority/guaranteed fills (NULL if no exclusive relayer)\n        filled_relay_data_exclusive_relayer AS exclusive_relayer_address,\n        \n        -- ============================================================\n        -- USER INFORMATION\n        -- ============================================================\n        \n        -- Depositor: The original user who initiated the bridge\n        -- Already decoded by ETL to proper address format\n        filled_relay_data_depositor AS depositor_address,\n        \n        -- Recipient: Who receives the funds on the destination chain\n        -- Already decoded by ETL to proper address format\n        -- Usually the same as depositor, but can be different (gift/transfer)\n        filled_relay_data_recipient AS recipient_address,\n        \n        -- ============================================================\n        -- GAS DATA (for relayer cost analysis)\n        -- ============================================================\n        gas_price_wei::BIGINT AS gas_price_wei,\n        gas_used::BIGINT AS gas_used\n        \n    FROM raw_fills\n)\n\n-- ============================================================\n-- FINAL SELECT: Join with token metadata and rescale amounts\n-- ============================================================\nSELECT\n    -- Event identity\n    c.fill_timestamp,\n    c.transaction_hash,\n    c.blockchain,\n    c.source_file,\n    \n    -- Indexed fields\n    c.origin_chain_id,\n    c.deposit_id,\n    c.relayer_address,\n    \n    -- Token info WITH NAMES\n    -- Input token (what was deposited on origin chain)\n    c.input_token_address,\n    input_tok.token_symbol AS input_token_symbol,        -- e.g., 'USDC', 'WETH'\n    \n    -- Output token (what was received on destination chain - Linea)\n    c.output_token_address,\n    output_tok.token_symbol AS output_token_symbol,      -- e.g., 'USDC', 'WETH'\n    \n    -- Rescaled amounts (human-readable)\n    -- These use the rescale_amount macro which divides raw amount by 10^decimals\n    -- Example: 5000000 raw USDC (6 decimals) \u2192 5.0 USDC\n    \n    c.input_amount_raw / POWER(10, COALESCE(input_tok.decimals, 18))\n AS input_amount,\n    \n    c.output_amount_raw / POWER(10, COALESCE(output_tok.decimals, 18))\n AS output_amount,\n    \n    -- Raw amounts (preserved for auditing)\n    -- These are the original blockchain values before rescaling\n    -- Example: 5.0 USDC is stored as 5000000 on blockchain\n    c.input_amount_raw,\n    c.output_amount_raw,\n    \n    -- Relayer routing\n    c.repayment_chain_id,\n    c.exclusive_relayer_address,\n    \n    -- User info\n    c.depositor_address,\n    c.recipient_address,\n    \n    -- Gas data (for relayer cost analysis)\n    c.gas_price_wei,\n    c.gas_used,\n    (c.gas_price_wei * c.gas_used) AS gas_cost_wei\n    \nFROM cleaned_fills c\n\n-- Join with token metadata to get decimals and symbols for INPUT token\n-- Join on BOTH address AND chain_id to match the correct origin chain\nLEFT JOIN input_token_meta AS input_tok\n    ON c.input_token_address = input_tok.token_address\n    AND c.origin_chain_id = input_tok.chain_id\n\n-- Join with token metadata to get decimals and symbols for OUTPUT token\n-- Join on address only - we already filtered to destination chain (Linea)\nLEFT JOIN output_token_meta AS output_tok\n    ON c.output_token_address = output_tok.token_address\n\n-- Data quality: Only include rows with essential fields populated\nWHERE c.deposit_id IS NOT NULL\n    AND c.relayer_address IS NOT NULL\n    AND c.input_token_address IS NOT NULL\n    AND c.output_token_address IS NOT NULL\n    AND c.input_amount_raw IS NOT NULL\n    AND c.output_amount_raw IS NOT NULL", "relation_name": "\"across_analytics\".\"dbt_staging\".\"stg_linea__fills\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2026-01-05T13:09:39.064810Z", "completed_at": "2026-01-05T13:09:39.079117Z"}, {"name": "execute", "started_at": "2026-01-05T13:09:39.080922Z", "completed_at": "2026-01-05T13:09:39.718761Z"}], "thread_id": "Thread-1 (worker)", "execution_time": 0.660254955291748, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.stg_monad__deposits", "compiled": true, "compiled_code": "-- Staging model for FundsDeposited events from Monad\n-- This model extracts deposit events where users initiate cross-chain bridge transactions\n\nWITH raw_deposits AS (\n\n    SELECT\n        timestamp_datetime,\n        transactionHash,\n        blockchain,\n        source_file,\n        topic_destination_chain_id,\n        topic_deposit_id,\n        topic_depositor,\n        funds_deposited_data_input_token,\n        funds_deposited_data_output_token,\n        funds_deposited_data_input_amount,\n        funds_deposited_data_output_amount,\n        funds_deposited_data_recipient\n        \n    FROM raw.monad_logs_processed\n    \n    WHERE topic_0 = '0x32ed1a409ef04c7b0227189c3a103dc5ac10e775a15b785dcc510201f7c25ad3'\n        AND funds_deposited_data_input_amount IS NOT NULL\n        AND funds_deposited_data_output_amount IS NOT NULL\n),\n\n-- INPUT token metadata: filtered to origin chain (Monad)\n-- Note: monad may not have tokens in token_metadata yet\ninput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n    WHERE chain = 'monad'\n\n),\n\n-- OUTPUT token metadata: includes chain_id for matching with destination_chain_id\noutput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals,\n        chain_id\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n\n),\n\ncleaned_deposits AS (\n    SELECT\n        CASE \n            WHEN timestamp_datetime ~ '^\\d+$' THEN\n                TO_TIMESTAMP(timestamp_datetime::BIGINT)\n            ELSE \n                timestamp_datetime::TIMESTAMP\n        END AS deposit_timestamp,\n        \n        transactionHash AS transaction_hash,\n        blockchain,\n        source_file,\n        (topic_destination_chain_id::NUMERIC)::BIGINT AS destination_chain_id,\n        (topic_deposit_id::NUMERIC)::BIGINT AS deposit_id,\n        topic_depositor AS depositor_address,\n        LOWER(funds_deposited_data_input_token) AS input_token_address,\n        LOWER(funds_deposited_data_output_token) AS output_token_address,\n        funds_deposited_data_input_amount::NUMERIC AS input_amount_raw,\n        funds_deposited_data_output_amount::NUMERIC AS output_amount_raw,\n        funds_deposited_data_recipient AS recipient_address\n        \n    FROM raw_deposits\n)\n\nSELECT\n    c.deposit_timestamp,\n    c.transaction_hash,\n    c.blockchain,\n    c.source_file,\n    c.destination_chain_id,\n    c.deposit_id,\n    c.depositor_address,\n    c.input_token_address,\n    input_tok.token_symbol AS input_token_symbol,\n    c.output_token_address,\n    output_tok.token_symbol AS output_token_symbol,\n    \n    c.input_amount_raw / POWER(10, COALESCE(input_tok.decimals, 18))\n AS input_amount,\n    \n    CASE \n        WHEN c.output_token_address = '0x0000000000000000000000000000000000000000' \n        THEN c.output_amount_raw / POWER(10, COALESCE(input_tok.decimals, 18))\n        ELSE c.output_amount_raw / POWER(10, COALESCE(output_tok.decimals, 18))\n    END\n AS output_amount,\n    c.input_amount_raw,\n    c.output_amount_raw,\n    c.recipient_address\n    \nFROM cleaned_deposits c\n\nLEFT JOIN input_token_meta AS input_tok\n    ON c.input_token_address = input_tok.token_address\n\nLEFT JOIN output_token_meta AS output_tok\n    ON c.output_token_address = output_tok.token_address\n    AND c.destination_chain_id = output_tok.chain_id\n\nWHERE c.deposit_id IS NOT NULL\n    AND c.depositor_address IS NOT NULL\n    AND c.input_token_address IS NOT NULL\n    AND c.output_token_address IS NOT NULL\n    AND c.input_amount_raw IS NOT NULL\n    AND c.output_amount_raw IS NOT NULL", "relation_name": "\"across_analytics\".\"dbt_staging\".\"stg_monad__deposits\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2026-01-05T13:09:39.427195Z", "completed_at": "2026-01-05T13:09:39.477856Z"}, {"name": "execute", "started_at": "2026-01-05T13:09:39.479362Z", "completed_at": "2026-01-05T13:09:40.018900Z"}], "thread_id": "Thread-3 (worker)", "execution_time": 0.597400426864624, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.stg_monad__fills", "compiled": true, "compiled_code": "-- Staging model for FilledRelay events from Monad\n\nWITH raw_fills AS (\n    SELECT\n        timestamp_datetime, transactionHash, blockchain, source_file,\n        topic_origin_chain_id, topic_deposit_id, topic_relayer,\n        filled_relay_data_input_token, filled_relay_data_output_token,\n        filled_relay_data_input_amount, filled_relay_data_output_amount,\n        filled_relay_data_repayment_chain_id, filled_relay_data_exclusive_relayer,\n        filled_relay_data_depositor, filled_relay_data_recipient,\n        gas_price_wei, gas_used\n    FROM raw.monad_logs_processed\n    WHERE topic_0 = '0x44b559f101f8fbcc8a0ea43fa91a05a729a5ea6e14a7c75aa750374690137208'\n        AND filled_relay_data_input_amount IS NOT NULL\n        AND filled_relay_data_output_amount IS NOT NULL\n),\n\ninput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals,\n        chain_id\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n\n),\n\noutput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n    WHERE chain = 'monad'\n\n),\n\ncleaned_fills AS (\n    SELECT\n        CASE \n            WHEN timestamp_datetime ~ '^\\d+$' THEN TO_TIMESTAMP(timestamp_datetime::BIGINT)\n            ELSE timestamp_datetime::TIMESTAMP\n        END AS fill_timestamp,\n        transactionHash AS transaction_hash,\n        blockchain, source_file,\n        (topic_origin_chain_id::NUMERIC)::BIGINT AS origin_chain_id,\n        (topic_deposit_id::NUMERIC)::BIGINT AS deposit_id,\n        topic_relayer AS relayer_address,\n        LOWER(filled_relay_data_input_token) AS input_token_address,\n        LOWER(filled_relay_data_output_token) AS output_token_address,\n        filled_relay_data_input_amount::NUMERIC AS input_amount_raw,\n        filled_relay_data_output_amount::NUMERIC AS output_amount_raw,\n        (filled_relay_data_repayment_chain_id::NUMERIC)::BIGINT AS repayment_chain_id,\n        filled_relay_data_exclusive_relayer AS exclusive_relayer_address,\n        filled_relay_data_depositor AS depositor_address,\n        filled_relay_data_recipient AS recipient_address,\n        gas_price_wei::BIGINT AS gas_price_wei,\n        gas_used::BIGINT AS gas_used\n    FROM raw_fills\n)\n\nSELECT\n    c.fill_timestamp, c.transaction_hash, c.blockchain, c.source_file,\n    c.origin_chain_id, c.deposit_id, c.relayer_address,\n    c.input_token_address, input_tok.token_symbol AS input_token_symbol,\n    c.output_token_address, output_tok.token_symbol AS output_token_symbol,\n    \n    c.input_amount_raw / POWER(10, COALESCE(input_tok.decimals, 18))\n AS input_amount,\n    \n    c.output_amount_raw / POWER(10, COALESCE(output_tok.decimals, 18))\n AS output_amount,\n    c.input_amount_raw, c.output_amount_raw,\n    c.repayment_chain_id, c.exclusive_relayer_address,\n    c.depositor_address, c.recipient_address,\n    c.gas_price_wei, c.gas_used, (c.gas_price_wei * c.gas_used) AS gas_cost_wei\nFROM cleaned_fills c\nLEFT JOIN input_token_meta AS input_tok\n    ON c.input_token_address = input_tok.token_address AND c.origin_chain_id = input_tok.chain_id\nLEFT JOIN output_token_meta AS output_tok\n    ON c.output_token_address = output_tok.token_address\nWHERE c.deposit_id IS NOT NULL AND c.relayer_address IS NOT NULL\n    AND c.input_token_address IS NOT NULL AND c.output_token_address IS NOT NULL\n    AND c.input_amount_raw IS NOT NULL AND c.output_amount_raw IS NOT NULL", "relation_name": "\"across_analytics\".\"dbt_staging\".\"stg_monad__fills\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2026-01-05T13:09:39.547600Z", "completed_at": "2026-01-05T13:09:39.565275Z"}, {"name": "execute", "started_at": "2026-01-05T13:09:39.588659Z", "completed_at": "2026-01-05T13:09:40.150753Z"}], "thread_id": "Thread-2 (worker)", "execution_time": 0.6240267753601074, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.stg_monad__refunds", "compiled": true, "compiled_code": "-- Staging model for ExecutedRelayerRefundRoot events from Monad\r\n\r\nWITH raw_refunds AS (\r\n\r\n    SELECT\r\n        -- Core event metadata - these identify WHEN and WHERE the event happened\r\n        timestamp_datetime,              -- When the event occurred (from blockchain)\r\n        transactionHash,                 -- Unique transaction identifier\r\n        blockchain,                      -- Which blockchain (should be 'monad')\r\n        source_file,                     -- Which source file this came from (for lineage)\r\n        \r\n        -- Indexed fields from topics (these are searchable/filterable in blockchain)\r\n        -- Topics are like \"indexed columns\" in SQL - they're stored separately for fast queries\r\n        topic_chain_id,                  -- Chain ID where the refund was executed\r\n        topic_root_bundle_id,            -- Merkle root bundle identifier (groups multiple refunds)\r\n        topic_leaf_id,                   -- Leaf index in the merkle tree (identifies this specific refund batch)\r\n        \r\n        -- Non-indexed fields from the event's data field\r\n        -- These are the actual business data about the refund\r\n        amount_to_return,                -- Total amount of capital returned in this batch\r\n        l2_token_address,                -- Token address being refunded (on Monad)\r\n        refund_amounts,                  -- Comma-separated list of individual refund amounts\r\n        refund_addresses,                -- Comma-separated list of relayer addresses receiving refunds\r\n        refund_count                     -- Number of relayers being refunded in this batch\r\n        \r\n    FROM raw.monad_logs_processed\r\n    \r\n    -- Filter: Only include rows where ExecutedRelayerRefundRoot data exists\r\n    -- Topic_0 is the event signature hash that identifies the event type\r\n    WHERE topic_0 = '0xf4ad92585b1bc117fbdd644990adf0827bc4c95baeae8a23322af807b6d0020e'\r\n        AND amount_to_return IS NOT NULL\r\n        AND l2_token_address IS NOT NULL\r\n        AND refund_count IS NOT NULL\r\n),\r\n\r\ncleaned_refunds AS (\r\n    SELECT\r\n        \r\n        -- Timestamp: Convert text to proper timestamp type\r\n        CASE \r\n            WHEN timestamp_datetime ~ '^\\d+$' THEN          -- If it's pure digits, treat as Unix timestamp (seconds since 1970-01-01)\r\n                TO_TIMESTAMP(timestamp_datetime::BIGINT)    -- PostgreSQL's TO_TIMESTAMP converts Unix seconds to timestamp\r\n            ELSE \r\n                timestamp_datetime::TIMESTAMP               -- Otherwise, try to parse as ISO format string (e.g., \"2025-12-03 10:30:00\")\r\n        END AS refund_timestamp,\r\n        \r\n        transactionHash AS transaction_hash,\r\n        blockchain,\r\n        source_file,\r\n        \r\n        -- ============================================================\r\n        -- INDEXED FIELDS (from topics) - Already decoded by ETL\r\n        -- ============================================================\r\n        -- These come from the event's \"topics\" array, which is indexed for fast blockchain queries\r\n        \r\n        -- Chain ID: Which blockchain this refund was executed on\r\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\r\n        -- Example: 42161 = Arbitrum, 1 = Ethereum, 143 = Monad\r\n        -- Cast via NUMERIC first to handle decimal strings like \"42161.0\" from ETL\r\n        (topic_chain_id::NUMERIC)::BIGINT AS chain_id,\r\n        \r\n        -- Root bundle ID: Groups multiple refunds together in a merkle tree\r\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\r\n        -- Multiple refund batches can be grouped into one merkle root for gas efficiency\r\n        -- Cast via NUMERIC first to handle decimal strings like \"12345.0\" from ETL\r\n        (topic_root_bundle_id::NUMERIC)::BIGINT AS root_bundle_id,\r\n        \r\n        -- Leaf ID: Identifies this specific refund batch within the merkle tree\r\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\r\n        -- Each leaf represents one batch of refunds to multiple relayers\r\n        -- Cast via NUMERIC first to handle decimal strings like \"789.0\" from ETL\r\n        (topic_leaf_id::NUMERIC)::BIGINT AS leaf_id,\r\n        \r\n        -- ============================================================\r\n        -- TOKEN INFORMATION (what was refunded)\r\n        -- ============================================================\r\n        \r\n        -- L2 token address: The token being refunded on Monad\r\n        -- Already decoded by ETL to proper address format (0x...)\r\n        -- Example: 0x754704Bc059F8C67012fEd69BC8A327a5aafb603 = USDC on Monad\r\n        l2_token_address AS refund_token_address,\r\n        \r\n        -- ============================================================\r\n        -- AMOUNT INFORMATION (how much was refunded)\r\n        -- ============================================================\r\n        \r\n        -- Amount to return: Total capital returned in this batch\r\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\r\n        -- This is the sum of all individual refund amounts in the batch\r\n        -- Stored as NUMERIC to handle large token amounts (18 decimals)\r\n        amount_to_return::NUMERIC AS total_refund_amount,\r\n        \r\n        -- Refund amounts: Individual refund amounts as comma-separated string\r\n        -- Format: \"1000000000000000000,2000000000000000000,3000000000000000000\"\r\n        -- Each value is in token's base units (e.g., wei for ETH, 6 decimals for USDC)\r\n        -- This is a comma-separated string because the ETL processes dynamic arrays\r\n        refund_amounts AS refund_amounts_string,\r\n        \r\n        -- Refund addresses: Relayer addresses receiving refunds as comma-separated string\r\n        -- Format: \"0xAAA...,0xBBB...,0xCCC...\"\r\n        -- Matches 1:1 with refund_amounts (first address gets first amount, etc.)\r\n        -- This is a comma-separated string because the ETL processes dynamic arrays\r\n        refund_addresses AS refund_addresses_string,\r\n        \r\n        -- Refund count: Number of relayers being refunded in this batch\r\n        -- Already converted by ETL to integer\r\n        -- This tells us how many individual refunds are in the arrays above\r\n        refund_count::INTEGER AS refund_count\r\n        \r\n    FROM raw_refunds\r\n)\r\n\r\n-- Final SELECT: Add any computed fields and ensure data quality\r\nSELECT\r\n    -- Event identity\r\n    refund_timestamp,\r\n    transaction_hash,\r\n    blockchain,\r\n    source_file,\r\n    \r\n    -- Indexed fields (merkle tree identifiers)\r\n    chain_id,\r\n    root_bundle_id,\r\n    leaf_id,\r\n    \r\n    -- Token info\r\n    refund_token_address,\r\n    \r\n    -- Amounts\r\n    total_refund_amount,\r\n    refund_amounts_string,\r\n    refund_addresses_string,\r\n    refund_count\r\n    \r\nFROM cleaned_refunds\r\n\r\n-- Data quality: Only include rows with essential fields populated\r\nWHERE chain_id IS NOT NULL\r\n    AND refund_token_address IS NOT NULL\r\n    AND total_refund_amount IS NOT NULL\r\n    AND refund_count IS NOT NULL", "relation_name": "\"across_analytics\".\"dbt_staging\".\"stg_monad__refunds\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2026-01-05T13:09:39.628130Z", "completed_at": "2026-01-05T13:09:39.686779Z"}, {"name": "execute", "started_at": "2026-01-05T13:09:39.693849Z", "completed_at": "2026-01-05T13:09:40.258609Z"}], "thread_id": "Thread-4 (worker)", "execution_time": 0.7070925235748291, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.stg_polygon__deposits", "compiled": true, "compiled_code": "-- Staging model for FundsDeposited events from Polygon\n-- This model extracts deposit events where users initiate cross-chain bridge transactions\n\nWITH raw_deposits AS (\n\n    SELECT\n        -- Core event metadata\n        timestamp_datetime,\n        transactionHash,\n        blockchain,\n        source_file,\n        \n        -- Indexed fields from topics\n        topic_destination_chain_id,\n        topic_deposit_id,\n        topic_depositor,\n        \n        -- Non-indexed fields from the event's data field\n        funds_deposited_data_input_token,\n        funds_deposited_data_output_token,\n        funds_deposited_data_input_amount,\n        funds_deposited_data_output_amount,\n        funds_deposited_data_recipient\n        \n    FROM raw.polygon_logs_processed\n    \n    WHERE topic_0 = '0x32ed1a409ef04c7b0227189c3a103dc5ac10e775a15b785dcc510201f7c25ad3'\n        AND funds_deposited_data_input_amount IS NOT NULL\n        AND funds_deposited_data_output_amount IS NOT NULL\n),\n\n-- INPUT token metadata: filtered to origin chain (Polygon)\ninput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n    WHERE chain = 'polygon'\n\n),\n\n-- OUTPUT token metadata: includes chain_id for matching with destination_chain_id\noutput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals,\n        chain_id\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n\n),\n\ncleaned_deposits AS (\n    SELECT\n        CASE \n            WHEN timestamp_datetime ~ '^\\d+$' THEN\n                TO_TIMESTAMP(timestamp_datetime::BIGINT)\n            ELSE \n                timestamp_datetime::TIMESTAMP\n        END AS deposit_timestamp,\n        \n        transactionHash AS transaction_hash,\n        blockchain,\n        source_file,\n        (topic_destination_chain_id::NUMERIC)::BIGINT AS destination_chain_id,\n        (topic_deposit_id::NUMERIC)::BIGINT AS deposit_id,\n        topic_depositor AS depositor_address,\n        LOWER(funds_deposited_data_input_token) AS input_token_address,\n        LOWER(funds_deposited_data_output_token) AS output_token_address,\n        funds_deposited_data_input_amount::NUMERIC AS input_amount_raw,\n        funds_deposited_data_output_amount::NUMERIC AS output_amount_raw,\n        funds_deposited_data_recipient AS recipient_address\n        \n    FROM raw_deposits\n)\n\nSELECT\n    c.deposit_timestamp,\n    c.transaction_hash,\n    c.blockchain,\n    c.source_file,\n    c.destination_chain_id,\n    c.deposit_id,\n    c.depositor_address,\n    c.input_token_address,\n    input_tok.token_symbol AS input_token_symbol,\n    c.output_token_address,\n    output_tok.token_symbol AS output_token_symbol,\n    \n    c.input_amount_raw / POWER(10, COALESCE(input_tok.decimals, 18))\n AS input_amount,\n    \n    CASE \n        WHEN c.output_token_address = '0x0000000000000000000000000000000000000000' \n        THEN c.output_amount_raw / POWER(10, COALESCE(input_tok.decimals, 18))\n        ELSE c.output_amount_raw / POWER(10, COALESCE(output_tok.decimals, 18))\n    END\n AS output_amount,\n    c.input_amount_raw,\n    c.output_amount_raw,\n    c.recipient_address\n    \nFROM cleaned_deposits c\n\nLEFT JOIN input_token_meta AS input_tok\n    ON c.input_token_address = input_tok.token_address\n\nLEFT JOIN output_token_meta AS output_tok\n    ON c.output_token_address = output_tok.token_address\n    AND c.destination_chain_id = output_tok.chain_id\n\nWHERE c.deposit_id IS NOT NULL\n    AND c.depositor_address IS NOT NULL\n    AND c.input_token_address IS NOT NULL\n    AND c.output_token_address IS NOT NULL\n    AND c.input_amount_raw IS NOT NULL\n    AND c.output_amount_raw IS NOT NULL", "relation_name": "\"across_analytics\".\"dbt_staging\".\"stg_polygon__deposits\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2026-01-05T13:09:39.764477Z", "completed_at": "2026-01-05T13:09:39.788736Z"}, {"name": "execute", "started_at": "2026-01-05T13:09:39.806959Z", "completed_at": "2026-01-05T13:09:40.267300Z"}], "thread_id": "Thread-1 (worker)", "execution_time": 0.5098876953125, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.stg_polygon__fills", "compiled": true, "compiled_code": "-- Staging model for FilledRelay events from Polygon\n-- This model extracts fill events where relayers complete cross-chain bridge transactions\n\nWITH raw_fills AS (\n\n    SELECT\n        -- Core event metadata - these identify WHEN and WHERE the event happened\n        timestamp_datetime,              -- When the event occurred (from blockchain)\n        transactionHash,                 -- Unique transaction identifier\n        blockchain,                      -- Which blockchain (should be 'polygon')\n        source_file,                     -- Which source file this came from (for lineage)\n        \n        -- Indexed fields from topics (these are searchable/filterable in blockchain)\n        -- Topics are like \"indexed columns\" in SQL - they're stored separately for fast queries\n        topic_origin_chain_id,           -- Chain ID where the original deposit happened\n        topic_deposit_id,                -- Unique deposit identifier (links to deposit event)\n        topic_relayer,                   -- Address of the relayer who provided liquidity\n        \n        -- Non-indexed fields from the event's data field\n        -- These are the actual business data about the fill\n        filled_relay_data_input_token,          -- Token address on origin chain\n        filled_relay_data_output_token,          -- Token address on destination chain\n        filled_relay_data_input_amount,          -- Amount sent (in origin token units)\n        filled_relay_data_output_amount,         -- Amount received (in destination token units)\n        filled_relay_data_repayment_chain_id,     -- Where relayer gets reimbursed\n        filled_relay_data_exclusive_relayer,     -- Address with exclusive fill rights (if any)\n        filled_relay_data_depositor,             -- Original user who initiated the bridge\n        filled_relay_data_recipient,              -- Final recipient of the bridged funds\n        \n        -- Gas data (for relayer cost analysis)\n        gas_price_wei,\n        gas_used\n        \n    FROM raw.polygon_logs_processed\n    \n    -- Filter: Only include rows where FilledRelay data exists\n    WHERE topic_0 = '0x44b559f101f8fbcc8a0ea43fa91a05a729a5ea6e14a7c75aa750374690137208'\n        AND filled_relay_data_input_amount IS NOT NULL\n        AND filled_relay_data_output_amount IS NOT NULL\n),\n\n-- INPUT token metadata: includes chain_id for matching with origin_chain_id\n-- This allows us to find the correct token on whichever chain the deposit came from\ninput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals,\n        chain_id\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n\n),\n\n-- OUTPUT token metadata: filtered to destination chain (Polygon)\n-- This is correct because on fills, output_token is always on the destination chain\noutput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n    WHERE chain = 'polygon'\n\n),\n\ncleaned_fills AS (\n    SELECT\n        \n        -- Timestamp: Convert text to proper timestamp type\n        \n        CASE \n            WHEN timestamp_datetime ~ '^\\d+$' THEN          -- If it's pure digits, treat as Unix timestamp (seconds since 1970-01-01)\n                TO_TIMESTAMP(timestamp_datetime::BIGINT)    -- PostgreSQL's TO_TIMESTAMP can parse various formats, but we use the simplest approach\n            ELSE \n                timestamp_datetime::TIMESTAMP               -- Otherwise, try to parse as ISO format string\n        END AS fill_timestamp,\n        \n        transactionHash AS transaction_hash,\n        blockchain,\n        source_file,\n        \n        -- ============================================================\n        -- INDEXED FIELDS (from topics) - Already decoded by ETL\n        -- ============================================================\n        -- These come from the event's \"topics\" array, which is indexed for fast blockchain queries\n        \n        -- Origin chain ID: Which blockchain the funds came FROM\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\n        -- Example: 1 = Ethereum, 42161 = Arbitrum, 8453 = Base\n        -- Cast via NUMERIC first to handle decimal strings like \"8453.0\" from ETL\n        (topic_origin_chain_id::NUMERIC)::BIGINT AS origin_chain_id,\n        \n        -- Deposit ID: Unique identifier linking this fill to its original deposit\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\n        -- This is the KEY that connects deposits \u2194 fills across chains\n        -- Cast via NUMERIC first to handle decimal strings like \"12345.0\" from ETL\n        (topic_deposit_id::NUMERIC)::BIGINT AS deposit_id,\n        \n        -- Relayer address: Who provided the liquidity\n        -- Already decoded by ETL to proper address format (0x...)\n        topic_relayer AS relayer_address,\n        \n        -- ============================================================\n        -- TOKEN INFORMATION (what was bridged)\n        -- ============================================================\n        \n        -- Input token: The token address on the origin chain\n        -- Already decoded by ETL to proper address format\n        -- Example: 0xA0b86991c6218b36c1d19D4a2e9Eb0cE3606eB48 = USDC on Ethereum\n        -- Normalize to lowercase for joining with token metadata\n        LOWER(filled_relay_data_input_token) AS input_token_address,\n        \n        -- Output token: The token address on the destination chain (Polygon in this case)\n        -- Already decoded by ETL to proper address format\n        -- Normalize to lowercase for joining with token metadata\n        LOWER(filled_relay_data_output_token) AS output_token_address,\n        \n        -- ============================================================\n        -- AMOUNT INFORMATION (how much was bridged)\n        -- ============================================================\n        \n        -- Input amount: How much was sent from origin chain\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\n        -- Store as RAW amount first (before rescaling by decimals)\n        filled_relay_data_input_amount::NUMERIC AS input_amount_raw,\n        \n        -- Output amount: How much was received on destination chain\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\n        -- Usually slightly less than input due to fees/spread\n        -- Store as RAW amount first (before rescaling by decimals)\n        filled_relay_data_output_amount::NUMERIC AS output_amount_raw,\n        \n        -- ============================================================\n        -- RELAYER & ROUTING INFORMATION\n        -- ============================================================\n        \n        -- Repayment chain ID: Where the relayer gets reimbursed\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\n        -- Relayers front capital, then get paid back (often on a different chain)\n        -- Cast via NUMERIC first to handle decimal strings like \"1.0\" from ETL\n        (filled_relay_data_repayment_chain_id::NUMERIC)::BIGINT AS repayment_chain_id,\n        \n        -- Exclusive relayer: If set, only this address can fill this deposit\n        -- Already decoded by ETL to proper address format\n        -- Used for priority/guaranteed fills (NULL if no exclusive relayer)\n        filled_relay_data_exclusive_relayer AS exclusive_relayer_address,\n        \n        -- ============================================================\n        -- USER INFORMATION\n        -- ============================================================\n        \n        -- Depositor: The original user who initiated the bridge\n        -- Already decoded by ETL to proper address format\n        filled_relay_data_depositor AS depositor_address,\n        \n        -- Recipient: Who receives the funds on the destination chain\n        -- Already decoded by ETL to proper address format\n        -- Usually the same as depositor, but can be different (gift/transfer)\n        filled_relay_data_recipient AS recipient_address,\n        \n        -- ============================================================\n        -- GAS DATA (for relayer cost analysis)\n        -- ============================================================\n        gas_price_wei::BIGINT AS gas_price_wei,\n        gas_used::BIGINT AS gas_used\n        \n    FROM raw_fills\n)\n\n-- ============================================================\n-- FINAL SELECT: Join with token metadata and rescale amounts\n-- ============================================================\nSELECT\n    -- Event identity\n    c.fill_timestamp,\n    c.transaction_hash,\n    c.blockchain,\n    c.source_file,\n    \n    -- Indexed fields\n    c.origin_chain_id,\n    c.deposit_id,\n    c.relayer_address,\n    \n    -- Token info WITH NAMES\n    -- Input token (what was deposited on origin chain)\n    c.input_token_address,\n    input_tok.token_symbol AS input_token_symbol,        -- e.g., 'USDC', 'WETH'\n    \n    -- Output token (what was received on destination chain - Polygon)\n    c.output_token_address,\n    output_tok.token_symbol AS output_token_symbol,      -- e.g., 'USDC', 'WETH'\n    \n    -- Rescaled amounts (human-readable)\n    -- These use the rescale_amount macro which divides raw amount by 10^decimals\n    -- Example: 5000000 raw USDC (6 decimals) \u2192 5.0 USDC\n    \n    c.input_amount_raw / POWER(10, COALESCE(input_tok.decimals, 18))\n AS input_amount,\n    \n    c.output_amount_raw / POWER(10, COALESCE(output_tok.decimals, 18))\n AS output_amount,\n    \n    -- Raw amounts (preserved for auditing)\n    -- These are the original blockchain values before rescaling\n    -- Example: 5.0 USDC is stored as 5000000 on blockchain\n    c.input_amount_raw,\n    c.output_amount_raw,\n    \n    -- Relayer routing\n    c.repayment_chain_id,\n    c.exclusive_relayer_address,\n    \n    -- User info\n    c.depositor_address,\n    c.recipient_address,\n    \n    -- Gas data (for relayer cost analysis)\n    c.gas_price_wei,\n    c.gas_used,\n    (c.gas_price_wei * c.gas_used) AS gas_cost_wei\n    \nFROM cleaned_fills c\n\n-- Join with token metadata to get decimals and symbols for INPUT token\n-- Join on BOTH address AND chain_id to match the correct origin chain\nLEFT JOIN input_token_meta AS input_tok\n    ON c.input_token_address = input_tok.token_address\n    AND c.origin_chain_id = input_tok.chain_id\n\n-- Join with token metadata to get decimals and symbols for OUTPUT token\n-- Join on address only - we already filtered to destination chain (Polygon)\nLEFT JOIN output_token_meta AS output_tok\n    ON c.output_token_address = output_tok.token_address\n\n-- Data quality: Only include rows with essential fields populated\nWHERE c.deposit_id IS NOT NULL\n    AND c.relayer_address IS NOT NULL\n    AND c.input_token_address IS NOT NULL\n    AND c.output_token_address IS NOT NULL\n    AND c.input_amount_raw IS NOT NULL\n    AND c.output_amount_raw IS NOT NULL", "relation_name": "\"across_analytics\".\"dbt_staging\".\"stg_polygon__fills\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2026-01-05T13:09:40.067829Z", "completed_at": "2026-01-05T13:09:40.117673Z"}, {"name": "execute", "started_at": "2026-01-05T13:09:40.121919Z", "completed_at": "2026-01-05T13:09:40.462252Z"}], "thread_id": "Thread-3 (worker)", "execution_time": 0.41203951835632324, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.stg_polygon__refunds", "compiled": true, "compiled_code": "-- Staging model for ExecutedRelayerRefundRoot events from Polygon\r\n\r\nWITH raw_refunds AS (\r\n\r\n    SELECT\r\n        -- Core event metadata - these identify WHEN and WHERE the event happened\r\n        timestamp_datetime,              -- When the event occurred (from blockchain)\r\n        transactionHash,                 -- Unique transaction identifier\r\n        blockchain,                      -- Which blockchain (should be 'polygon')\r\n        source_file,                     -- Which source file this came from (for lineage)\r\n        \r\n        -- Indexed fields from topics (these are searchable/filterable in blockchain)\r\n        -- Topics are like \"indexed columns\" in SQL - they're stored separately for fast queries\r\n        topic_chain_id,                  -- Chain ID where the refund was executed\r\n        topic_root_bundle_id,            -- Merkle root bundle identifier (groups multiple refunds)\r\n        topic_leaf_id,                   -- Leaf index in the merkle tree (identifies this specific refund batch)\r\n        \r\n        -- Non-indexed fields from the event's data field\r\n        -- These are the actual business data about the refund\r\n        amount_to_return,                -- Total amount of capital returned in this batch\r\n        l2_token_address,                -- Token address being refunded (on Polygon)\r\n        refund_amounts,                  -- Comma-separated list of individual refund amounts\r\n        refund_addresses,                -- Comma-separated list of relayer addresses receiving refunds\r\n        refund_count                     -- Number of relayers being refunded in this batch\r\n        \r\n    FROM raw.polygon_logs_processed\r\n    \r\n    -- Filter: Only include rows where ExecutedRelayerRefundRoot data exists\r\n    -- Topic_0 is the event signature hash that identifies the event type\r\n    WHERE topic_0 = '0xf4ad92585b1bc117fbdd644990adf0827bc4c95baeae8a23322af807b6d0020e'\r\n        AND amount_to_return IS NOT NULL\r\n        AND l2_token_address IS NOT NULL\r\n        AND refund_count IS NOT NULL\r\n),\r\n\r\ncleaned_refunds AS (\r\n    SELECT\r\n        \r\n        -- Timestamp: Convert text to proper timestamp type\r\n        CASE \r\n            WHEN timestamp_datetime ~ '^\\d+$' THEN          -- If it's pure digits, treat as Unix timestamp (seconds since 1970-01-01)\r\n                TO_TIMESTAMP(timestamp_datetime::BIGINT)    -- PostgreSQL's TO_TIMESTAMP converts Unix seconds to timestamp\r\n            ELSE \r\n                timestamp_datetime::TIMESTAMP               -- Otherwise, try to parse as ISO format string (e.g., \"2025-12-03 10:30:00\")\r\n        END AS refund_timestamp,\r\n        \r\n        transactionHash AS transaction_hash,\r\n        blockchain,\r\n        source_file,\r\n        \r\n        -- ============================================================\r\n        -- INDEXED FIELDS (from topics) - Already decoded by ETL\r\n        -- ============================================================\r\n        -- These come from the event's \"topics\" array, which is indexed for fast blockchain queries\r\n        \r\n        -- Chain ID: Which blockchain this refund was executed on\r\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\r\n        -- Example: 42161 = Arbitrum, 1 = Ethereum, 137 = Polygon\r\n        -- Cast via NUMERIC first to handle decimal strings like \"42161.0\" from ETL\r\n        (topic_chain_id::NUMERIC)::BIGINT AS chain_id,\r\n        \r\n        -- Root bundle ID: Groups multiple refunds together in a merkle tree\r\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\r\n        -- Multiple refund batches can be grouped into one merkle root for gas efficiency\r\n        -- Cast via NUMERIC first to handle decimal strings like \"12345.0\" from ETL\r\n        (topic_root_bundle_id::NUMERIC)::BIGINT AS root_bundle_id,\r\n        \r\n        -- Leaf ID: Identifies this specific refund batch within the merkle tree\r\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\r\n        -- Each leaf represents one batch of refunds to multiple relayers\r\n        -- Cast via NUMERIC first to handle decimal strings like \"789.0\" from ETL\r\n        (topic_leaf_id::NUMERIC)::BIGINT AS leaf_id,\r\n        \r\n        -- ============================================================\r\n        -- TOKEN INFORMATION (what was refunded)\r\n        -- ============================================================\r\n        \r\n        -- L2 token address: The token being refunded on Polygon\r\n        -- Already decoded by ETL to proper address format (0x...)\r\n        -- Example: 0x2791Bca1f2de4661ED88A30C99A7a9449Aa84174 = USDC on Polygon\r\n        l2_token_address AS refund_token_address,\r\n        \r\n        -- ============================================================\r\n        -- AMOUNT INFORMATION (how much was refunded)\r\n        -- ============================================================\r\n        \r\n        -- Amount to return: Total capital returned in this batch\r\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\r\n        -- This is the sum of all individual refund amounts in the batch\r\n        -- Stored as NUMERIC to handle large token amounts (18 decimals)\r\n        amount_to_return::NUMERIC AS total_refund_amount,\r\n        \r\n        -- Refund amounts: Individual refund amounts as comma-separated string\r\n        -- Format: \"1000000000000000000,2000000000000000000,3000000000000000000\"\r\n        -- Each value is in token's base units (e.g., wei for ETH, 6 decimals for USDC)\r\n        -- This is a comma-separated string because the ETL processes dynamic arrays\r\n        refund_amounts AS refund_amounts_string,\r\n        \r\n        -- Refund addresses: Relayer addresses receiving refunds as comma-separated string\r\n        -- Format: \"0xAAA...,0xBBB...,0xCCC...\"\r\n        -- Matches 1:1 with refund_amounts (first address gets first amount, etc.)\r\n        -- This is a comma-separated string because the ETL processes dynamic arrays\r\n        refund_addresses AS refund_addresses_string,\r\n        \r\n        -- Refund count: Number of relayers being refunded in this batch\r\n        -- Already converted by ETL to integer\r\n        -- This tells us how many individual refunds are in the arrays above\r\n        refund_count::INTEGER AS refund_count\r\n        \r\n    FROM raw_refunds\r\n)\r\n\r\n-- Final SELECT: Add any computed fields and ensure data quality\r\nSELECT\r\n    -- Event identity\r\n    refund_timestamp,\r\n    transaction_hash,\r\n    blockchain,\r\n    source_file,\r\n    \r\n    -- Indexed fields (merkle tree identifiers)\r\n    chain_id,\r\n    root_bundle_id,\r\n    leaf_id,\r\n    \r\n    -- Token info\r\n    refund_token_address,\r\n    \r\n    -- Amounts\r\n    total_refund_amount,\r\n    refund_amounts_string,\r\n    refund_addresses_string,\r\n    refund_count\r\n    \r\nFROM cleaned_refunds\r\n\r\n-- Data quality: Only include rows with essential fields populated\r\nWHERE chain_id IS NOT NULL\r\n    AND refund_token_address IS NOT NULL\r\n    AND total_refund_amount IS NOT NULL\r\n    AND refund_count IS NOT NULL", "relation_name": "\"across_analytics\".\"dbt_staging\".\"stg_polygon__refunds\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2026-01-05T13:09:40.224038Z", "completed_at": "2026-01-05T13:09:40.240530Z"}, {"name": "execute", "started_at": "2026-01-05T13:09:40.246224Z", "completed_at": "2026-01-05T13:09:40.620913Z"}], "thread_id": "Thread-2 (worker)", "execution_time": 0.40708088874816895, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.stg_unichain__deposits", "compiled": true, "compiled_code": "-- Staging model for FundsDeposited events from Unichain\n-- This model extracts deposit events where users initiate cross-chain bridge transactions\n\nWITH raw_deposits AS (\n\n    SELECT\n        timestamp_datetime,\n        transactionHash,\n        blockchain,\n        source_file,\n        topic_destination_chain_id,\n        topic_deposit_id,\n        topic_depositor,\n        funds_deposited_data_input_token,\n        funds_deposited_data_output_token,\n        funds_deposited_data_input_amount,\n        funds_deposited_data_output_amount,\n        funds_deposited_data_recipient\n        \n    FROM raw.unichain_logs_processed\n    \n    WHERE topic_0 = '0x32ed1a409ef04c7b0227189c3a103dc5ac10e775a15b785dcc510201f7c25ad3'\n        AND funds_deposited_data_input_amount IS NOT NULL\n        AND funds_deposited_data_output_amount IS NOT NULL\n),\n\n-- INPUT token metadata: filtered to origin chain (Unichain)\n-- Note: unichain may not have tokens in token_metadata yet\ninput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n    WHERE chain = 'unichain'\n\n),\n\n-- OUTPUT token metadata: includes chain_id for matching with destination_chain_id\noutput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals,\n        chain_id\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n\n),\n\ncleaned_deposits AS (\n    SELECT\n        CASE \n            WHEN timestamp_datetime ~ '^\\d+$' THEN\n                TO_TIMESTAMP(timestamp_datetime::BIGINT)\n            ELSE \n                timestamp_datetime::TIMESTAMP\n        END AS deposit_timestamp,\n        \n        transactionHash AS transaction_hash,\n        blockchain,\n        source_file,\n        (topic_destination_chain_id::NUMERIC)::BIGINT AS destination_chain_id,\n        (topic_deposit_id::NUMERIC)::BIGINT AS deposit_id,\n        topic_depositor AS depositor_address,\n        LOWER(funds_deposited_data_input_token) AS input_token_address,\n        LOWER(funds_deposited_data_output_token) AS output_token_address,\n        funds_deposited_data_input_amount::NUMERIC AS input_amount_raw,\n        funds_deposited_data_output_amount::NUMERIC AS output_amount_raw,\n        funds_deposited_data_recipient AS recipient_address\n        \n    FROM raw_deposits\n)\n\nSELECT\n    c.deposit_timestamp,\n    c.transaction_hash,\n    c.blockchain,\n    c.source_file,\n    c.destination_chain_id,\n    c.deposit_id,\n    c.depositor_address,\n    c.input_token_address,\n    input_tok.token_symbol AS input_token_symbol,\n    c.output_token_address,\n    output_tok.token_symbol AS output_token_symbol,\n    \n    c.input_amount_raw / POWER(10, COALESCE(input_tok.decimals, 18))\n AS input_amount,\n    \n    CASE \n        WHEN c.output_token_address = '0x0000000000000000000000000000000000000000' \n        THEN c.output_amount_raw / POWER(10, COALESCE(input_tok.decimals, 18))\n        ELSE c.output_amount_raw / POWER(10, COALESCE(output_tok.decimals, 18))\n    END\n AS output_amount,\n    c.input_amount_raw,\n    c.output_amount_raw,\n    c.recipient_address\n    \nFROM cleaned_deposits c\n\nLEFT JOIN input_token_meta AS input_tok\n    ON c.input_token_address = input_tok.token_address\n\nLEFT JOIN output_token_meta AS output_tok\n    ON c.output_token_address = output_tok.token_address\n    AND c.destination_chain_id = output_tok.chain_id\n\nWHERE c.deposit_id IS NOT NULL\n    AND c.depositor_address IS NOT NULL\n    AND c.input_token_address IS NOT NULL\n    AND c.output_token_address IS NOT NULL\n    AND c.input_amount_raw IS NOT NULL\n    AND c.output_amount_raw IS NOT NULL", "relation_name": "\"across_analytics\".\"dbt_staging\".\"stg_unichain__deposits\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2026-01-05T13:09:40.305153Z", "completed_at": "2026-01-05T13:09:40.329641Z"}, {"name": "execute", "started_at": "2026-01-05T13:09:40.331113Z", "completed_at": "2026-01-05T13:09:40.742558Z"}], "thread_id": "Thread-4 (worker)", "execution_time": 0.44408178329467773, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.stg_unichain__fills", "compiled": true, "compiled_code": "-- Staging model for FilledRelay events from Unichain\n\nWITH raw_fills AS (\n    SELECT\n        timestamp_datetime, transactionHash, blockchain, source_file,\n        topic_origin_chain_id, topic_deposit_id, topic_relayer,\n        filled_relay_data_input_token, filled_relay_data_output_token,\n        filled_relay_data_input_amount, filled_relay_data_output_amount,\n        filled_relay_data_repayment_chain_id, filled_relay_data_exclusive_relayer,\n        filled_relay_data_depositor, filled_relay_data_recipient,\n        gas_price_wei, gas_used\n    FROM raw.unichain_logs_processed\n    WHERE topic_0 = '0x44b559f101f8fbcc8a0ea43fa91a05a729a5ea6e14a7c75aa750374690137208'\n        AND filled_relay_data_input_amount IS NOT NULL\n        AND filled_relay_data_output_amount IS NOT NULL\n),\n\ninput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals,\n        chain_id\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n\n),\n\noutput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n    WHERE chain = 'unichain'\n\n),\n\ncleaned_fills AS (\n    SELECT\n        CASE \n            WHEN timestamp_datetime ~ '^\\d+$' THEN TO_TIMESTAMP(timestamp_datetime::BIGINT)\n            ELSE timestamp_datetime::TIMESTAMP\n        END AS fill_timestamp,\n        transactionHash AS transaction_hash,\n        blockchain, source_file,\n        (topic_origin_chain_id::NUMERIC)::BIGINT AS origin_chain_id,\n        (topic_deposit_id::NUMERIC)::BIGINT AS deposit_id,\n        topic_relayer AS relayer_address,\n        LOWER(filled_relay_data_input_token) AS input_token_address,\n        LOWER(filled_relay_data_output_token) AS output_token_address,\n        filled_relay_data_input_amount::NUMERIC AS input_amount_raw,\n        filled_relay_data_output_amount::NUMERIC AS output_amount_raw,\n        (filled_relay_data_repayment_chain_id::NUMERIC)::BIGINT AS repayment_chain_id,\n        filled_relay_data_exclusive_relayer AS exclusive_relayer_address,\n        filled_relay_data_depositor AS depositor_address,\n        filled_relay_data_recipient AS recipient_address,\n        gas_price_wei::BIGINT AS gas_price_wei,\n        gas_used::BIGINT AS gas_used\n    FROM raw_fills\n)\n\nSELECT\n    c.fill_timestamp, c.transaction_hash, c.blockchain, c.source_file,\n    c.origin_chain_id, c.deposit_id, c.relayer_address,\n    c.input_token_address, input_tok.token_symbol AS input_token_symbol,\n    c.output_token_address, output_tok.token_symbol AS output_token_symbol,\n    \n    c.input_amount_raw / POWER(10, COALESCE(input_tok.decimals, 18))\n AS input_amount,\n    \n    c.output_amount_raw / POWER(10, COALESCE(output_tok.decimals, 18))\n AS output_amount,\n    c.input_amount_raw, c.output_amount_raw,\n    c.repayment_chain_id, c.exclusive_relayer_address,\n    c.depositor_address, c.recipient_address,\n    c.gas_price_wei, c.gas_used, (c.gas_price_wei * c.gas_used) AS gas_cost_wei\nFROM cleaned_fills c\nLEFT JOIN input_token_meta AS input_tok\n    ON c.input_token_address = input_tok.token_address AND c.origin_chain_id = input_tok.chain_id\nLEFT JOIN output_token_meta AS output_tok\n    ON c.output_token_address = output_tok.token_address\nWHERE c.deposit_id IS NOT NULL AND c.relayer_address IS NOT NULL\n    AND c.input_token_address IS NOT NULL AND c.output_token_address IS NOT NULL\n    AND c.input_amount_raw IS NOT NULL AND c.output_amount_raw IS NOT NULL", "relation_name": "\"across_analytics\".\"dbt_staging\".\"stg_unichain__fills\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2026-01-05T13:09:40.322738Z", "completed_at": "2026-01-05T13:09:40.373729Z"}, {"name": "execute", "started_at": "2026-01-05T13:09:40.375306Z", "completed_at": "2026-01-05T13:09:40.840578Z"}], "thread_id": "Thread-1 (worker)", "execution_time": 0.5231988430023193, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.stg_unichain__refunds", "compiled": true, "compiled_code": "-- Staging model for ExecutedRelayerRefundRoot events from Unichain\r\n\r\nWITH raw_refunds AS (\r\n\r\n    SELECT\r\n        -- Core event metadata - these identify WHEN and WHERE the event happened\r\n        timestamp_datetime,              -- When the event occurred (from blockchain)\r\n        transactionHash,                 -- Unique transaction identifier\r\n        blockchain,                      -- Which blockchain (should be 'unichain')\r\n        source_file,                     -- Which source file this came from (for lineage)\r\n        \r\n        -- Indexed fields from topics (these are searchable/filterable in blockchain)\r\n        -- Topics are like \"indexed columns\" in SQL - they're stored separately for fast queries\r\n        topic_chain_id,                  -- Chain ID where the refund was executed\r\n        topic_root_bundle_id,            -- Merkle root bundle identifier (groups multiple refunds)\r\n        topic_leaf_id,                   -- Leaf index in the merkle tree (identifies this specific refund batch)\r\n        \r\n        -- Non-indexed fields from the event's data field\r\n        -- These are the actual business data about the refund\r\n        amount_to_return,                -- Total amount of capital returned in this batch\r\n        l2_token_address,                -- Token address being refunded (on Unichain)\r\n        refund_amounts,                  -- Comma-separated list of individual refund amounts\r\n        refund_addresses,                -- Comma-separated list of relayer addresses receiving refunds\r\n        refund_count                     -- Number of relayers being refunded in this batch\r\n        \r\n    FROM raw.unichain_logs_processed\r\n    \r\n    -- Filter: Only include rows where ExecutedRelayerRefundRoot data exists\r\n    -- Topic_0 is the event signature hash that identifies the event type\r\n    WHERE topic_0 = '0xf4ad92585b1bc117fbdd644990adf0827bc4c95baeae8a23322af807b6d0020e'\r\n        AND amount_to_return IS NOT NULL\r\n        AND l2_token_address IS NOT NULL\r\n        AND refund_count IS NOT NULL\r\n),\r\n\r\ncleaned_refunds AS (\r\n    SELECT\r\n        \r\n        -- Timestamp: Convert text to proper timestamp type\r\n        CASE \r\n            WHEN timestamp_datetime ~ '^\\d+$' THEN          -- If it's pure digits, treat as Unix timestamp (seconds since 1970-01-01)\r\n                TO_TIMESTAMP(timestamp_datetime::BIGINT)    -- PostgreSQL's TO_TIMESTAMP converts Unix seconds to timestamp\r\n            ELSE \r\n                timestamp_datetime::TIMESTAMP               -- Otherwise, try to parse as ISO format string (e.g., \"2025-12-03 10:30:00\")\r\n        END AS refund_timestamp,\r\n        \r\n        transactionHash AS transaction_hash,\r\n        blockchain,\r\n        source_file,\r\n        \r\n        -- ============================================================\r\n        -- INDEXED FIELDS (from topics) - Already decoded by ETL\r\n        -- ============================================================\r\n        -- These come from the event's \"topics\" array, which is indexed for fast blockchain queries\r\n        \r\n        -- Chain ID: Which blockchain this refund was executed on\r\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\r\n        -- Example: 42161 = Arbitrum, 1 = Ethereum, 130 = Unichain\r\n        -- Cast via NUMERIC first to handle decimal strings like \"42161.0\" from ETL\r\n        (topic_chain_id::NUMERIC)::BIGINT AS chain_id,\r\n        \r\n        -- Root bundle ID: Groups multiple refunds together in a merkle tree\r\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\r\n        -- Multiple refund batches can be grouped into one merkle root for gas efficiency\r\n        -- Cast via NUMERIC first to handle decimal strings like \"12345.0\" from ETL\r\n        (topic_root_bundle_id::NUMERIC)::BIGINT AS root_bundle_id,\r\n        \r\n        -- Leaf ID: Identifies this specific refund batch within the merkle tree\r\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\r\n        -- Each leaf represents one batch of refunds to multiple relayers\r\n        -- Cast via NUMERIC first to handle decimal strings like \"789.0\" from ETL\r\n        (topic_leaf_id::NUMERIC)::BIGINT AS leaf_id,\r\n        \r\n        -- ============================================================\r\n        -- TOKEN INFORMATION (what was refunded)\r\n        -- ============================================================\r\n        \r\n        -- L2 token address: The token being refunded on Unichain\r\n        -- Already decoded by ETL to proper address format (0x...)\r\n        -- This is the token address on the Unichain\r\n        l2_token_address AS refund_token_address,\r\n        \r\n        -- ============================================================\r\n        -- AMOUNT INFORMATION (how much was refunded)\r\n        -- ============================================================\r\n        \r\n        -- Amount to return: Total capital returned in this batch\r\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\r\n        -- This is the sum of all individual refund amounts in the batch\r\n        -- Stored as NUMERIC to handle large token amounts (18 decimals)\r\n        amount_to_return::NUMERIC AS total_refund_amount,\r\n        \r\n        -- Refund amounts: Individual refund amounts as comma-separated string\r\n        -- Format: \"1000000000000000000,2000000000000000000,3000000000000000000\"\r\n        -- Each value is in token's base units (e.g., wei for ETH, 6 decimals for USDC)\r\n        -- This is a comma-separated string because the ETL processes dynamic arrays\r\n        refund_amounts AS refund_amounts_string,\r\n        \r\n        -- Refund addresses: Relayer addresses receiving refunds as comma-separated string\r\n        -- Format: \"0xAAA...,0xBBB...,0xCCC...\"\r\n        -- Matches 1:1 with refund_amounts (first address gets first amount, etc.)\r\n        -- This is a comma-separated string because the ETL processes dynamic arrays\r\n        refund_addresses AS refund_addresses_string,\r\n        \r\n        -- Refund count: Number of relayers being refunded in this batch\r\n        -- Already converted by ETL to integer\r\n        -- This tells us how many individual refunds are in the arrays above\r\n        refund_count::INTEGER AS refund_count\r\n        \r\n    FROM raw_refunds\r\n)\r\n\r\n-- Final SELECT: Add any computed fields and ensure data quality\r\nSELECT\r\n    -- Event identity\r\n    refund_timestamp,\r\n    transaction_hash,\r\n    blockchain,\r\n    source_file,\r\n    \r\n    -- Indexed fields (merkle tree identifiers)\r\n    chain_id,\r\n    root_bundle_id,\r\n    leaf_id,\r\n    \r\n    -- Token info\r\n    refund_token_address,\r\n    \r\n    -- Amounts\r\n    total_refund_amount,\r\n    refund_amounts_string,\r\n    refund_addresses_string,\r\n    refund_count\r\n    \r\nFROM cleaned_refunds\r\n\r\n-- Data quality: Only include rows with essential fields populated\r\nWHERE chain_id IS NOT NULL\r\n    AND refund_token_address IS NOT NULL\r\n    AND total_refund_amount IS NOT NULL\r\n    AND refund_count IS NOT NULL", "relation_name": "\"across_analytics\".\"dbt_staging\".\"stg_unichain__refunds\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2026-01-05T13:09:40.481995Z", "completed_at": "2026-01-05T13:09:40.496877Z"}, {"name": "execute", "started_at": "2026-01-05T13:09:40.498308Z", "completed_at": "2026-01-05T13:09:40.868244Z"}], "thread_id": "Thread-3 (worker)", "execution_time": 0.39125657081604004, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.stg_worldchain__deposits", "compiled": true, "compiled_code": "-- Staging model for FundsDeposited events from Worldchain\n-- This model extracts deposit events where users initiate cross-chain bridge transactions\n\nWITH raw_deposits AS (\n\n    SELECT\n        timestamp_datetime,\n        transactionHash,\n        blockchain,\n        source_file,\n        topic_destination_chain_id,\n        topic_deposit_id,\n        topic_depositor,\n        funds_deposited_data_input_token,\n        funds_deposited_data_output_token,\n        funds_deposited_data_input_amount,\n        funds_deposited_data_output_amount,\n        funds_deposited_data_recipient\n        \n    FROM raw.worldchain_logs_processed\n    \n    WHERE topic_0 = '0x32ed1a409ef04c7b0227189c3a103dc5ac10e775a15b785dcc510201f7c25ad3'\n        AND funds_deposited_data_input_amount IS NOT NULL\n        AND funds_deposited_data_output_amount IS NOT NULL\n),\n\n-- INPUT token metadata: filtered to origin chain (Worldchain)\ninput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n    WHERE chain = 'worldchain'\n\n),\n\n-- OUTPUT token metadata: includes chain_id for matching with destination_chain_id\noutput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals,\n        chain_id\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n\n),\n\ncleaned_deposits AS (\n    SELECT\n        CASE \n            WHEN timestamp_datetime ~ '^\\d+$' THEN\n                TO_TIMESTAMP(timestamp_datetime::BIGINT)\n            ELSE \n                timestamp_datetime::TIMESTAMP\n        END AS deposit_timestamp,\n        \n        transactionHash AS transaction_hash,\n        blockchain,\n        source_file,\n        (topic_destination_chain_id::NUMERIC)::BIGINT AS destination_chain_id,\n        (topic_deposit_id::NUMERIC)::BIGINT AS deposit_id,\n        topic_depositor AS depositor_address,\n        LOWER(funds_deposited_data_input_token) AS input_token_address,\n        LOWER(funds_deposited_data_output_token) AS output_token_address,\n        funds_deposited_data_input_amount::NUMERIC AS input_amount_raw,\n        funds_deposited_data_output_amount::NUMERIC AS output_amount_raw,\n        funds_deposited_data_recipient AS recipient_address\n        \n    FROM raw_deposits\n)\n\nSELECT\n    c.deposit_timestamp,\n    c.transaction_hash,\n    c.blockchain,\n    c.source_file,\n    c.destination_chain_id,\n    c.deposit_id,\n    c.depositor_address,\n    c.input_token_address,\n    input_tok.token_symbol AS input_token_symbol,\n    c.output_token_address,\n    output_tok.token_symbol AS output_token_symbol,\n    \n    c.input_amount_raw / POWER(10, COALESCE(input_tok.decimals, 18))\n AS input_amount,\n    \n    CASE \n        WHEN c.output_token_address = '0x0000000000000000000000000000000000000000' \n        THEN c.output_amount_raw / POWER(10, COALESCE(input_tok.decimals, 18))\n        ELSE c.output_amount_raw / POWER(10, COALESCE(output_tok.decimals, 18))\n    END\n AS output_amount,\n    c.input_amount_raw,\n    c.output_amount_raw,\n    c.recipient_address\n    \nFROM cleaned_deposits c\n\nLEFT JOIN input_token_meta AS input_tok\n    ON c.input_token_address = input_tok.token_address\n\nLEFT JOIN output_token_meta AS output_tok\n    ON c.output_token_address = output_tok.token_address\n    AND c.destination_chain_id = output_tok.chain_id\n\nWHERE c.deposit_id IS NOT NULL\n    AND c.depositor_address IS NOT NULL\n    AND c.input_token_address IS NOT NULL\n    AND c.output_token_address IS NOT NULL\n    AND c.input_amount_raw IS NOT NULL\n    AND c.output_amount_raw IS NOT NULL", "relation_name": "\"across_analytics\".\"dbt_staging\".\"stg_worldchain__deposits\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2026-01-05T13:09:40.666459Z", "completed_at": "2026-01-05T13:09:40.715903Z"}, {"name": "execute", "started_at": "2026-01-05T13:09:40.722858Z", "completed_at": "2026-01-05T13:09:41.189282Z"}], "thread_id": "Thread-2 (worker)", "execution_time": 0.5325958728790283, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.stg_worldchain__fills", "compiled": true, "compiled_code": "-- Staging model for FilledRelay events from Worldchain\n-- This model extracts fill events where relayers complete cross-chain bridge transactions\n\nWITH raw_fills AS (\n\n    SELECT\n        -- Core event metadata - these identify WHEN and WHERE the event happened\n        timestamp_datetime,              -- When the event occurred (from blockchain)\n        transactionHash,                 -- Unique transaction identifier\n        blockchain,                      -- Which blockchain (should be 'worldchain')\n        source_file,                     -- Which source file this came from (for lineage)\n        \n        -- Indexed fields from topics (these are searchable/filterable in blockchain)\n        -- Topics are like \"indexed columns\" in SQL - they're stored separately for fast queries\n        topic_origin_chain_id,           -- Chain ID where the original deposit happened\n        topic_deposit_id,                -- Unique deposit identifier (links to deposit event)\n        topic_relayer,                   -- Address of the relayer who provided liquidity\n        \n        -- Non-indexed fields from the event's data field\n        -- These are the actual business data about the fill\n        filled_relay_data_input_token,          -- Token address on origin chain\n        filled_relay_data_output_token,          -- Token address on destination chain\n        filled_relay_data_input_amount,          -- Amount sent (in origin token units)\n        filled_relay_data_output_amount,         -- Amount received (in destination token units)\n        filled_relay_data_repayment_chain_id,     -- Where relayer gets reimbursed\n        filled_relay_data_exclusive_relayer,     -- Address with exclusive fill rights (if any)\n        filled_relay_data_depositor,             -- Original user who initiated the bridge\n        filled_relay_data_recipient,              -- Final recipient of the bridged funds\n        \n        -- Gas data (for relayer cost analysis)\n        gas_price_wei,\n        gas_used\n        \n    FROM raw.worldchain_logs_processed\n    \n    -- Filter: Only include rows where FilledRelay data exists\n    WHERE topic_0 = '0x44b559f101f8fbcc8a0ea43fa91a05a729a5ea6e14a7c75aa750374690137208'\n        AND filled_relay_data_input_amount IS NOT NULL\n        AND filled_relay_data_output_amount IS NOT NULL\n),\n\n-- INPUT token metadata: includes chain_id for matching with origin_chain_id\n-- This allows us to find the correct token on whichever chain the deposit came from\ninput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals,\n        chain_id\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n\n),\n\n-- OUTPUT token metadata: filtered to destination chain (Worldchain)\n-- This is correct because on fills, output_token is always on the destination chain\noutput_token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n    WHERE chain = 'worldchain'\n\n),\n\ncleaned_fills AS (\n    SELECT\n        \n        -- Timestamp: Convert text to proper timestamp type\n        \n        CASE \n            WHEN timestamp_datetime ~ '^\\d+$' THEN          -- If it's pure digits, treat as Unix timestamp (seconds since 1970-01-01)\n                TO_TIMESTAMP(timestamp_datetime::BIGINT)    -- PostgreSQL's TO_TIMESTAMP can parse various formats, but we use the simplest approach\n            ELSE \n                timestamp_datetime::TIMESTAMP               -- Otherwise, try to parse as ISO format string\n        END AS fill_timestamp,\n        \n        transactionHash AS transaction_hash,\n        blockchain,\n        source_file,\n        \n        -- ============================================================\n        -- INDEXED FIELDS (from topics) - Already decoded by ETL\n        -- ============================================================\n        -- These come from the event's \"topics\" array, which is indexed for fast blockchain queries\n        \n        -- Origin chain ID: Which blockchain the funds came FROM\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\n        -- Example: 1 = Ethereum, 42161 = Arbitrum, 8453 = Base\n        -- Cast via NUMERIC first to handle decimal strings like \"8453.0\" from ETL\n        (topic_origin_chain_id::NUMERIC)::BIGINT AS origin_chain_id,\n        \n        -- Deposit ID: Unique identifier linking this fill to its original deposit\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\n        -- This is the KEY that connects deposits \u2194 fills across chains\n        -- Cast via NUMERIC first to handle decimal strings like \"12345.0\" from ETL\n        (topic_deposit_id::NUMERIC)::BIGINT AS deposit_id,\n        \n        -- Relayer address: Who provided the liquidity\n        -- Already decoded by ETL to proper address format (0x...)\n        topic_relayer AS relayer_address,\n        \n        -- ============================================================\n        -- TOKEN INFORMATION (what was bridged)\n        -- ============================================================\n        \n        -- Input token: The token address on the origin chain\n        -- Already decoded by ETL to proper address format\n        -- Example: 0xA0b86991c6218b36c1d19D4a2e9Eb0cE3606eB48 = USDC on Ethereum\n        -- Normalize to lowercase for joining with token metadata\n        LOWER(filled_relay_data_input_token) AS input_token_address,\n        \n        -- Output token: The token address on the destination chain (Worldchain in this case)\n        -- Already decoded by ETL to proper address format\n        -- Normalize to lowercase for joining with token metadata\n        LOWER(filled_relay_data_output_token) AS output_token_address,\n        \n        -- ============================================================\n        -- AMOUNT INFORMATION (how much was bridged)\n        -- ============================================================\n        \n        -- Input amount: How much was sent from origin chain\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\n        -- Store as RAW amount first (before rescaling by decimals)\n        filled_relay_data_input_amount::NUMERIC AS input_amount_raw,\n        \n        -- Output amount: How much was received on destination chain\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\n        -- Usually slightly less than input due to fees/spread\n        -- Store as RAW amount first (before rescaling by decimals)\n        filled_relay_data_output_amount::NUMERIC AS output_amount_raw,\n        \n        -- ============================================================\n        -- RELAYER & ROUTING INFORMATION\n        -- ============================================================\n        \n        -- Repayment chain ID: Where the relayer gets reimbursed\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\n        -- Relayers front capital, then get paid back (often on a different chain)\n        -- Cast via NUMERIC first to handle decimal strings like \"1.0\" from ETL\n        (filled_relay_data_repayment_chain_id::NUMERIC)::BIGINT AS repayment_chain_id,\n        \n        -- Exclusive relayer: If set, only this address can fill this deposit\n        -- Already decoded by ETL to proper address format\n        -- Used for priority/guaranteed fills (NULL if no exclusive relayer)\n        filled_relay_data_exclusive_relayer AS exclusive_relayer_address,\n        \n        -- ============================================================\n        -- USER INFORMATION\n        -- ============================================================\n        \n        -- Depositor: The original user who initiated the bridge\n        -- Already decoded by ETL to proper address format\n        filled_relay_data_depositor AS depositor_address,\n        \n        -- Recipient: Who receives the funds on the destination chain\n        -- Already decoded by ETL to proper address format\n        -- Usually the same as depositor, but can be different (gift/transfer)\n        filled_relay_data_recipient AS recipient_address,\n        \n        -- ============================================================\n        -- GAS DATA (for relayer cost analysis)\n        -- ============================================================\n        gas_price_wei::BIGINT AS gas_price_wei,\n        gas_used::BIGINT AS gas_used\n        \n    FROM raw_fills\n)\n\n-- ============================================================\n-- FINAL SELECT: Join with token metadata and rescale amounts\n-- ============================================================\nSELECT\n    -- Event identity\n    c.fill_timestamp,\n    c.transaction_hash,\n    c.blockchain,\n    c.source_file,\n    \n    -- Indexed fields\n    c.origin_chain_id,\n    c.deposit_id,\n    c.relayer_address,\n    \n    -- Token info WITH NAMES\n    -- Input token (what was deposited on origin chain)\n    c.input_token_address,\n    input_tok.token_symbol AS input_token_symbol,        -- e.g., 'USDC', 'WETH'\n    \n    -- Output token (what was received on destination chain - Worldchain)\n    c.output_token_address,\n    output_tok.token_symbol AS output_token_symbol,      -- e.g., 'USDC', 'WETH'\n    \n    -- Rescaled amounts (human-readable)\n    -- These use the rescale_amount macro which divides raw amount by 10^decimals\n    -- Example: 5000000 raw USDC (6 decimals) \u2192 5.0 USDC\n    \n    c.input_amount_raw / POWER(10, COALESCE(input_tok.decimals, 18))\n AS input_amount,\n    \n    c.output_amount_raw / POWER(10, COALESCE(output_tok.decimals, 18))\n AS output_amount,\n    \n    -- Raw amounts (preserved for auditing)\n    -- These are the original blockchain values before rescaling\n    -- Example: 5.0 USDC is stored as 5000000 on blockchain\n    c.input_amount_raw,\n    c.output_amount_raw,\n    \n    -- Relayer routing\n    c.repayment_chain_id,\n    c.exclusive_relayer_address,\n    \n    -- User info\n    c.depositor_address,\n    c.recipient_address,\n    \n    -- Gas data (for relayer cost analysis)\n    c.gas_price_wei,\n    c.gas_used,\n    (c.gas_price_wei * c.gas_used) AS gas_cost_wei\n    \nFROM cleaned_fills c\n\n-- Join with token metadata to get decimals and symbols for INPUT token\n-- Join on BOTH address AND chain_id to match the correct origin chain\nLEFT JOIN input_token_meta AS input_tok\n    ON c.input_token_address = input_tok.token_address\n    AND c.origin_chain_id = input_tok.chain_id\n\n-- Join with token metadata to get decimals and symbols for OUTPUT token\n-- Join on address only - we already filtered to destination chain (Worldchain)\nLEFT JOIN output_token_meta AS output_tok\n    ON c.output_token_address = output_tok.token_address\n\n-- Data quality: Only include rows with essential fields populated\nWHERE c.deposit_id IS NOT NULL\n    AND c.relayer_address IS NOT NULL\n    AND c.input_token_address IS NOT NULL\n    AND c.output_token_address IS NOT NULL\n    AND c.input_amount_raw IS NOT NULL\n    AND c.output_amount_raw IS NOT NULL", "relation_name": "\"across_analytics\".\"dbt_staging\".\"stg_worldchain__fills\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2026-01-05T13:09:40.890193Z", "completed_at": "2026-01-05T13:09:40.911255Z"}, {"name": "execute", "started_at": "2026-01-05T13:09:40.912552Z", "completed_at": "2026-01-05T13:09:41.211111Z"}], "thread_id": "Thread-1 (worker)", "execution_time": 0.32711362838745117, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.int_unified_deposits", "compiled": true, "compiled_code": "-- int_unified_deposits.sql\n-- PURPOSE: Combine deposits from ALL chains into ONE table\n-- WHY: Right now deposits are separate per chain. We need them unified to track cross-chain flows.\n\n\n\n-- Each CTE selects from a chain's staging model and adds the origin chain ID\nWITH arbitrum_deposits AS (\n    SELECT \n        deposit_timestamp,\n        transaction_hash,\n        42161 AS origin_chain_id,  -- Arbitrum's chain ID\n        destination_chain_id,\n        deposit_id,\n        depositor_address,\n        recipient_address,\n        input_token_address,\n        input_token_symbol,\n        output_token_address,\n        output_token_symbol,\n        input_amount,\n        output_amount\n    FROM \"across_analytics\".\"dbt_staging\".\"stg_arbitrum__deposits\"\n),\n\nethereum_deposits AS (\n    SELECT \n        deposit_timestamp,\n        transaction_hash,\n        1 AS origin_chain_id,  -- Ethereum's chain ID\n        destination_chain_id,\n        deposit_id,\n        depositor_address,\n        recipient_address,\n        input_token_address,\n        input_token_symbol,\n        output_token_address,\n        output_token_symbol,\n        input_amount,\n        output_amount\n    FROM \"across_analytics\".\"dbt_staging\".\"stg_ethereum__deposits\"\n),\n\npolygon_deposits AS (\n    SELECT \n        deposit_timestamp,\n        transaction_hash,\n        137 AS origin_chain_id,  -- Polygon's chain ID\n        destination_chain_id,\n        deposit_id,\n        depositor_address,\n        recipient_address,\n        input_token_address,\n        input_token_symbol,\n        output_token_address,\n        output_token_symbol,\n        input_amount,\n        output_amount\n    FROM \"across_analytics\".\"dbt_staging\".\"stg_polygon__deposits\"\n),\n\nlinea_deposits AS (\n    SELECT \n        deposit_timestamp,\n        transaction_hash,\n        59144 AS origin_chain_id,  -- Linea's chain ID\n        destination_chain_id,\n        deposit_id,\n        depositor_address,\n        recipient_address,\n        input_token_address,\n        input_token_symbol,\n        output_token_address,\n        output_token_symbol,\n        input_amount,\n        output_amount\n    FROM \"across_analytics\".\"dbt_staging\".\"stg_linea__deposits\"\n),\n\nworldchain_deposits AS (\n    SELECT \n        deposit_timestamp,\n        transaction_hash,\n        480 AS origin_chain_id,  -- WorldChain's chain ID\n        destination_chain_id,\n        deposit_id,\n        depositor_address,\n        recipient_address,\n        input_token_address,\n        input_token_symbol,\n        output_token_address,\n        output_token_symbol,\n        input_amount,\n        output_amount\n    FROM \"across_analytics\".\"dbt_staging\".\"stg_worldchain__deposits\"\n),\n\nunichain_deposits AS (\n    SELECT \n        deposit_timestamp,\n        transaction_hash,\n        130 AS origin_chain_id,  -- Unichain's chain ID\n        destination_chain_id,\n        deposit_id,\n        depositor_address,\n        recipient_address,\n        input_token_address,\n        input_token_symbol,\n        output_token_address,\n        output_token_symbol,\n        input_amount,\n        output_amount\n    FROM \"across_analytics\".\"dbt_staging\".\"stg_unichain__deposits\"\n),\n\nhyperevm_deposits AS (\n    SELECT \n        deposit_timestamp,\n        transaction_hash,\n        999 AS origin_chain_id,  -- HyperEVM's chain ID\n        destination_chain_id,\n        deposit_id,\n        depositor_address,\n        recipient_address,\n        input_token_address,\n        input_token_symbol,\n        output_token_address,\n        output_token_symbol,\n        input_amount,\n        output_amount\n    FROM \"across_analytics\".\"dbt_staging\".\"stg_hyperevm__deposits\"\n),\n\nmonad_deposits AS (\n    SELECT \n        deposit_timestamp,\n        transaction_hash,\n        143 AS origin_chain_id,  -- Monad's chain ID\n        destination_chain_id,\n        deposit_id,\n        depositor_address,\n        recipient_address,\n        input_token_address,\n        input_token_symbol,\n        output_token_address,\n        output_token_symbol,\n        input_amount,\n        output_amount\n    FROM \"across_analytics\".\"dbt_staging\".\"stg_monad__deposits\"\n),\n\n-- Supported chain IDs (chains we have parquet data for)\n-- 42161=Arbitrum, 1=Ethereum, 137=Polygon, 59144=Linea, 480=Worldchain, 130=Unichain, 999=HyperEVM, 143=Monad\n\n-- Chain ID to Name mapping for chains with parquet data\nchain_names AS (\n    SELECT chain_id, chain_name\n    FROM (\n        VALUES\n        (1, 'Ethereum'), (42161, 'Arbitrum'), (137, 'Polygon'),\n        (59144, 'Linea'), (480, 'Worldchain'), (130, 'Unichain'),\n        (999, 'HyperEVM'), (143, 'Monad')\n    ) AS chains(chain_id, chain_name)\n),\n\n-- Hourly token prices for USD conversion\ntoken_prices AS (\n    SELECT \n        token_symbol,\n        DATE_TRUNC('hour', timestamp::TIMESTAMP) AS price_hour,\n        AVG(price_usd) AS price_usd\n    FROM \"across_analytics\".\"dbt\".\"token_prices\"\n    GROUP BY token_symbol, DATE_TRUNC('hour', timestamp::TIMESTAMP)\n),\n\n-- UNION ALL: Stack all deposits from all chains into one table\n-- Filter: Only include deposits where destination_chain_id is a supported chain\nall_deposits AS (\n    SELECT * FROM arbitrum_deposits WHERE destination_chain_id IN (42161, 1, 137, 59144, 480, 130, 999, 143)\n    UNION ALL\n    SELECT * FROM ethereum_deposits WHERE destination_chain_id IN (42161, 1, 137, 59144, 480, 130, 999, 143)\n    UNION ALL\n    SELECT * FROM polygon_deposits WHERE destination_chain_id IN (42161, 1, 137, 59144, 480, 130, 999, 143)\n    UNION ALL\n    SELECT * FROM linea_deposits WHERE destination_chain_id IN (42161, 1, 137, 59144, 480, 130, 999, 143)\n    UNION ALL\n    SELECT * FROM worldchain_deposits WHERE destination_chain_id IN (42161, 1, 137, 59144, 480, 130, 999, 143)\n    UNION ALL\n    SELECT * FROM unichain_deposits WHERE destination_chain_id IN (42161, 1, 137, 59144, 480, 130, 999, 143)\n    UNION ALL\n    SELECT * FROM hyperevm_deposits WHERE destination_chain_id IN (42161, 1, 137, 59144, 480, 130, 999, 143)\n    UNION ALL\n    SELECT * FROM monad_deposits WHERE destination_chain_id IN (42161, 1, 137, 59144, 480, 130, 999, 143)\n)\n\n-- Final SELECT with descriptive chain names and USD amounts\nSELECT\n    d.deposit_timestamp,\n    d.transaction_hash,\n    d.origin_chain_id,\n    oc.chain_name AS origin_chain_name,\n    d.destination_chain_id,\n    dc.chain_name AS destination_chain_name,\n    d.deposit_id,\n    d.depositor_address,\n    d.recipient_address,\n    d.input_token_address,\n    d.input_token_symbol,\n    d.output_token_address,\n    d.output_token_symbol,\n    d.input_amount,\n    d.output_amount,\n    -- USD price data\n    tp.price_usd AS input_token_price_usd,\n    ROUND((d.input_amount * COALESCE(tp.price_usd, 1))::NUMERIC, 2) AS input_amount_usd\nFROM all_deposits d\nLEFT JOIN chain_names oc ON d.origin_chain_id = oc.chain_id\nLEFT JOIN chain_names dc ON d.destination_chain_id = dc.chain_id\n-- Join for input token price at deposit hour\nLEFT JOIN token_prices tp\n    ON d.input_token_symbol = tp.token_symbol\n    AND DATE_TRUNC('hour', d.deposit_timestamp) = tp.price_hour", "relation_name": "\"across_analytics\".\"dbt_intermediate\".\"int_unified_deposits\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2026-01-05T13:09:40.811804Z", "completed_at": "2026-01-05T13:09:40.819080Z"}, {"name": "execute", "started_at": "2026-01-05T13:09:40.826558Z", "completed_at": "2026-01-05T13:09:41.175373Z"}], "thread_id": "Thread-4 (worker)", "execution_time": 0.3777921199798584, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.stg_worldchain__refunds", "compiled": true, "compiled_code": "-- Staging model for ExecutedRelayerRefundRoot events from Worldchain\r\n\r\nWITH raw_refunds AS (\r\n\r\n    SELECT\r\n        -- Core event metadata - these identify WHEN and WHERE the event happened\r\n        timestamp_datetime,              -- When the event occurred (from blockchain)\r\n        transactionHash,                 -- Unique transaction identifier\r\n        blockchain,                      -- Which blockchain (should be 'worldchain')\r\n        source_file,                     -- Which source file this came from (for lineage)\r\n        \r\n        -- Indexed fields from topics (these are searchable/filterable in blockchain)\r\n        -- Topics are like \"indexed columns\" in SQL - they're stored separately for fast queries\r\n        topic_chain_id,                  -- Chain ID where the refund was executed\r\n        topic_root_bundle_id,            -- Merkle root bundle identifier (groups multiple refunds)\r\n        topic_leaf_id,                   -- Leaf index in the merkle tree (identifies this specific refund batch)\r\n        \r\n        -- Non-indexed fields from the event's data field\r\n        -- These are the actual business data about the refund\r\n        amount_to_return,                -- Total amount of capital returned in this batch\r\n        l2_token_address,                -- Token address being refunded (on Worldchain)\r\n        refund_amounts,                  -- Comma-separated list of individual refund amounts\r\n        refund_addresses,                -- Comma-separated list of relayer addresses receiving refunds\r\n        refund_count                     -- Number of relayers being refunded in this batch\r\n        \r\n    FROM raw.worldchain_logs_processed\r\n    \r\n    -- Filter: Only include rows where ExecutedRelayerRefundRoot data exists\r\n    -- Topic_0 is the event signature hash that identifies the event type\r\n    WHERE topic_0 = '0xf4ad92585b1bc117fbdd644990adf0827bc4c95baeae8a23322af807b6d0020e'\r\n        AND amount_to_return IS NOT NULL\r\n        AND l2_token_address IS NOT NULL\r\n        AND refund_count IS NOT NULL\r\n),\r\n\r\ncleaned_refunds AS (\r\n    SELECT\r\n        \r\n        -- Timestamp: Convert text to proper timestamp type\r\n        CASE \r\n            WHEN timestamp_datetime ~ '^\\d+$' THEN          -- If it's pure digits, treat as Unix timestamp (seconds since 1970-01-01)\r\n                TO_TIMESTAMP(timestamp_datetime::BIGINT)    -- PostgreSQL's TO_TIMESTAMP converts Unix seconds to timestamp\r\n            ELSE \r\n                timestamp_datetime::TIMESTAMP               -- Otherwise, try to parse as ISO format string (e.g., \"2025-12-03 10:30:00\")\r\n        END AS refund_timestamp,\r\n        \r\n        transactionHash AS transaction_hash,\r\n        blockchain,\r\n        source_file,\r\n        \r\n        -- ============================================================\r\n        -- INDEXED FIELDS (from topics) - Already decoded by ETL\r\n        -- ============================================================\r\n        -- These come from the event's \"topics\" array, which is indexed for fast blockchain queries\r\n        \r\n        -- Chain ID: Which blockchain this refund was executed on\r\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\r\n        -- Example: 42161 = Arbitrum, 1 = Ethereum, 480 = Worldchain\r\n        -- Cast via NUMERIC first to handle decimal strings like \"42161.0\" from ETL\r\n        (topic_chain_id::NUMERIC)::BIGINT AS chain_id,\r\n        \r\n        -- Root bundle ID: Groups multiple refunds together in a merkle tree\r\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\r\n        -- Multiple refund batches can be grouped into one merkle root for gas efficiency\r\n        -- Cast via NUMERIC first to handle decimal strings like \"12345.0\" from ETL\r\n        (topic_root_bundle_id::NUMERIC)::BIGINT AS root_bundle_id,\r\n        \r\n        -- Leaf ID: Identifies this specific refund batch within the merkle tree\r\n        -- Already converted by ETL from hex to integer (stored as text in raw table)\r\n        -- Each leaf represents one batch of refunds to multiple relayers\r\n        -- Cast via NUMERIC first to handle decimal strings like \"789.0\" from ETL\r\n        (topic_leaf_id::NUMERIC)::BIGINT AS leaf_id,\r\n        \r\n        -- ============================================================\r\n        -- TOKEN INFORMATION (what was refunded)\r\n        -- ============================================================\r\n        \r\n        -- L2 token address: The token being refunded on Worldchain\r\n        -- Already decoded by ETL to proper address format (0x...)\r\n        -- This is the token address on the Worldchain\r\n        l2_token_address AS refund_token_address,\r\n        \r\n        -- ============================================================\r\n        -- AMOUNT INFORMATION (how much was refunded)\r\n        -- ============================================================\r\n        \r\n        -- Amount to return: Total capital returned in this batch\r\n        -- Already converted by ETL from hex to numeric (stored as text in raw table)\r\n        -- This is the sum of all individual refund amounts in the batch\r\n        -- Stored as NUMERIC to handle large token amounts (18 decimals)\r\n        amount_to_return::NUMERIC AS total_refund_amount,\r\n        \r\n        -- Refund amounts: Individual refund amounts as comma-separated string\r\n        -- Format: \"1000000000000000000,2000000000000000000,3000000000000000000\"\r\n        -- Each value is in token's base units (e.g., wei for ETH, 6 decimals for USDC)\r\n        -- This is a comma-separated string because the ETL processes dynamic arrays\r\n        refund_amounts AS refund_amounts_string,\r\n        \r\n        -- Refund addresses: Relayer addresses receiving refunds as comma-separated string\r\n        -- Format: \"0xAAA...,0xBBB...,0xCCC...\"\r\n        -- Matches 1:1 with refund_amounts (first address gets first amount, etc.)\r\n        -- This is a comma-separated string because the ETL processes dynamic arrays\r\n        refund_addresses AS refund_addresses_string,\r\n        \r\n        -- Refund count: Number of relayers being refunded in this batch\r\n        -- Already converted by ETL to integer\r\n        -- This tells us how many individual refunds are in the arrays above\r\n        refund_count::INTEGER AS refund_count\r\n        \r\n    FROM raw_refunds\r\n)\r\n\r\n-- Final SELECT: Add any computed fields and ensure data quality\r\nSELECT\r\n    -- Event identity\r\n    refund_timestamp,\r\n    transaction_hash,\r\n    blockchain,\r\n    source_file,\r\n    \r\n    -- Indexed fields (merkle tree identifiers)\r\n    chain_id,\r\n    root_bundle_id,\r\n    leaf_id,\r\n    \r\n    -- Token info\r\n    refund_token_address,\r\n    \r\n    -- Amounts\r\n    total_refund_amount,\r\n    refund_amounts_string,\r\n    refund_addresses_string,\r\n    refund_count\r\n    \r\nFROM cleaned_refunds\r\n\r\n-- Data quality: Only include rows with essential fields populated\r\nWHERE chain_id IS NOT NULL\r\n    AND refund_token_address IS NOT NULL\r\n    AND total_refund_amount IS NOT NULL\r\n    AND refund_count IS NOT NULL", "relation_name": "\"across_analytics\".\"dbt_staging\".\"stg_worldchain__refunds\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2026-01-05T13:09:43.001015Z", "completed_at": "2026-01-05T13:09:43.104833Z"}, {"name": "execute", "started_at": "2026-01-05T13:09:43.106507Z", "completed_at": "2026-01-05T13:09:43.465011Z"}], "thread_id": "Thread-3 (worker)", "execution_time": 0.4683537483215332, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.int_unified_fills", "compiled": true, "compiled_code": "-- int_unified_fills.sql\n-- PURPOSE: Combine fills from ALL chains into ONE table\n-- WHY: Fills happen on the DESTINATION chain. We need to see all fills to match with deposits.\n\n\n\n-- Each CTE selects from a chain's staging model and adds the destination chain ID\nWITH arbitrum_fills AS (\n    SELECT \n        fill_timestamp,\n        transaction_hash,\n        origin_chain_id,\n        42161 AS destination_chain_id,  -- Fill happened ON Arbitrum\n        deposit_id,\n        relayer_address,\n        depositor_address,\n        recipient_address,\n        input_token_address,\n        output_token_address,\n        output_token_symbol,\n        input_amount,\n        output_amount,\n        repayment_chain_id,\n        gas_price_wei,\n        gas_used,\n        gas_cost_wei\n    FROM \"across_analytics\".\"dbt_staging\".\"stg_arbitrum__fills\"\n),\n\nethereum_fills AS (\n    SELECT \n        fill_timestamp,\n        transaction_hash,\n        origin_chain_id,\n        1 AS destination_chain_id,  -- Fill happened ON Ethereum\n        deposit_id,\n        relayer_address,\n        depositor_address,\n        recipient_address,\n        input_token_address,\n        output_token_address,\n        output_token_symbol,\n        input_amount,\n        output_amount,\n        repayment_chain_id,\n        gas_price_wei,\n        gas_used,\n        gas_cost_wei\n    FROM \"across_analytics\".\"dbt_staging\".\"stg_ethereum__fills\"\n),\n\npolygon_fills AS (\n    SELECT \n        fill_timestamp,\n        transaction_hash,\n        origin_chain_id,\n        137 AS destination_chain_id,  -- Fill happened ON Polygon\n        deposit_id,\n        relayer_address,\n        depositor_address,\n        recipient_address,\n        input_token_address,\n        output_token_address,\n        output_token_symbol,\n        input_amount,\n        output_amount,\n        repayment_chain_id,\n        gas_price_wei,\n        gas_used,\n        gas_cost_wei\n    FROM \"across_analytics\".\"dbt_staging\".\"stg_polygon__fills\"\n),\n\nlinea_fills AS (\n    SELECT \n        fill_timestamp,\n        transaction_hash,\n        origin_chain_id,\n        59144 AS destination_chain_id,  -- Fill happened ON Linea\n        deposit_id,\n        relayer_address,\n        depositor_address,\n        recipient_address,\n        input_token_address,\n        output_token_address,\n        output_token_symbol,\n        input_amount,\n        output_amount,\n        repayment_chain_id,\n        gas_price_wei,\n        gas_used,\n        gas_cost_wei\n    FROM \"across_analytics\".\"dbt_staging\".\"stg_linea__fills\"\n),\n\nworldchain_fills AS (\n    SELECT \n        fill_timestamp,\n        transaction_hash,\n        origin_chain_id,\n        480 AS destination_chain_id,  -- Fill happened ON WorldChain\n        deposit_id,\n        relayer_address,\n        depositor_address,\n        recipient_address,\n        input_token_address,\n        output_token_address,\n        output_token_symbol,\n        input_amount,\n        output_amount,\n        repayment_chain_id,\n        gas_price_wei,\n        gas_used,\n        gas_cost_wei\n    FROM \"across_analytics\".\"dbt_staging\".\"stg_worldchain__fills\"\n),\n\nunichain_fills AS (\n    SELECT \n        fill_timestamp,\n        transaction_hash,\n        origin_chain_id,\n        130 AS destination_chain_id,  -- Fill happened ON Unichain\n        deposit_id,\n        relayer_address,\n        depositor_address,\n        recipient_address,\n        input_token_address,\n        output_token_address,\n        output_token_symbol,\n        input_amount,\n        output_amount,\n        repayment_chain_id,\n        gas_price_wei,\n        gas_used,\n        gas_cost_wei\n    FROM \"across_analytics\".\"dbt_staging\".\"stg_unichain__fills\"\n),\n\nhyperevm_fills AS (\n    SELECT \n        fill_timestamp,\n        transaction_hash,\n        origin_chain_id,\n        999 AS destination_chain_id,  -- Fill happened ON HyperEVM\n        deposit_id,\n        relayer_address,\n        depositor_address,\n        recipient_address,\n        input_token_address,\n        output_token_address,\n        output_token_symbol,\n        input_amount,\n        output_amount,\n        repayment_chain_id,\n        gas_price_wei,\n        gas_used,\n        gas_cost_wei\n    FROM \"across_analytics\".\"dbt_staging\".\"stg_hyperevm__fills\"\n),\n\nmonad_fills AS (\n    SELECT \n        fill_timestamp,\n        transaction_hash,\n        origin_chain_id,\n        143 AS destination_chain_id,  -- Fill happened ON Monad\n        deposit_id,\n        relayer_address,\n        depositor_address,\n        recipient_address,\n        input_token_address,\n        output_token_address,\n        output_token_symbol,\n        input_amount,\n        output_amount,\n        repayment_chain_id,\n        gas_price_wei,\n        gas_used,\n        gas_cost_wei\n    FROM \"across_analytics\".\"dbt_staging\".\"stg_monad__fills\"\n),\n\n-- Supported chain IDs (chains we have parquet data for)\n-- 42161=Arbitrum, 1=Ethereum, 137=Polygon, 59144=Linea, 480=Worldchain, 130=Unichain, 999=HyperEVM, 143=Monad\n\n-- Chain ID to Name mapping for chains with parquet data\nchain_names AS (\n    SELECT chain_id, chain_name\n    FROM (\n        VALUES\n        (1, 'Ethereum'), (42161, 'Arbitrum'), (137, 'Polygon'),\n        (59144, 'Linea'), (480, 'Worldchain'), (130, 'Unichain'),\n        (999, 'HyperEVM'), (143, 'Monad')\n    ) AS chains(chain_id, chain_name)\n),\n\n-- Hourly token prices for USD conversion\ntoken_prices AS (\n    SELECT \n        token_symbol,\n        DATE_TRUNC('hour', timestamp::TIMESTAMP) AS price_hour,\n        AVG(price_usd) AS price_usd\n    FROM \"across_analytics\".\"dbt\".\"token_prices\"\n    GROUP BY token_symbol, DATE_TRUNC('hour', timestamp::TIMESTAMP)\n),\n\n-- UNION ALL: Stack all fills from all chains into one table\n-- Filter: Only include fills where origin_chain_id is a supported chain\nall_fills AS (\n    SELECT * FROM arbitrum_fills WHERE origin_chain_id IN (42161, 1, 137, 59144, 480, 130, 999, 143)\n    UNION ALL\n    SELECT * FROM ethereum_fills WHERE origin_chain_id IN (42161, 1, 137, 59144, 480, 130, 999, 143)\n    UNION ALL\n    SELECT * FROM polygon_fills WHERE origin_chain_id IN (42161, 1, 137, 59144, 480, 130, 999, 143)\n    UNION ALL\n    SELECT * FROM linea_fills WHERE origin_chain_id IN (42161, 1, 137, 59144, 480, 130, 999, 143)\n    UNION ALL\n    SELECT * FROM worldchain_fills WHERE origin_chain_id IN (42161, 1, 137, 59144, 480, 130, 999, 143)\n    UNION ALL\n    SELECT * FROM unichain_fills WHERE origin_chain_id IN (42161, 1, 137, 59144, 480, 130, 999, 143)\n    UNION ALL\n    SELECT * FROM hyperevm_fills WHERE origin_chain_id IN (42161, 1, 137, 59144, 480, 130, 999, 143)\n    UNION ALL\n    SELECT * FROM monad_fills WHERE origin_chain_id IN (42161, 1, 137, 59144, 480, 130, 999, 143)\n)\n\n-- Final SELECT with descriptive chain names and USD amounts\nSELECT\n    f.fill_timestamp,\n    f.transaction_hash,\n    f.origin_chain_id,\n    oc.chain_name AS origin_chain_name,\n    f.destination_chain_id,\n    dc.chain_name AS destination_chain_name,\n    f.deposit_id,\n    f.relayer_address,\n    f.depositor_address,\n    f.recipient_address,\n    f.input_token_address,\n    f.output_token_address,\n    f.output_token_symbol AS fill_token_symbol,  -- Token symbol for the filled amount\n    f.input_amount,\n    f.output_amount,\n    f.repayment_chain_id,\n    -- Gas data (for relayer cost analysis)\n    f.gas_price_wei,\n    f.gas_used,\n    f.gas_cost_wei,\n    -- USD price data\n    tp.price_usd AS output_token_price_usd,\n    ROUND((f.output_amount * COALESCE(tp.price_usd, 1))::NUMERIC, 2) AS output_amount_usd\nFROM all_fills f\nLEFT JOIN chain_names oc ON f.origin_chain_id = oc.chain_id\nLEFT JOIN chain_names dc ON f.destination_chain_id = dc.chain_id\n-- Join for output token price at fill hour\nLEFT JOIN token_prices tp\n    ON f.output_token_symbol = tp.token_symbol\n    AND DATE_TRUNC('hour', f.fill_timestamp) = tp.price_hour", "relation_name": "\"across_analytics\".\"dbt_intermediate\".\"int_unified_fills\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2026-01-05T13:09:43.030629Z", "completed_at": "2026-01-05T13:09:43.119805Z"}, {"name": "execute", "started_at": "2026-01-05T13:09:43.121760Z", "completed_at": "2026-01-05T13:09:43.630974Z"}], "thread_id": "Thread-2 (worker)", "execution_time": 0.6047670841217041, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.int_unified_refunds", "compiled": true, "compiled_code": "-- int_unified_refunds.sql\n-- PURPOSE: Combine refunds from ALL chains into ONE table with converted amounts\n-- WHY: Refunds = capital returning to relayers. Tracks when relayers get paid back.\n\n\n\n-- Token metadata for amount conversion\nWITH token_meta AS (\n    \n    SELECT \n        LOWER(token_address) AS token_address,\n        token_symbol,\n        decimals,\n        chain_id\n    FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n\n),\n\n-- Chain ID to Name mapping for chains with parquet data\nchain_names AS (\n    SELECT chain_id, chain_name\n    FROM (\n        VALUES\n        (1, 'Ethereum'), (42161, 'Arbitrum'), (137, 'Polygon'),\n        (59144, 'Linea'), (480, 'Worldchain'), (130, 'Unichain'),\n        (999, 'HyperEVM'), (143, 'Monad')\n    ) AS chains(chain_id, chain_name)\n),\n\n-- Each CTE selects from a chain's staging model\narbitrum_refunds AS (\n    SELECT \n        refund_timestamp,\n        transaction_hash,\n        chain_id,\n        root_bundle_id,\n        leaf_id,\n        LOWER(refund_token_address) AS refund_token_address,\n        total_refund_amount AS total_refund_amount_raw,\n        refund_addresses_string,\n        refund_amounts_string,\n        refund_count,\n        'arbitrum' AS source_blockchain\n    FROM \"across_analytics\".\"dbt_staging\".\"stg_arbitrum__refunds\"\n),\n\nethereum_refunds AS (\n    SELECT \n        refund_timestamp,\n        transaction_hash,\n        chain_id,\n        root_bundle_id,\n        leaf_id,\n        LOWER(refund_token_address) AS refund_token_address,\n        total_refund_amount AS total_refund_amount_raw,\n        refund_addresses_string,\n        refund_amounts_string,\n        refund_count,\n        'ethereum' AS source_blockchain\n    FROM \"across_analytics\".\"dbt_staging\".\"stg_ethereum__refunds\"\n),\n\npolygon_refunds AS (\n    SELECT \n        refund_timestamp,\n        transaction_hash,\n        chain_id,\n        root_bundle_id,\n        leaf_id,\n        LOWER(refund_token_address) AS refund_token_address,\n        total_refund_amount AS total_refund_amount_raw,\n        refund_addresses_string,\n        refund_amounts_string,\n        refund_count,\n        'polygon' AS source_blockchain\n    FROM \"across_analytics\".\"dbt_staging\".\"stg_polygon__refunds\"\n),\n\nlinea_refunds AS (\n    SELECT \n        refund_timestamp,\n        transaction_hash,\n        chain_id,\n        root_bundle_id,\n        leaf_id,\n        LOWER(refund_token_address) AS refund_token_address,\n        total_refund_amount AS total_refund_amount_raw,\n        refund_addresses_string,\n        refund_amounts_string,\n        refund_count,\n        'linea' AS source_blockchain\n    FROM \"across_analytics\".\"dbt_staging\".\"stg_linea__refunds\"\n),\n\nworldchain_refunds AS (\n    SELECT \n        refund_timestamp,\n        transaction_hash,\n        chain_id,\n        root_bundle_id,\n        leaf_id,\n        LOWER(refund_token_address) AS refund_token_address,\n        total_refund_amount AS total_refund_amount_raw,\n        refund_addresses_string,\n        refund_amounts_string,\n        refund_count,\n        'worldchain' AS source_blockchain\n    FROM \"across_analytics\".\"dbt_staging\".\"stg_worldchain__refunds\"\n),\n\nunichain_refunds AS (\n    SELECT \n        refund_timestamp,\n        transaction_hash,\n        chain_id,\n        root_bundle_id,\n        leaf_id,\n        LOWER(refund_token_address) AS refund_token_address,\n        total_refund_amount AS total_refund_amount_raw,\n        refund_addresses_string,\n        refund_amounts_string,\n        refund_count,\n        'unichain' AS source_blockchain\n    FROM \"across_analytics\".\"dbt_staging\".\"stg_unichain__refunds\"\n),\n\nhyperevm_refunds AS (\n    SELECT \n        refund_timestamp,\n        transaction_hash,\n        chain_id,\n        root_bundle_id,\n        leaf_id,\n        LOWER(refund_token_address) AS refund_token_address,\n        total_refund_amount AS total_refund_amount_raw,\n        refund_addresses_string,\n        refund_amounts_string,\n        refund_count,\n        'hyperevm' AS source_blockchain\n    FROM \"across_analytics\".\"dbt_staging\".\"stg_hyperevm__refunds\"\n),\n\nmonad_refunds AS (\n    SELECT \n        refund_timestamp,\n        transaction_hash,\n        chain_id,\n        root_bundle_id,\n        leaf_id,\n        LOWER(refund_token_address) AS refund_token_address,\n        total_refund_amount AS total_refund_amount_raw,\n        refund_addresses_string,\n        refund_amounts_string,\n        refund_count,\n        'monad' AS source_blockchain\n    FROM \"across_analytics\".\"dbt_staging\".\"stg_monad__refunds\"\n),\n\n-- UNION ALL: Stack all refunds from all chains\nall_refunds AS (\n    SELECT * FROM arbitrum_refunds\n    UNION ALL\n    SELECT * FROM ethereum_refunds\n    UNION ALL\n    SELECT * FROM polygon_refunds\n    UNION ALL\n    SELECT * FROM linea_refunds\n    UNION ALL\n    SELECT * FROM worldchain_refunds\n    UNION ALL\n    SELECT * FROM unichain_refunds\n    UNION ALL\n    SELECT * FROM hyperevm_refunds\n    UNION ALL\n    SELECT * FROM monad_refunds\n)\n\n-- Final SELECT: Join with token metadata and convert amounts\nSELECT\n    r.refund_timestamp,\n    r.transaction_hash,\n    r.chain_id,\n    cn.chain_name,\n    r.root_bundle_id,\n    r.leaf_id,\n    r.refund_token_address,\n    tok.token_symbol AS refund_token_symbol,\n    tok.decimals AS token_decimals,\n    -- Rescaled amount (human-readable)\n    \n    r.total_refund_amount_raw / POWER(10, COALESCE(tok.decimals, 18))\n AS total_refund_amount,\n    -- Raw amount (preserved for auditing)\n    r.total_refund_amount_raw,\n    r.refund_addresses_string,\n    r.refund_amounts_string,\n    r.refund_count,\n    r.source_blockchain\n\nFROM all_refunds r\n\n-- Join for chain name\nLEFT JOIN chain_names cn ON r.chain_id = cn.chain_id\n\n-- Join with token metadata on address + chain_id\nLEFT JOIN token_meta AS tok\n    ON r.refund_token_address = tok.token_address\n    AND r.chain_id = tok.chain_id", "relation_name": "\"across_analytics\".\"dbt_intermediate\".\"int_unified_refunds\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2026-01-05T13:09:43.506270Z", "completed_at": "2026-01-05T13:09:43.523707Z"}, {"name": "execute", "started_at": "2026-01-05T13:09:43.525076Z", "completed_at": "2026-01-05T13:09:43.850788Z"}], "thread_id": "Thread-1 (worker)", "execution_time": 0.35027003288269043, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.int_deposit_fill_matching", "compiled": true, "compiled_code": "-- int_deposit_fill_matching.sql\n-- PURPOSE: Match every deposit to its fill (if it exists)\n-- WHY: This is the CORE model - connects money leaving one chain to money arriving on another.\n-- \n-- HOW IT WORKS:\n-- 1. User deposits on Chain A \u2192 creates deposit_id\n-- 2. Relayer fills on Chain B \u2192 same deposit_id\n-- 3. We JOIN on deposit_id + origin/destination chain match\n-- 4. Unfilled deposits = rows with NULL fill (stuck capital)\n\n\n\n-- Chain ID to Name mapping for chains with parquet data\nWITH chain_names AS (\n    SELECT \n        chain_id,\n        chain_name\n    FROM (\n        VALUES\n        -- Only chains we have parquet data for:\n        (1, 'Ethereum'),\n        (42161, 'Arbitrum'),\n        (137, 'Polygon'),\n        (59144, 'Linea'),\n        (480, 'Worldchain'),\n        (130, 'Unichain'),\n        (999, 'HyperEVM'),\n        (143, 'Monad')\n    ) AS chains(chain_id, chain_name)\n),\n\n-- Token metadata for symbol lookups\ntoken_metadata AS (\n    SELECT * FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n),\n\n-- Hourly token prices for USD conversion\n-- Truncate to hour for matching with transaction timestamps\ntoken_prices AS (\n    SELECT \n        token_symbol,\n        DATE_TRUNC('hour', timestamp::TIMESTAMP) AS price_hour,\n        AVG(price_usd) AS price_usd  -- Average in case of duplicate hours\n    FROM \"across_analytics\".\"dbt\".\"token_prices\"\n    GROUP BY token_symbol, DATE_TRUNC('hour', timestamp::TIMESTAMP)\n),\n\n-- Native token symbol per chain (for gas cost USD conversion)\n-- Gas fees are paid in the chain's native token\nnative_tokens AS (\n    SELECT chain_id, native_token_symbol\n    FROM (\n        VALUES\n        (1, 'WETH'),       -- Ethereum: ETH (use WETH price)\n        (42161, 'WETH'),   -- Arbitrum: ETH\n        (137, 'MATIC'),    -- Polygon: MATIC\n        (59144, 'WETH'),   -- Linea: ETH\n        (480, 'WETH'),     -- Worldchain: ETH (WLD for app, but gas in ETH)\n        (130, 'WETH'),     -- Unichain: ETH\n        (999, 'WETH'),     -- HyperEVM: ETH\n        (143, 'MON')       -- Monad: MON (native token, not ETH!)\n    ) AS nt(chain_id, native_token_symbol)\n),\n\ndeposits AS (\n    SELECT * FROM \"across_analytics\".\"dbt_intermediate\".\"int_unified_deposits\"\n),\n\nfills AS (\n    SELECT * FROM \"across_analytics\".\"dbt_intermediate\".\"int_unified_fills\"\n),\n\n-- JOIN deposits to fills on deposit_id + chain matching\nmatched AS (\n    SELECT\n        -- === DEPOSIT INFO (Origin side) ===\n        d.deposit_timestamp,\n        d.transaction_hash AS deposit_tx_hash,\n        d.origin_chain_id,\n        d.destination_chain_id,\n        d.deposit_id,\n        d.depositor_address,\n        d.recipient_address AS deposit_recipient,\n        d.input_token_address AS deposit_token,\n        d.input_amount AS deposit_amount,\n        d.output_amount AS expected_output_amount,\n        \n        -- === FILL INFO (Destination side) ===\n        f.fill_timestamp,\n        f.transaction_hash AS fill_tx_hash,\n        f.relayer_address,\n        f.output_token_address AS fill_token,\n        f.output_amount AS actual_output_amount,\n        f.repayment_chain_id,\n        \n        -- === GAS DATA (for relayer cost analysis) ===\n        f.gas_price_wei,\n        f.gas_used,\n        f.gas_cost_wei,\n        \n        -- === COMPUTED FIELDS ===\n        -- Fill latency: How long did it take to fill? (in seconds)\n        -- Use GREATEST(0, ...) to handle cross-chain timestamp sync issues\n        GREATEST(0, EXTRACT(EPOCH FROM (f.fill_timestamp - d.deposit_timestamp))) AS fill_latency_seconds,\n        \n        -- Is this deposit filled?\n        CASE WHEN f.deposit_id IS NOT NULL THEN TRUE ELSE FALSE END AS is_filled,\n        \n        -- Fee: difference between deposited amount and filled amount\n        CASE \n            WHEN f.output_amount IS NOT NULL\n            THEN ROUND((d.input_amount - f.output_amount)::NUMERIC, 2)\n            ELSE NULL \n        END AS bridge_fee_nominal,\n        \n        CASE \n            WHEN f.output_amount IS NOT NULL\n            THEN ROUND(((d.input_amount - f.output_amount) / d.input_amount * 100)::NUMERIC, 2)\n            ELSE NULL \n        END AS bridge_fee_percent,\n        \n        -- Slippage: Difference between expected and actual output\n        CASE \n            WHEN f.output_amount IS NOT NULL AND d.output_amount > 0 \n            THEN ROUND(((d.output_amount - f.output_amount) / d.output_amount * 100)::NUMERIC, 2)\n            ELSE NULL \n        END AS slippage_percent\n\n    FROM deposits d\n    \n    -- LEFT JOIN: Keep ALL deposits, even unfilled ones\n    LEFT JOIN fills f \n        ON d.deposit_id = f.deposit_id\n        AND d.origin_chain_id = f.origin_chain_id  -- Must match origin\n        AND d.destination_chain_id = f.destination_chain_id  -- Must match destination\n)\n\nSELECT\n    -- Identity\n    m.deposit_timestamp,\n    m.deposit_tx_hash,\n    m.origin_chain_id,\n    oc.chain_name AS origin_chain_name,\n    m.destination_chain_id,\n    dc.chain_name AS destination_chain_name,\n    m.deposit_id,\n    \n    -- Deposit details\n    m.depositor_address,\n    m.deposit_recipient,\n    m.deposit_token,\n    dt.token_symbol AS deposit_token_symbol,\n    m.deposit_amount,\n    m.expected_output_amount,\n    \n    -- USD-normalized amounts (joined from hourly price data)\n    dp.price_usd AS deposit_token_price_usd,\n    ROUND((m.deposit_amount * COALESCE(dp.price_usd, 1))::NUMERIC, 2) AS deposit_amount_usd,\n    \n    -- Fill details (NULL if unfilled)\n    m.fill_timestamp,\n    m.fill_tx_hash,\n    m.relayer_address,\n    m.fill_token,\n    ft.token_symbol AS fill_token_symbol,\n    m.actual_output_amount,\n    m.repayment_chain_id,\n    \n    -- USD-normalized fill amount\n    fp.price_usd AS fill_token_price_usd,\n    CASE \n        WHEN m.actual_output_amount IS NOT NULL \n        THEN ROUND((m.actual_output_amount * COALESCE(fp.price_usd, 1))::NUMERIC, 2)\n        ELSE NULL \n    END AS fill_amount_usd,\n    \n    -- Metrics\n    m.fill_latency_seconds,\n    m.is_filled,\n    m.bridge_fee_nominal,\n    m.bridge_fee_percent,\n    m.slippage_percent,\n    \n    -- Gas data (for relayer cost analysis)\n    -- These represent the cost incurred by the relayer on the destination chain\n    -- NOTE: Different chains use different native tokens (ETH, MATIC, WLD, etc.)\n    m.gas_price_wei,\n    m.gas_used,\n    m.gas_cost_wei,\n    -- Convert gas cost to native token units for readability (wei / 10^18)\n    ROUND((m.gas_cost_wei / 1e18)::NUMERIC, 8) AS gas_cost_native,\n    -- Gas cost in USD (using native token price at fill hour)\n    CASE \n        WHEN m.gas_cost_wei IS NOT NULL \n        THEN ROUND(((m.gas_cost_wei / 1e18) * COALESCE(np.price_usd, 0))::NUMERIC, 2)\n        ELSE NULL \n    END AS gas_cost_usd,\n    \n    -- Bridge fee in USD (fee * deposit token price)\n    CASE \n        WHEN m.bridge_fee_nominal IS NOT NULL \n        THEN ROUND((m.bridge_fee_nominal * COALESCE(dp.price_usd, 1))::NUMERIC, 2)\n        ELSE NULL \n    END AS bridge_fee_nominal_usd,\n    \n    -- Route identifier (for aggregations)\n    oc.chain_name || ' \u2192 ' || dc.chain_name AS route_name,\n    m.origin_chain_id || '_' || m.destination_chain_id AS route_id,\n    \n    -- ========================================================================\n    -- LATENCY TIER (Fill-Level Speed Classification)\n    -- ========================================================================\n    -- WHAT: Classifies EACH individual fill's speed as CRITICAL / SLOW / MODERATE / FAST\n    -- WHY:  Enables drill-down analysis of specific slow transactions.\n    --       Complements the route-level liquidity_gap_status in mart_fill_latency_analysis.\n    --\n    -- DATA-DRIVEN THRESHOLDS (based on raw fill distribution from dataset):\n    --   Distribution: median=8s, p75=15s, p95=42s\n    --\n    --   - CRITICAL: > 100s \u2192 2.5x slower than global p95, severe delay\n    --   - SLOW:     > 42s  \u2192 Exceeds global p95, investigate\n    --   - MODERATE: > 15s  \u2192 Exceeds global p75, slower than typical\n    --   - FAST:     \u2264 15s  \u2192 At or below p75, good UX\n    --\n    -- USE:  Incident investigation, outlier analysis, user experience audit.\n    -- ========================================================================\n    CASE \n        WHEN m.is_filled = FALSE THEN NULL  -- Unfilled deposits have no latency tier\n        WHEN m.fill_latency_seconds > 100 THEN 'CRITICAL'\n        WHEN m.fill_latency_seconds > 42 THEN 'SLOW'\n        WHEN m.fill_latency_seconds > 15 THEN 'MODERATE'\n        ELSE 'FAST'\n    END AS latency_tier\n\nFROM matched m\n\n-- Join for origin chain name\nLEFT JOIN chain_names oc ON m.origin_chain_id = oc.chain_id\n\n-- Join for destination chain name  \nLEFT JOIN chain_names dc ON m.destination_chain_id = dc.chain_id\n\n-- Join for deposit token symbol (origin chain token)\nLEFT JOIN token_metadata dt \n    ON m.origin_chain_id = dt.chain_id \n    AND LOWER(m.deposit_token) = LOWER(dt.token_address)\n\n-- Join for fill token symbol (destination chain token)\nLEFT JOIN token_metadata ft \n    ON m.destination_chain_id = ft.chain_id \n    AND LOWER(m.fill_token) = LOWER(ft.token_address)\n\n-- Join for deposit token price at deposit hour\nLEFT JOIN token_prices dp\n    ON dt.token_symbol = dp.token_symbol\n    AND DATE_TRUNC('hour', m.deposit_timestamp) = dp.price_hour\n\n-- Join for fill token price at fill hour\nLEFT JOIN token_prices fp\n    ON ft.token_symbol = fp.token_symbol\n    AND DATE_TRUNC('hour', m.fill_timestamp) = fp.price_hour\n\n-- Join for native token symbol on destination chain (for gas cost USD)\nLEFT JOIN native_tokens nt ON m.destination_chain_id = nt.chain_id\n\n-- Join for native token price at fill hour (for gas cost USD)\nLEFT JOIN token_prices np\n    ON nt.native_token_symbol = np.token_symbol\n    AND DATE_TRUNC('hour', m.fill_timestamp) = np.price_hour", "relation_name": "\"across_analytics\".\"dbt_intermediate\".\"int_deposit_fill_matching\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2026-01-05T13:09:43.651816Z", "completed_at": "2026-01-05T13:09:43.673222Z"}, {"name": "execute", "started_at": "2026-01-05T13:09:43.677766Z", "completed_at": "2026-01-05T13:09:44.178136Z"}], "thread_id": "Thread-4 (worker)", "execution_time": 0.531336784362793, "adapter_response": {"_message": "CREATE VIEW", "code": "CREATE VIEW", "rows_affected": -1}, "message": "CREATE VIEW", "failures": null, "unique_id": "model.across_analytics.int_unified_refunds_expanded", "compiled": true, "compiled_code": "-- int_refunds_expanded.sql\n-- CHALLENGE: some refund_amounts_string are arrays that have to be unnested\n-- PURPOSE: Expand comma-separated refund arrays into individual rows\n-- WHY: Each row = one relayer receiving one refund amount\n-- ENABLES: Easy aggregation, filtering by relayer, joins with relayer metrics\n\n\n\nWITH unified AS (\n    -- Get all refunds from all chains (already unified in int_unified_refunds)\n    SELECT * \n    FROM \"across_analytics\".\"dbt_intermediate\".\"int_unified_refunds\"\n    WHERE refund_count > 0  -- Only process batches with actual refunds\n),\n\n-- Hourly token prices for USD conversion\ntoken_prices AS (\n    SELECT \n        token_symbol,\n        DATE_TRUNC('hour', timestamp::TIMESTAMP) AS price_hour,\n        AVG(price_usd) AS price_usd\n    FROM \"across_analytics\".\"dbt\".\"token_prices\"\n    GROUP BY token_symbol, DATE_TRUNC('hour', timestamp::TIMESTAMP)\n),\n\n-- Unnest the comma-separated strings into individual rows\n-- Uses CROSS JOIN LATERAL with UNNEST to expand arrays\n-- WITH ORDINALITY gives us the position index for matching amounts to addresses\nexpanded AS (\n    SELECT\n        -- Batch-level identifiers (same for all rows from same batch)\n        refund_timestamp,\n        transaction_hash,\n        chain_id,\n        chain_name,\n        root_bundle_id,\n        leaf_id,\n        refund_token_address,\n        refund_token_symbol,\n        token_decimals,\n        total_refund_amount,\n        total_refund_amount_raw,\n        refund_count,\n        source_blockchain,\n        \n        -- Individual refund data (one row per relayer/amount pair)\n        -- TRIM handles any whitespace that might exist in the CSV-like strings\n        TRIM(amounts.amount)::NUMERIC AS refund_amount_raw,\n        TRIM(addresses.address) AS relayer_address,\n        amounts.idx AS refund_index\n        \n    FROM unified\n    -- Expand refund_amounts_string: \"100,200,300\" \u2192 3 rows with values 100, 200, 300\n    CROSS JOIN LATERAL UNNEST(\n        string_to_array(refund_amounts_string, ',')\n    ) WITH ORDINALITY AS amounts(amount, idx)\n    -- Expand refund_addresses_string: \"0xAAA,0xBBB,0xCCC\" \u2192 3 rows with addresses\n    CROSS JOIN LATERAL UNNEST(\n        string_to_array(refund_addresses_string, ',')\n    ) WITH ORDINALITY AS addresses(address, idx)\n    -- Match by position: 1st amount goes to 1st address, 2nd to 2nd, etc.\n    WHERE amounts.idx = addresses.idx\n)\n\nSELECT\n    -- Event identification\n    e.refund_timestamp,\n    e.transaction_hash,\n    e.source_blockchain,\n    \n    -- Batch identifiers (for grouping back if needed)\n    e.chain_id,\n    e.chain_name,\n    e.root_bundle_id,\n    e.leaf_id,\n    \n    -- Individual refund details\n    e.refund_index,\n    e.relayer_address,\n    e.refund_token_address,\n    e.refund_token_symbol,\n    \n    -- Individual refund amount (rescaled using token decimals)\n    \n    e.refund_amount_raw / POWER(10, COALESCE(e.token_decimals, 18))\n AS refund_amount,\n    e.refund_amount_raw,\n    \n    -- USD price data\n    tp.price_usd AS refund_token_price_usd,\n    ROUND((\n    e.refund_amount_raw / POWER(10, COALESCE(e.token_decimals, 18))\n * COALESCE(tp.price_usd, 1))::NUMERIC, 2) AS refund_amount_usd,\n    \n    -- Batch context (useful for analysis)\n    e.total_refund_amount AS batch_total_amount,\n    e.total_refund_amount_raw AS batch_total_amount_raw,\n    e.refund_count AS batch_refund_count,\n    \n    -- Unique identifier for each individual refund record\n    e.transaction_hash || '-' || e.leaf_id || '-' || e.refund_index AS refund_id\n    \nFROM expanded e\n-- Join for refund token price at refund hour\nLEFT JOIN token_prices tp\n    ON e.refund_token_symbol = tp.token_symbol\n    AND DATE_TRUNC('hour', e.refund_timestamp) = tp.price_hour", "relation_name": "\"across_analytics\".\"dbt_intermediate\".\"int_unified_refunds_expanded\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2026-01-05T13:09:43.925070Z", "completed_at": "2026-01-05T13:09:43.946786Z"}, {"name": "execute", "started_at": "2026-01-05T13:09:43.973077Z", "completed_at": "2026-01-05T13:09:51.530845Z"}], "thread_id": "Thread-2 (worker)", "execution_time": 7.649380683898926, "adapter_response": {"_message": "SELECT 3676", "code": "SELECT", "rows_affected": 3676}, "message": "SELECT 3676", "failures": null, "unique_id": "model.across_analytics.mart_fill_latency_analysis", "compiled": true, "compiled_code": "-- ============================================================================\n-- mart_fill_latency_analysis.sql\n-- ============================================================================\n-- PURPOSE: Analyze Time-to-Fill (TTF) to identify filler hesitation and \n--          liquidity gaps across routes\n-- \n-- GRAIN: Minute \u00d7 Route \u00d7 Token\n-- \n-- KEY BUSINESS QUESTIONS:\n--   1. Which routes have \"Filler Hesitation\"? \u2192 slow_fill_pct > 10%\n--   2. Is a new chain's filler infrastructure mature? \u2192 Compare p95_ttf\n--   3. Where should we prioritize filler incentives? \u2192 liquidity_gap_status = 'HIGH'\n-- ============================================================================\n\n\n\nWITH base_data AS (\n    SELECT\n        *,\n        DATE_TRUNC('minute', deposit_timestamp) AS deposit_minute\n    FROM \"across_analytics\".\"dbt_intermediate\".\"int_deposit_fill_matching\"\n),\n\n-- Chain name mapping\nchain_names AS (\n    SELECT \n        chain_id,\n        chain_name\n    FROM (VALUES\n        (1, 'Ethereum'),\n        (42161, 'Arbitrum'),\n        (137, 'Polygon'),\n        (59144, 'Linea'),\n        (480, 'Worldchain'),\n        (130, 'Unichain'),\n        (999, 'HyperEVM'),\n        (143, 'Monad')\n    ) AS chains(chain_id, chain_name)\n),\n\n-- Token metadata for symbol lookup\ntoken_metadata AS (\n    SELECT * FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n),\n\n-- Aggregate by hour, route, and token\nhourly_latency_metrics AS (\n    SELECT\n        deposit_minute,\n        origin_chain_id,\n        destination_chain_id,\n        route_id,\n        deposit_token,\n        \n        -- === VOLUME METRICS ===\n        COUNT(*) AS total_deposits,\n        SUM(CASE WHEN is_filled THEN 1 ELSE 0 END) AS total_fills,\n        SUM(CASE WHEN NOT is_filled THEN 1 ELSE 0 END) AS unfilled_count,\n        \n        -- Fill rate\n        ROUND(\n            (SUM(CASE WHEN is_filled THEN 1 ELSE 0 END)::NUMERIC / NULLIF(COUNT(*), 0)) * 100, \n            2\n        ) AS fill_rate_pct,\n        \n        -- === LATENCY DISTRIBUTION ===\n        -- Core TTF metrics (only for filled deposits)\n        ROUND(AVG(CASE WHEN is_filled THEN fill_latency_seconds END)::NUMERIC, 2) AS avg_ttf_seconds,\n        ROUND(\n            PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY fill_latency_seconds) \n            FILTER (WHERE is_filled)::NUMERIC, \n            2\n        ) AS median_ttf_seconds,\n        ROUND(\n            PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY fill_latency_seconds) \n            FILTER (WHERE is_filled)::NUMERIC, \n            2\n        ) AS p95_ttf_seconds,\n        MIN(CASE WHEN is_filled THEN fill_latency_seconds END) AS min_ttf_seconds,\n        MAX(CASE WHEN is_filled THEN fill_latency_seconds END) AS max_ttf_seconds,\n        \n        -- === SPEED BUCKET COUNTS ===\n        -- Instant fills (\u226430 seconds) - \"Lightning fast\"\n        SUM(CASE WHEN is_filled AND fill_latency_seconds <= 30 THEN 1 ELSE 0 END) AS instant_fill_count,\n        \n        -- Fast fills (\u226460 seconds)\n        SUM(CASE WHEN is_filled AND fill_latency_seconds <= 60 THEN 1 ELSE 0 END) AS fast_fill_count,\n        \n        -- Slow fills (>300 seconds / 5 minutes) - \"Filler Hesitation\"\n        SUM(CASE WHEN is_filled AND fill_latency_seconds > 300 THEN 1 ELSE 0 END) AS slow_fill_count,\n        \n        -- Very slow fills (>900 seconds / 15 minutes) - \"Critical Delay\"\n        SUM(CASE WHEN is_filled AND fill_latency_seconds > 900 THEN 1 ELSE 0 END) AS very_slow_fill_count,\n        \n        -- === VOLUME CONTEXT ===\n        SUM(deposit_amount_usd) AS total_deposit_volume_usd,\n        SUM(CASE WHEN is_filled THEN fill_amount_usd ELSE 0 END) AS total_filled_volume_usd,\n        \n        -- === PARTICIPANT METRICS ===\n        COUNT(DISTINCT relayer_address) FILTER (WHERE is_filled) AS unique_relayers\n        \n    FROM base_data\n    GROUP BY \n        deposit_minute,\n        origin_chain_id,\n        destination_chain_id,\n        route_id,\n        deposit_token\n),\n\n-- ============================================================================\n-- COMPUTED INSIGHTS CTE\n-- ============================================================================\n-- PURPOSE: Transform raw counts into actionable percentages and categorical \n--          labels that can be used directly in Superset filters and alerts.\n-- ============================================================================\nwith_insights AS (\n    SELECT\n        *,\n        \n        -- ========================================================================\n        -- INSTANT FILL PERCENTAGE\n        -- ========================================================================\n        -- WHAT: % of fills completed in \u226430 seconds\n        -- WHY:  Measures \"lightning fast\" user experience. High % = excellent UX.\n        --       Across's competitive advantage is speed - this proves it.\n        -- USE:  Dashboard KPI, compare across routes to find speed champions.\n        -- ========================================================================\n        ROUND(\n            (instant_fill_count::NUMERIC / NULLIF(total_fills, 0)) * 100, \n            2\n        ) AS instant_fill_pct,\n        \n        -- ========================================================================\n        -- FAST FILL PERCENTAGE\n        -- ========================================================================\n        -- WHAT: % of fills completed in \u226460 seconds (includes instant fills)\n        -- WHY:  Broader \"good experience\" threshold. Users generally don't \n        --       notice delays under 1 minute. Target: >90% for healthy routes.\n        -- USE:  Route health monitoring, alerting if drops below threshold.\n        -- ========================================================================\n        ROUND(\n            (fast_fill_count::NUMERIC / NULLIF(total_fills, 0)) * 100, \n            2\n        ) AS fast_fill_pct,\n        \n        -- ========================================================================\n        -- SLOW FILL PERCENTAGE (\"Filler Hesitation\" Indicator)\n        -- ========================================================================\n        -- WHAT: % of fills taking >5 minutes (300 seconds)\n        -- WHY:  5+ minute waits indicate fillers are HESITATING to fill this route.\n        --       Causes: low liquidity, high risk perception, poor RPC, or \n        --       unfavorable economics on that destination chain.\n        -- USE:  Alert trigger (>10% = investigate), filler incentive prioritization.\n        -- ========================================================================\n        ROUND(\n            (slow_fill_count::NUMERIC / NULLIF(total_fills, 0)) * 100, \n            2\n        ) AS slow_fill_pct,\n        \n        -- ========================================================================\n        -- VERY SLOW FILL PERCENTAGE (\"Critical Delay\" Indicator)\n        -- ========================================================================\n        -- WHAT: % of fills taking >15 minutes (900 seconds)\n        -- WHY:  15+ minute waits are CRITICAL failures. Users likely abandoned\n        --       or contacted support. These routes need immediate attention.\n        -- USE:  High-priority alert, escalation to engineering team.\n        -- ========================================================================\n        ROUND(\n            (very_slow_fill_count::NUMERIC / NULLIF(total_fills, 0)) * 100, \n            2\n        ) AS very_slow_fill_pct,\n        \n        -- ========================================================================\n        -- LIQUIDITY GAP STATUS (Categorical Health Label)\n        -- ========================================================================\n        -- WHAT: Classifies each hourly observation as CRITICAL / HIGH / MODERATE / HEALTHY \n        --       based on the p95 TTF for that hour-route-token combination.\n        -- WHY:  Provides a simple filter in Superset: \"Show me all CRITICAL gap routes\"\n        --       The p95 is used (not avg) because we care about WORST-CASE experience.\n        -- \n        -- DATA-DRIVEN THRESHOLDS (based on actual distribution from dataset):\n        --   Raw fill latencies (all users): median=8s, p75=15s, p95=42s\n        --   Route-level p95s: median=10.5s, p75=24s, p95=96s\n        --\n        --   - CRITICAL: > 100s  \u2192 Route's worst 5% is 2.5x slower than global worst 5%\n        --   - HIGH:     30-100s \u2192 Route's worst 5% exceeds global p95 (42s), investigate\n        --   - MODERATE: 15-30s  \u2192 Route's worst 5% is typical to slightly slow\n        --   - HEALTHY:  < 15s   \u2192 Route's worst 5% beats global p75 (15s), excellent UX\n        --\n        -- USE:  Superset filter, executive summary, filler incentive targeting.\n        -- ========================================================================\n        CASE \n            WHEN p95_ttf_seconds > 100 THEN 'CRITICAL'\n            WHEN p95_ttf_seconds > 30 THEN 'HIGH'\n            WHEN p95_ttf_seconds > 15 THEN 'MODERATE'\n            ELSE 'HEALTHY'\n        END AS liquidity_gap_status\n        \n    FROM hourly_latency_metrics\n)\n\nSELECT\n    -- Time dimension\n    wi.deposit_minute,\n    \n    -- Route identifiers with human-readable names\n    wi.origin_chain_id,\n    oc.chain_name AS origin_chain_name,\n    wi.destination_chain_id,\n    dc.chain_name AS destination_chain_name,\n    wi.route_id,\n    oc.chain_name || ' \u2192 ' || dc.chain_name AS route_name,\n    \n    -- Token info\n    wi.deposit_token,\n    tm.token_symbol,\n    CASE \n        WHEN tm.token_symbol LIKE 'USDC%' THEN 'USDC'\n        WHEN tm.token_symbol LIKE 'USDT%' THEN 'USDT'\n        WHEN tm.token_symbol LIKE 'DAI%' THEN 'DAI'\n        WHEN tm.token_symbol LIKE 'WETH%' THEN 'WETH'\n        WHEN tm.token_symbol LIKE 'WBTC%' THEN 'WBTC'\n        ELSE tm.token_symbol\n    END AS base_token_symbol,\n    \n    -- Volume metrics\n    wi.total_deposits,\n    wi.total_fills,\n    wi.unfilled_count,\n    wi.fill_rate_pct,\n    wi.total_deposit_volume_usd,\n    wi.total_filled_volume_usd,\n    \n    -- Latency distribution\n    wi.avg_ttf_seconds,\n    wi.median_ttf_seconds,\n    wi.p95_ttf_seconds,\n    wi.min_ttf_seconds,\n    wi.max_ttf_seconds,\n    \n    -- Speed bucket counts\n    wi.instant_fill_count,\n    wi.fast_fill_count,\n    wi.slow_fill_count,\n    wi.very_slow_fill_count,\n    \n    -- Speed bucket percentages\n    wi.instant_fill_pct,\n    wi.fast_fill_pct,\n    wi.slow_fill_pct,\n    wi.very_slow_fill_pct,\n    \n    -- Computed insights\n    wi.liquidity_gap_status,\n    \n    -- Participant metrics\n    wi.unique_relayers\n\nFROM with_insights wi\nLEFT JOIN chain_names oc ON wi.origin_chain_id = oc.chain_id\nLEFT JOIN chain_names dc ON wi.destination_chain_id = dc.chain_id\nLEFT JOIN token_metadata tm ON wi.origin_chain_id = tm.chain_id \n    AND LOWER(wi.deposit_token) = LOWER(tm.token_address)\n\nORDER BY wi.deposit_minute DESC, wi.total_deposit_volume_usd DESC", "relation_name": "\"across_analytics\".\"dbt_marts\".\"mart_fill_latency_analysis\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2026-01-05T13:09:43.893979Z", "completed_at": "2026-01-05T13:09:43.948223Z"}, {"name": "execute", "started_at": "2026-01-05T13:09:43.985340Z", "completed_at": "2026-01-05T13:09:51.614002Z"}], "thread_id": "Thread-1 (worker)", "execution_time": 7.7406229972839355, "adapter_response": {"_message": "SELECT 3676", "code": "SELECT", "rows_affected": 3676}, "message": "SELECT 3676", "failures": null, "unique_id": "model.across_analytics.mart_general_overview", "compiled": true, "compiled_code": "-- mart_general_overview.sql\n-- PURPOSE: High-level protocol overview for the main Superset dashboard\n-- GRAIN: Minute \u00d7 Origin Chain \u00d7 Destination Chain \u00d7 Token\n-- WHY: Provides general metrics with drill-down capability for flexible Superset filtering\n\n\n\nWITH base_data AS (\n    SELECT\n        *,\n        DATE_TRUNC('minute', deposit_timestamp) AS deposit_minute\n    FROM \"across_analytics\".\"dbt_intermediate\".\"int_deposit_fill_matching\"\n),\n\n-- Chain name mapping\nchain_names AS (\n    SELECT \n        chain_id,\n        chain_name\n    FROM (VALUES\n        (1, 'Ethereum'),\n        (42161, 'Arbitrum'),\n        (137, 'Polygon'),\n        (59144, 'Linea'),\n        (480, 'Worldchain'),\n        (130, 'Unichain'),\n        (999, 'HyperEVM'),\n        (143, 'Monad')\n    ) AS chains(chain_id, chain_name)\n),\n\n-- Token metadata for symbol lookup\ntoken_metadata AS (\n    SELECT * FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n),\n\n-- Aggregate by hour, route, and token\nhourly_metrics AS (\n    SELECT\n        deposit_minute,\n        origin_chain_id,\n        destination_chain_id,\n        route_id,\n        deposit_token,\n        \n        -- === VOLUME METRICS ===\n        -- Transaction counts\n        COUNT(*) AS total_deposits,\n        SUM(CASE WHEN is_filled THEN 1 ELSE 0 END) AS total_fills,\n        \n        -- Volume in native token units\n        SUM(deposit_amount) AS total_deposit_volume,\n        SUM(CASE WHEN is_filled THEN actual_output_amount ELSE 0 END) AS total_filled_volume,\n        \n        -- Volume in USD\n        SUM(deposit_amount_usd) AS total_deposit_volume_usd,\n        SUM(CASE WHEN is_filled THEN fill_amount_usd ELSE 0 END) AS total_filled_volume_usd,\n        \n        -- === ACTIVITY METRICS ===\n        COUNT(DISTINCT depositor_address) AS unique_depositors,\n        COUNT(DISTINCT relayer_address) FILTER (WHERE is_filled) AS unique_relayers,\n        \n        -- === PERFORMANCE SNAPSHOT (summary only) ===\n        -- Fill rate\n        ROUND((SUM(CASE WHEN is_filled THEN 1 ELSE 0 END)::NUMERIC / NULLIF(COUNT(*), 0)) * 100, 2) AS fill_rate_pct,\n        \n        -- Latency summary (just avg and median for overview)\n        ROUND(AVG(CASE WHEN is_filled THEN fill_latency_seconds END)::NUMERIC, 2) AS avg_fill_latency_seconds,\n        ROUND(PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY fill_latency_seconds) FILTER (WHERE is_filled)::NUMERIC, 2) AS median_fill_latency_seconds,\n        \n        -- === ECONOMIC SNAPSHOT (summary only) ===\n        -- Total fees collected\n        SUM(CASE WHEN is_filled THEN bridge_fee_nominal ELSE 0 END) AS total_bridge_fees,\n        \n        -- Average fee percentage\n        ROUND(AVG(CASE WHEN is_filled THEN bridge_fee_percent END)::NUMERIC, 4) AS avg_bridge_fee_pct,\n        \n        -- Average slippage\n        ROUND(AVG(CASE WHEN is_filled THEN slippage_percent END)::NUMERIC, 4) AS avg_slippage_pct\n        \n    FROM base_data\n    GROUP BY \n        deposit_minute,\n        origin_chain_id,\n        destination_chain_id,\n        route_id,\n        deposit_token\n)\n\nSELECT\n    -- Time dimension\n    hm.deposit_minute,\n    \n    -- Route identifiers with human-readable names\n    hm.origin_chain_id,\n    oc.chain_name AS origin_chain_name,\n    hm.destination_chain_id,\n    dc.chain_name AS destination_chain_name,\n    hm.route_id,\n    oc.chain_name || ' \u2192 ' || dc.chain_name AS route_name,\n    \n    -- Token info\n    hm.deposit_token,\n    tm.token_symbol,\n    -- handle token symbol for base token as USDC.e and USDC.bridged are the same, etc.\n    CASE \n        WHEN token_symbol LIKE 'USDC%' THEN 'USDC'\n        WHEN token_symbol LIKE 'USDT%' THEN 'USDT'\n        WHEN token_symbol LIKE 'DAI%' THEN 'DAI'\n        WHEN token_symbol LIKE 'WETH%' THEN 'WETH'\n        WHEN token_symbol LIKE 'WBTC%' THEN 'WBTC'\n    ELSE token_symbol\n    END AS base_token_symbol,\n    \n    -- Volume metrics\n    hm.total_deposits,\n    hm.total_fills,\n    hm.total_deposit_volume,\n    hm.total_filled_volume,\n    hm.total_deposit_volume_usd,\n    hm.total_filled_volume_usd,\n    \n    -- Activity metrics\n    hm.unique_depositors,\n    hm.unique_relayers,\n    \n    -- Performance snapshot\n    hm.fill_rate_pct,\n    hm.avg_fill_latency_seconds,\n    hm.median_fill_latency_seconds,\n    \n    -- Economic snapshot\n    hm.total_bridge_fees,\n    hm.avg_bridge_fee_pct,\n    hm.avg_slippage_pct\n\nFROM hourly_metrics hm\nLEFT JOIN chain_names oc ON hm.origin_chain_id = oc.chain_id\nLEFT JOIN chain_names dc ON hm.destination_chain_id = dc.chain_id\nLEFT JOIN token_metadata tm ON hm.origin_chain_id = tm.chain_id \n    AND LOWER(hm.deposit_token) = LOWER(tm.token_address)\n\nORDER BY hm.deposit_minute DESC, hm.total_deposit_volume_usd DESC", "relation_name": "\"across_analytics\".\"dbt_marts\".\"mart_general_overview\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2026-01-05T13:09:43.949660Z", "completed_at": "2026-01-05T13:09:44.128370Z"}, {"name": "execute", "started_at": "2026-01-05T13:09:44.135741Z", "completed_at": "2026-01-05T13:09:52.253170Z"}], "thread_id": "Thread-3 (worker)", "execution_time": 8.335095405578613, "adapter_response": {"_message": "SELECT 3672", "code": "SELECT", "rows_affected": 3672}, "message": "SELECT 3672", "failures": null, "unique_id": "model.across_analytics.mart_bridge_fee_analysis", "compiled": true, "compiled_code": "-- ============================================================================\n-- mart_bridge_fee_analysis.sql\n-- ============================================================================\n-- PURPOSE: Analyze bridge fees to identify over-priced corridors and \n--          inform competitive pricing strategy\n-- \n-- GRAIN: Minute \u00d7 Route \u00d7 Token\n-- \n-- KEY INSIGHT: bridge_fee_nominal = deposit_amount - actual_output_amount\n--              This already includes ALL user costs: gas + relayer + bridge + slippage\n-- \n-- KEY BUSINESS QUESTIONS:\n--   1. Which corridors are overpriced? \u2192 effective_fee_pct comparison\n--   2. ETH\u2192Unichain vs ETH\u2192Optimism fee comparison \u2192 Filter by origin, group by destination\n--   3. Where are users paying the most? \u2192 Sort by effective_fee_pct DESC\n-- ============================================================================\n\n\n\nWITH base_data AS (\n    SELECT\n        *,\n        DATE_TRUNC('minute', deposit_timestamp) AS deposit_minute\n    FROM \"across_analytics\".\"dbt_intermediate\".\"int_deposit_fill_matching\"\n    WHERE is_filled = TRUE  -- Only filled deposits have fee data\n),\n\n-- Chain name mapping\nchain_names AS (\n    SELECT \n        chain_id,\n        chain_name\n    FROM (VALUES\n        (1, 'Ethereum'),\n        (42161, 'Arbitrum'),\n        (137, 'Polygon'),\n        (59144, 'Linea'),\n        (480, 'Worldchain'),\n        (130, 'Unichain'),\n        (999, 'HyperEVM'),\n        (143, 'Monad')\n    ) AS chains(chain_id, chain_name)\n),\n\n-- Token metadata for symbol lookup\ntoken_metadata AS (\n    SELECT * FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n),\n\n-- ============================================================================\n-- HOURLY FEE METRICS CTE\n-- ============================================================================\n-- PURPOSE: Aggregate fee-related metrics by hour, route, and token.\n--          All fee calculations are in USD for cross-token comparability.\n-- ============================================================================\nhourly_fee_metrics AS (\n    SELECT\n        deposit_minute,\n        origin_chain_id,\n        destination_chain_id,\n        route_id,\n        deposit_token,\n        \n        -- ========================================================================\n        -- VOLUME METRICS\n        -- ========================================================================\n        -- WHAT: Count of filled transactions and USD volume\n        -- WHY:  Provides context for fee metrics. A route with $10M volume and\n        --       0.3% fee is more important than $10K volume with 0.1% fee.\n        -- USE:  Weight fee analysis by volume, filter out low-activity routes.\n        -- ========================================================================\n        COUNT(*) AS total_fills,\n        SUM(deposit_amount_usd) AS total_deposit_volume_usd,\n        SUM(fill_amount_usd) AS total_filled_volume_usd,\n        \n        -- ========================================================================\n        -- TOTAL FEES (USD)\n        -- ========================================================================\n        -- WHAT: Sum of all fees collected on this route, converted to USD\n        -- WHY:  bridge_fee_nominal = deposit_amount - actual_output_amount\n        --       This is the ALL-IN user cost including gas, relayer, and protocol fees.\n        -- USE:  Revenue analysis, compare fee revenue across routes.\n        -- ========================================================================\n        SUM(bridge_fee_nominal * COALESCE(deposit_token_price_usd, 1)) AS total_fees_usd,\n        \n        -- ========================================================================\n        -- EFFECTIVE FEE PERCENTAGE (Core Metric)\n        -- ========================================================================\n        -- WHAT: Total fees / Total volume \u00d7 100 (volume-weighted average fee)\n        -- WHY:  This is the TRUE cost per dollar bridged. Unlike avg_fee_pct,\n        --       this weights large transactions more heavily (as they should be).\n        --       Example: 10 small txns at 0.5% + 1 large txn at 0.1% \u2192 effective < 0.5%\n        -- USE:  Primary metric for corridor pricing comparison. Target: <0.3%.\n        -- ========================================================================\n        ROUND(\n            ((SUM(bridge_fee_nominal * COALESCE(deposit_token_price_usd, 1)) / \n             NULLIF(SUM(deposit_amount_usd), 0)) * 100)::NUMERIC, 4) \n        AS effective_fee_pct,\n        \n        -- ========================================================================\n        -- AVERAGE FEE PERCENTAGE\n        -- ========================================================================\n        -- WHAT: Simple arithmetic mean of fee % across all transactions\n        -- WHY:  Treats all transactions equally. Useful for understanding \"typical\"\n        --       user experience regardless of transaction size.\n        -- USE:  Compare with effective_fee_pct; big difference = size disparity.\n        -- ========================================================================\n        ROUND(AVG(bridge_fee_percent)::NUMERIC, 4) AS avg_fee_pct,\n        \n        -- ========================================================================\n        -- MEDIAN FEE PERCENTAGE\n        -- ========================================================================\n        -- WHAT: 50th percentile of fee % (half of txns pay more, half pay less)\n        -- WHY:  More robust than avg to outliers. If median << avg, there are\n        --       some transactions with unusually high fees (investigate why).\n        -- USE:  Robust \"typical\" fee, outlier detection when compared to avg.\n        -- ========================================================================\n        ROUND(\n            PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY bridge_fee_percent)::NUMERIC, \n            4\n        ) AS median_fee_pct,\n        \n        -- ========================================================================\n        -- FEE STANDARD DEVIATION\n        -- ========================================================================\n        -- WHAT: How much fee % varies across transactions\n        -- WHY:  High std dev = unpredictable pricing for users. Users prefer\n        --       consistent fees. High volatility may indicate dynamic pricing\n        --       issues or liquidity fluctuations.\n        -- USE:  Stability indicator, alerting on high volatility routes.\n        -- ========================================================================\n        ROUND(STDDEV(bridge_fee_percent)::NUMERIC, 4) AS fee_std_dev,\n        \n        -- ========================================================================\n        -- MIN/MAX FEE PERCENTAGE\n        -- ========================================================================\n        -- WHAT: Best and worst fee % observed on this route\n        -- WHY:  Shows the full range of user experience. Large spread indicates\n        --       inconsistent pricing that confuses users.\n        -- USE:  Outlier investigation, competitive analysis (\"best rate we offer\").\n        -- ========================================================================\n        ROUND(MIN(bridge_fee_percent)::NUMERIC, 4) AS min_fee_pct,\n        ROUND(MAX(bridge_fee_percent)::NUMERIC, 4) AS max_fee_pct,\n        \n        -- ========================================================================\n        -- SLIPPAGE METRICS (Transparency, Not Cost)\n        -- ========================================================================\n        -- WHAT: Difference between expected_output and actual_output as %\n        -- WHY:  Slippage is ALREADY INCLUDED in bridge_fee_nominal. We show it\n        --       separately for transparency: how much of the fee is \"predictable\"\n        --       vs \"market movement\".\n        -- NOTE: Positive slippage = user got less than expected (bad)\n        --       Negative slippage = user got more than expected (rare, good)\n        -- USE:  Transparency reporting, not for cost calculation.\n        -- ========================================================================\n        ROUND(AVG(slippage_percent)::NUMERIC, 4) AS avg_slippage_pct,\n        ROUND(\n            PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY slippage_percent)::NUMERIC, \n            4\n        ) AS median_slippage_pct,\n        \n        -- ========================================================================\n        -- PARTICIPANT METRICS\n        -- ========================================================================\n        -- WHAT: Count of unique users and relayers on this route\n        -- WHY:  High depositor count = popular route (user demand).\n        --       High relayer count = competitive filling (healthy supply).\n        --       Low relayer count + high volume = relayer concentration risk.\n        -- USE:  Market health assessment, relayer diversity monitoring.\n        -- ========================================================================\n        COUNT(DISTINCT depositor_address) AS unique_depositors,\n        COUNT(DISTINCT relayer_address) AS unique_relayers,\n        \n        -- ========================================================================\n        -- GAS COST METRICS (Relayer Cost Analysis)\n        -- ========================================================================\n        -- WHAT: Gas price and cost metrics for relayer fills\n        -- WHY:  Gas costs are incurred by RELAYERS on the destination chain.\n        --       High gas costs may:\n        --       - Reduce relayer profitability \u2192 fewer fillers \u2192 slower fills\n        --       - Force higher bridge fees to remain profitable\n        --       - Explain fee volatility on certain corridors\n        -- USE:  Relayer economics analysis, corridor cost comparison, fee justification.\n        -- ========================================================================\n        -- ========================================================================\n        -- Average gas price in Gwei (wei / 10^9)\n        ROUND((AVG(gas_price_wei) / 1e9)::NUMERIC, 4) AS avg_gas_price_gwei,\n        \n        -- Median gas price in Gwei (more robust than average)\n        ROUND(\n            (PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY gas_price_wei) / 1e9)::NUMERIC, \n            4\n        ) AS median_gas_price_gwei,\n        \n        -- Total gas cost in native token (sum of all relayer gas costs)\n        -- NOTE: Different chains use different native tokens (ETH, MATIC, WLD, etc.)\n        ROUND((SUM(gas_cost_wei) / 1e18)::NUMERIC, 8) AS total_gas_cost_native,\n        \n        -- Average gas cost per fill in native token\n        ROUND((AVG(gas_cost_wei) / 1e18)::NUMERIC, 8) AS avg_gas_cost_native,\n        \n        -- ========================================================================\n        -- GAS COST IN USD (Cross-Chain Comparable)\n        -- ========================================================================\n        -- WHAT: Gas costs converted to USD using native token price at fill time\n        -- WHY:  Native token costs aren't comparable across chains (0.001 ETH \u2260 0.001 MATIC).\n        --       USD conversion enables:\n        --       - Cross-chain gas cost comparison\n        --       - Total relayer operational cost analysis\n        --       - Fee vs gas cost ratio calculations\n        -- USE:  Relayer profitability analysis, corridor cost benchmarking.\n        -- ========================================================================\n        ROUND(SUM(gas_cost_usd)::NUMERIC, 2) AS total_gas_cost_usd,\n        ROUND(AVG(gas_cost_usd)::NUMERIC, 4) AS avg_gas_cost_usd\n        \n    FROM base_data\n    GROUP BY \n        deposit_minute,\n        origin_chain_id,\n        destination_chain_id,\n        route_id,\n        deposit_token\n),\n\n-- ============================================================================\n-- COMPUTED INSIGHTS CTE\n-- ============================================================================\n-- PURPOSE: Transform numeric metrics into categorical labels for easy\n--          Superset filtering, alerting, and executive reporting.\n-- ============================================================================\nwith_insights AS (\n    SELECT\n        *,\n        \n        -- ========================================================================\n        -- PRICING TIER (Categorical Competitive Position)\n        -- ========================================================================\n        -- WHAT: Classifies route as OVERPRICED / HIGH / COMPETITIVE / AGGRESSIVE / VERY_LOW\n        -- WHY:  Enables quick identification of routes where Across may be\n        --       losing users to competitors (OVERPRICED) or leaving money on\n        --       the table (VERY_LOW).\n        --\n        -- DATA-DRIVEN THRESHOLDS (based on actual distribution from dataset):\n        --   Distribution: median=0.0135%, p75=0.056%, p95=0.37%, p99=2.15%\n        --\n        --   - OVERPRICED:   > 0.5%    \u2192 True outliers (~5% of data), investigate\n        --   - HIGH:         0.1-0.5%  \u2192 Above p75, higher than typical (~15%)\n        --   - COMPETITIVE:  0.02-0.1% \u2192 Around median, market rate (~30-40%)\n        --   - AGGRESSIVE:   0.005-0.02% \u2192 Below median, undercutting (~20-30%)\n        --   - VERY_LOW:     < 0.005%  \u2192 Near-zero fees, possibly subsidized (~15%)\n        --\n        -- USE:  Superset filter \"Show OVERPRICED routes\", pricing strategy review.\n        -- ========================================================================\n        CASE \n            WHEN effective_fee_pct > 0.5 THEN 'OVERPRICED'\n            WHEN effective_fee_pct > 0.1 THEN 'HIGH'\n            WHEN effective_fee_pct > 0.02 THEN 'COMPETITIVE'\n            WHEN effective_fee_pct > 0.005 THEN 'AGGRESSIVE'\n            ELSE 'VERY_LOW'\n        END AS pricing_tier,\n        \n        -- ========================================================================\n        -- FEE VOLATILITY TIER (Pricing Stability)\n        -- ========================================================================\n        -- WHAT: Classifies fee consistency as HIGH_VOLATILITY / MODERATE / LOW / STABLE\n        -- WHY:  Users prefer predictable fees. High volatility indicates:\n        --       - Dynamic pricing swings (may be correct but confusing)\n        --       - Liquidity issues causing fee spikes\n        --       - Inconsistent relayer behavior\n        --\n        -- DATA-DRIVEN THRESHOLDS (based on actual distribution from dataset):\n        --   Distribution: median=0.057, p75=0.36, p95=1.68, p99=8.28\n        --\n        --   - HIGH_VOLATILITY:     > 1.0   \u2192 True outliers (~10%), investigate\n        --   - MODERATE_VOLATILITY: 0.3-1.0 \u2192 Above p75, notable variation (~15-20%)\n        --   - LOW_VOLATILITY:      0.05-0.3 \u2192 Around median, acceptable (~40-50%)\n        --   - STABLE:              < 0.05  \u2192 Below median, consistent pricing (~25%)\n        --\n        -- USE:  Alert on HIGH_VOLATILITY routes, UX improvement targeting.\n        -- ========================================================================\n        CASE \n            WHEN fee_std_dev > 1.0 THEN 'HIGH_VOLATILITY'\n            WHEN fee_std_dev > 0.3 THEN 'MODERATE_VOLATILITY'\n            WHEN fee_std_dev > 0.05 THEN 'LOW_VOLATILITY'\n            ELSE 'STABLE'\n        END AS fee_volatility_tier,\n        \n        -- ========================================================================\n        -- FEE SPREAD (Range Indicator)\n        -- ========================================================================\n        -- WHAT: max_fee_pct - min_fee_pct (width of fee range)\n        -- WHY:  Complements std_dev with a simpler measure. Large spread means\n        --       some users got great rates while others got terrible rates.\n        --       Example: min=0.1%, max=1.0% \u2192 spread=0.9% (very inconsistent!)\n        -- USE:  Fairness analysis, identify routes with extreme outliers.\n        -- ========================================================================\n        ROUND((max_fee_pct - min_fee_pct)::NUMERIC, 4) AS fee_spread_pct,\n        \n        -- ========================================================================\n        -- GAS COST TIER (Relayer Economics)\n        -- ========================================================================\n        -- WHAT: Classifies corridors by gas cost level\n        -- WHY:  High gas costs affect relayer profitability and may explain:\n        --       - Higher bridge fees on certain routes\n        --       - Fewer relayers willing to fill (leading to slower fills)\n        --       - Corridors that need optimization\n        --\n        -- DATA-DRIVEN THRESHOLDS (based on actual distribution from dataset):\n        --   Distribution: median=0.00001, p75=0.00016, p95=0.067, p99=0.19\n        --\n        --   - VERY_HIGH: > 0.05 native   \u2192 Top ~5%, expensive corridors\n        --   - HIGH:      0.001-0.05      \u2192 Above p75, notable cost (~10-15%)\n        --   - MEDIUM:    0.0001-0.001    \u2192 Around p75, moderate (~15-20%)\n        --   - LOW:       0.00001-0.0001  \u2192 Around median (~30-40%)\n        --   - VERY_LOW:  < 0.00001       \u2192 Below median, very cheap (~25%)\n        --\n        -- USE:  Corridor optimization, relayer incentive analysis.\n        -- ========================================================================\n        CASE \n            WHEN avg_gas_cost_native > 0.05 THEN 'VERY_HIGH'\n            WHEN avg_gas_cost_native > 0.001 THEN 'HIGH'\n            WHEN avg_gas_cost_native > 0.0001 THEN 'MEDIUM'\n            WHEN avg_gas_cost_native > 0.00001 THEN 'LOW'\n            ELSE 'VERY_LOW'\n        END AS gas_cost_tier\n        \n    FROM hourly_fee_metrics\n)\n\nSELECT\n    -- Time dimension\n    wi.deposit_minute,\n    \n    -- Route identifiers with human-readable names\n    wi.origin_chain_id,\n    oc.chain_name AS origin_chain_name,\n    wi.destination_chain_id,\n    dc.chain_name AS destination_chain_name,\n    wi.route_id,\n    oc.chain_name || ' \u2192 ' || dc.chain_name AS route_name,\n    \n    -- Token info\n    wi.deposit_token,\n    tm.token_symbol,\n    CASE \n        WHEN tm.token_symbol LIKE 'USDC%' THEN 'USDC'\n        WHEN tm.token_symbol LIKE 'USDT%' THEN 'USDT'\n        WHEN tm.token_symbol LIKE 'DAI%' THEN 'DAI'\n        WHEN tm.token_symbol LIKE 'WETH%' THEN 'WETH'\n        WHEN tm.token_symbol LIKE 'WBTC%' THEN 'WBTC'\n        ELSE tm.token_symbol\n    END AS base_token_symbol,\n    \n    -- Volume metrics\n    wi.total_fills,\n    wi.total_deposit_volume_usd,\n    wi.total_filled_volume_usd,\n    \n    -- Fee metrics (USD)\n    wi.total_fees_usd,\n    wi.effective_fee_pct,\n    \n    -- Fee distribution\n    wi.avg_fee_pct,\n    wi.median_fee_pct,\n    wi.fee_std_dev,\n    wi.min_fee_pct,\n    wi.max_fee_pct,\n    wi.fee_spread_pct,\n    \n    -- Slippage (for transparency)\n    wi.avg_slippage_pct,\n    wi.median_slippage_pct,\n    \n    -- Computed insights\n    wi.pricing_tier,\n    wi.fee_volatility_tier,\n    \n    -- Participant metrics\n    wi.unique_depositors,\n    wi.unique_relayers,\n    \n    -- Gas cost metrics (relayer economics)\n    wi.avg_gas_price_gwei,\n    wi.median_gas_price_gwei,\n    wi.total_gas_cost_native,\n    wi.avg_gas_cost_native,\n    wi.total_gas_cost_usd,\n    wi.avg_gas_cost_usd,\n    wi.gas_cost_tier\n\nFROM with_insights wi\nLEFT JOIN chain_names oc ON wi.origin_chain_id = oc.chain_id\nLEFT JOIN chain_names dc ON wi.destination_chain_id = dc.chain_id\nLEFT JOIN token_metadata tm ON wi.origin_chain_id = tm.chain_id \n    AND LOWER(wi.deposit_token) = LOWER(tm.token_address)\n\nORDER BY wi.deposit_minute DESC, wi.total_deposit_volume_usd DESC", "relation_name": "\"across_analytics\".\"dbt_marts\".\"mart_bridge_fee_analysis\"", "batch_results": null}], "elapsed_time": 16.306721210479736, "args": {"log_path": "C:\\Users\\Longin\\Desktop\\Projects\\across_analytics\\logs", "partial_parse_file_diff": true, "use_colors": true, "invocation_command": "dbt run", "strict_mode": false, "use_fast_test_edges": false, "version_check": true, "warn_error_options": {"error": [], "warn": [], "silence": []}, "which": "run", "indirect_selection": "eager", "log_format_file": "debug", "macro_debugging": false, "log_level": "info", "show_resource_report": false, "show_all_deprecations": false, "send_anonymous_usage_stats": true, "require_generic_test_arguments_property": true, "source_freshness_run_project_hooks": true, "require_batched_execution_for_custom_microbatch_strategy": false, "profiles_dir": "C:\\Users\\Longin\\.dbt", "cache_selected_only": false, "log_level_file": "debug", "empty": false, "write_json": true, "use_colors_file": true, "populate_cache": true, "quiet": false, "require_all_warnings_handled_by_warn_error": false, "require_nested_cumulative_type_params": false, "log_file_max_bytes": 10485760, "require_yaml_configuration_for_mf_time_spines": false, "skip_nodes_if_on_run_start_fails": false, "log_format": "default", "defer": false, "project_dir": "C:\\Users\\Longin\\Desktop\\Projects\\across_analytics", "partial_parse": true, "select": [], "upload_to_artifacts_ingest_api": false, "validate_macro_args": false, "vars": {}, "favor_state": false, "require_explicit_package_overrides_for_builtin_materializations": true, "print": true, "require_resource_names_without_spaces": true, "exclude": [], "printer_width": 80, "state_modified_compare_vars": false, "static_parser": true, "state_modified_compare_more_unrendered_values": false, "introspect": true}}