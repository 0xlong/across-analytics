{"metadata": {"dbt_schema_version": "https://schemas.getdbt.com/dbt/run-results/v6.json", "dbt_version": "1.10.15", "generated_at": "2025-12-29T22:05:31.397023Z", "invocation_id": "155b651d-208e-4fa7-9ac9-1f9b49b1879a", "invocation_started_at": "2025-12-29T22:05:23.484646Z", "env": {}}, "results": [{"status": "success", "timing": [{"name": "compile", "started_at": "2025-12-29T22:05:26.791540Z", "completed_at": "2025-12-29T22:05:26.831092Z"}, {"name": "execute", "started_at": "2025-12-29T22:05:26.835151Z", "completed_at": "2025-12-29T22:05:29.331061Z"}], "thread_id": "Thread-3 (worker)", "execution_time": 2.549805164337158, "adapter_response": {"_message": "SELECT 1403", "code": "SELECT", "rows_affected": 1403}, "message": "SELECT 1403", "failures": null, "unique_id": "model.across_analytics.mart_settlement_time", "compiled": true, "compiled_code": "-- ============================================================================\n-- mart_settlement_time.sql\n-- ============================================================================\n-- PURPOSE: Calculate Time-to-Settlement (TTS) for relayer refunds\n-- \n-- WHAT IS TTS?\n--   When a relayer fills a user's bridge request, they use their OWN money.\n--   Later, the Across protocol pays them back (settles) via the optimistic oracle.\n--   TTS = Time between FILL and SETTLEMENT (refund)\n--\n-- WHY THIS MATTERS:\n--   - High TTS = Relayer capital is locked longer = Higher risk for relayers\n--   - Low TTS = Faster capital recycling = More efficient for relayers\n--   - Helps identify chains where settlement is slow\n--\n-- GRAIN: Minute \u00d7 Chain \u00d7 Relayer\n-- ============================================================================\n\n\n\n-- ============================================================================\n-- STEP 1: Get all individual refunds (settlements) to relayers\n-- ============================================================================\n-- This model already expands batched refunds into one row per relayer payment\nWITH refunds AS (\n    SELECT\n        refund_timestamp,           -- When the relayer got their money back\n        chain_id,                   -- Which chain the refund was executed on\n        relayer_address,            -- Who received the refund\n        refund_token_address,\n        refund_token_symbol,\n        refund_amount,              -- How much they received (human-readable)\n        root_bundle_id,             -- Groups refunds into batches\n        source_blockchain\n    FROM \"across_analytics\".\"dbt_intermediate\".\"int_unified_refunds_expanded\"\n    WHERE relayer_address IS NOT NULL\n),\n\n-- ============================================================================\n-- STEP 2: Get all fills (when relayers fronted their capital)\n-- ============================================================================\n-- Each fill = relayer used their own money to fulfill a user's bridge request\nfills AS (\n    SELECT\n        fill_timestamp,             -- When the relayer filled the order\n        destination_chain_id,       -- Where the fill happened\n        relayer_address,            -- Who did the fill\n        output_amount,              -- How much they paid out\n        output_token_address\n    FROM \"across_analytics\".\"dbt_intermediate\".\"int_unified_fills\"\n    WHERE relayer_address IS NOT NULL\n),\n\n-- ============================================================================\n-- STEP 3: Find the FIRST fill before each refund for each relayer\n-- ============================================================================\n-- Logic: For each refund, find the most recent fill by the same relayer\n-- on the same chain. The difference = how long their capital was locked.\n--\n-- NOTE: This is an approximation because refunds are batched.\n-- We match by relayer + chain + time window (refund must come AFTER fill)\nrefund_with_prior_fill AS (\n    SELECT\n        r.refund_timestamp,\n        r.chain_id,\n        r.relayer_address,\n        r.refund_token_symbol,\n        r.refund_amount,\n        r.root_bundle_id,\n        r.source_blockchain,\n        \n        -- Find the LATEST fill before this refund (closest in time)\n        MAX(f.fill_timestamp) AS matched_fill_timestamp\n        \n    FROM refunds r\n    \n    -- Join fills where:\n    -- 1. Same relayer\n    -- 2. Same chain (destination chain = refund chain)\n    -- 3. Fill happened BEFORE the refund (makes sense - can't refund before filling)\n    LEFT JOIN fills f\n        ON r.relayer_address = f.relayer_address\n        AND r.chain_id = f.destination_chain_id\n        AND f.fill_timestamp < r.refund_timestamp\n        -- Only look at fills within 7 days of refund (reasonable settlement window)\n        AND f.fill_timestamp > r.refund_timestamp - INTERVAL '7 days'\n    \n    GROUP BY\n        r.refund_timestamp,\n        r.chain_id,\n        r.relayer_address,\n        r.refund_token_symbol,\n        r.refund_amount,\n        r.root_bundle_id,\n        r.source_blockchain\n),\n\n-- ============================================================================\n-- STEP 4: Calculate Time-to-Settlement (TTS) for each matched pair\n-- ============================================================================\nwith_tts AS (\n    SELECT\n        *,\n        -- TTS in seconds = refund time - fill time\n        CASE \n            WHEN matched_fill_timestamp IS NOT NULL \n            THEN EXTRACT(EPOCH FROM (refund_timestamp - matched_fill_timestamp))\n            ELSE NULL \n        END AS settlement_time_seconds,\n        \n        -- TTS in minutes (matches grain)\n        CASE \n            WHEN matched_fill_timestamp IS NOT NULL \n            THEN ROUND(\n                (EXTRACT(EPOCH FROM (refund_timestamp - matched_fill_timestamp)) / 60)::NUMERIC, \n                2\n            )\n            ELSE NULL \n        END AS settlement_time_minutes\n        \n    FROM refund_with_prior_fill\n),\n\n-- ============================================================================\n-- STEP 5: Aggregate by minute \u00d7 Chain \u00d7 Relayer\n-- ============================================================================\n-- Roll up individual TTS values into minute-level summaries\ndaily_aggregates AS (\n    SELECT\n        DATE_TRUNC('minute', refund_timestamp) AS settlement_date,\n        chain_id,\n        relayer_address,\n        source_blockchain,\n        \n        -- Count metrics\n        COUNT(*) AS settlement_count,\n        COUNT(settlement_time_seconds) AS matched_settlement_count,\n        \n        -- TTS metrics (in minutes for readability)\n        ROUND(AVG(settlement_time_minutes)::NUMERIC, 2) AS avg_settlement_time_minutes,\n        ROUND(MIN(settlement_time_minutes)::NUMERIC, 2) AS min_settlement_time_minutes,\n        ROUND(MAX(settlement_time_minutes)::NUMERIC, 2) AS max_settlement_time_minutes,\n        \n        -- Percentiles for distribution analysis\n        ROUND(\n            PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY settlement_time_minutes)::NUMERIC, \n            2\n        ) AS median_settlement_time_minutes,\n        ROUND(\n            PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY settlement_time_minutes)::NUMERIC, \n            2\n        ) AS p95_settlement_time_minutes,\n        \n        -- Volume settled\n        SUM(refund_amount) AS total_settled_volume\n        \n    FROM with_tts\n    WHERE settlement_time_seconds > 0  -- Only include valid TTS values\n    GROUP BY\n        DATE_TRUNC('minute', refund_timestamp),\n        chain_id,\n        relayer_address,\n        source_blockchain\n),\n\n-- ============================================================================\n-- STEP 6: Add human-readable chain names\n-- ============================================================================\nchain_names AS (\n    SELECT chain_id, chain_name\n    FROM (VALUES\n        (1, 'Ethereum'),\n        (42161, 'Arbitrum'),\n        (137, 'Polygon'),\n        (59144, 'Linea'),\n        (480, 'Worldchain'),\n        (130, 'Unichain'),\n        (999, 'HyperEVM'),\n        (143, 'Monad')\n    ) AS chains(chain_id, chain_name)\n)\n\n-- ============================================================================\n-- FINAL OUTPUT\n-- ============================================================================\nSELECT\n    -- Time dimension\n    da.settlement_date,\n    \n    -- Chain identification\n    da.chain_id,\n    cn.chain_name,\n    da.source_blockchain,\n    \n    -- Relayer identification\n    da.relayer_address,\n    \n    -- Settlement counts\n    da.settlement_count,\n    da.matched_settlement_count,\n    \n    -- TTS metrics (in minutes)\n    da.avg_settlement_time_minutes,\n    da.min_settlement_time_minutes,\n    da.max_settlement_time_minutes,\n    da.median_settlement_time_minutes,\n    da.p95_settlement_time_minutes,\n    \n    -- Volume\n    da.total_settled_volume\n\nFROM daily_aggregates da\nLEFT JOIN chain_names cn ON da.chain_id = cn.chain_id\n\nORDER BY da.settlement_date DESC, da.chain_id, da.total_settled_volume DESC", "relation_name": "\"across_analytics\".\"dbt_marts\".\"mart_settlement_time\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2025-12-29T22:05:26.815319Z", "completed_at": "2025-12-29T22:05:26.832657Z"}, {"name": "execute", "started_at": "2025-12-29T22:05:26.862142Z", "completed_at": "2025-12-29T22:05:30.985582Z"}], "thread_id": "Thread-2 (worker)", "execution_time": 4.201863527297974, "adapter_response": {"_message": "SELECT 3493", "code": "SELECT", "rows_affected": 3493}, "message": "SELECT 3493", "failures": null, "unique_id": "model.across_analytics.mart_fill_latency_distribution", "compiled": true, "compiled_code": "-- mart_fill_latency_distribution.sql\n-- PURPOSE: Fill latency histogram for \"Why Across is 5-20x faster\" analysis\n-- GRAIN: Hour \u00d7 Route \u00d7 Latency Bucket\n-- WHY: Enables visualization of speed distribution to prove Across's competitive advantage\n\n\n\nWITH base_data AS (\n    SELECT\n        *,\n        DATE_TRUNC('minute', deposit_timestamp) AS deposit_minute\n    FROM \"across_analytics\".\"dbt_intermediate\".\"int_deposit_fill_matching\"\n    WHERE is_filled = TRUE  -- Only filled deposits have latency\n),\n\n-- Chain name mapping for readability\nchain_names AS (\n    SELECT \n        chain_id,\n        chain_name\n    FROM (VALUES\n        (1, 'Ethereum'),\n        (42161, 'Arbitrum'),\n        (137, 'Polygon'),\n        (59144, 'Linea'),\n        (480, 'Worldchain'),\n        (130, 'Unichain'),\n        (999, 'HyperEVM'),\n        (143, 'Monad')\n    ) AS chains(chain_id, chain_name)\n),\n\n-- Classify each fill into latency buckets\nlatency_classified AS (\n    SELECT\n        deposit_minute,\n        origin_chain_id,\n        destination_chain_id,\n        route_id,\n        fill_latency_seconds,\n        deposit_amount,\n        \n        -- Latency bucket classification\n        CASE \n            WHEN fill_latency_seconds <= 30 THEN '1_instant'\n            WHEN fill_latency_seconds <= 60 THEN '2_fast'\n            WHEN fill_latency_seconds <= 300 THEN '3_normal'\n            WHEN fill_latency_seconds <= 900 THEN '4_slow'\n            ELSE '5_very_slow'\n        END AS latency_bucket,\n        \n        -- Human-readable bucket label\n        CASE \n            WHEN fill_latency_seconds <= 30 THEN '\u226430s (Instant)'\n            WHEN fill_latency_seconds <= 60 THEN '31-60s (Fast)'\n            WHEN fill_latency_seconds <= 300 THEN '1-5min (Normal)'\n            WHEN fill_latency_seconds <= 900 THEN '5-15min (Slow)'\n            ELSE '>15min (Very Slow)'\n        END AS latency_bucket_label\n        \n    FROM base_data\n),\n\n-- Aggregate by minute, route, and latency bucket\nbucket_aggregates AS (\n    SELECT\n        deposit_minute,\n        origin_chain_id,\n        destination_chain_id,\n        route_id,\n        latency_bucket,\n        latency_bucket_label,\n        \n        -- Count metrics\n        COUNT(*) AS fill_count,\n        \n        -- Volume metrics\n        SUM(deposit_amount) AS fill_volume,\n        \n        -- Latency stats within bucket\n        ROUND(AVG(fill_latency_seconds)::NUMERIC, 2) AS avg_latency_in_bucket,\n        MIN(fill_latency_seconds) AS min_latency_in_bucket,\n        MAX(fill_latency_seconds) AS max_latency_in_bucket\n        \n    FROM latency_classified\n    GROUP BY \n        deposit_minute,\n        origin_chain_id,\n        destination_chain_id,\n        route_id,\n        latency_bucket,\n        latency_bucket_label\n),\n\n-- Calculate percentages within each hour/route\nwith_percentages AS (\n    SELECT\n        ba.*,\n        \n        -- Percentage of fills in this bucket (within hour/route)\n        ROUND(\n            (ba.fill_count::NUMERIC / NULLIF(SUM(ba.fill_count) OVER (\n                PARTITION BY ba.deposit_minute, ba.route_id\n            ), 0)) * 100,\n            2\n        ) AS pct_of_fills,\n        \n        -- Cumulative percentage (for CDF charts)\n        ROUND(\n            (SUM(ba.fill_count) OVER (\n                PARTITION BY ba.deposit_minute, ba.route_id\n                ORDER BY ba.latency_bucket\n            )::NUMERIC / NULLIF(SUM(ba.fill_count) OVER (\n                PARTITION BY ba.deposit_minute, ba.route_id\n            ), 0)) * 100,\n            2\n        ) AS cumulative_pct\n        \n    FROM bucket_aggregates ba\n)\n\nSELECT\n    -- Time dimension\n    wp.deposit_minute,\n    \n    -- Route identifiers with human-readable names\n    wp.origin_chain_id,\n    oc.chain_name AS origin_chain_name,\n    wp.destination_chain_id,\n    dc.chain_name AS destination_chain_name,\n    wp.route_id,\n    \n    -- Latency bucket\n    wp.latency_bucket,\n    wp.latency_bucket_label,\n    \n    -- Counts and volume\n    wp.fill_count,\n    wp.fill_volume,\n    \n    -- Percentages\n    wp.pct_of_fills,\n    wp.cumulative_pct,\n    \n    -- Latency stats\n    wp.avg_latency_in_bucket,\n    wp.min_latency_in_bucket,\n    wp.max_latency_in_bucket\n\nFROM with_percentages wp\nLEFT JOIN chain_names oc ON wp.origin_chain_id = oc.chain_id\nLEFT JOIN chain_names dc ON wp.destination_chain_id = dc.chain_id\n\nORDER BY wp.deposit_minute DESC, wp.route_id, wp.latency_bucket", "relation_name": "\"across_analytics\".\"dbt_marts\".\"mart_fill_latency_distribution\"", "batch_results": null}, {"status": "success", "timing": [{"name": "compile", "started_at": "2025-12-29T22:05:26.822291Z", "completed_at": "2025-12-29T22:05:26.834083Z"}, {"name": "execute", "started_at": "2025-12-29T22:05:26.953303Z", "completed_at": "2025-12-29T22:05:31.181321Z"}], "thread_id": "Thread-1 (worker)", "execution_time": 4.394321441650391, "adapter_response": {"_message": "SELECT 3676", "code": "SELECT", "rows_affected": 3676}, "message": "SELECT 3676", "failures": null, "unique_id": "model.across_analytics.mart_bridge_performance", "compiled": true, "compiled_code": "-- mart_bridge_performance.sql\n-- PURPOSE: Hourly bridge health dashboard with core KPIs - performance metrics for monitoring bridge efficiency\n\n\n\nWITH base_data AS (\n    SELECT\n        *,\n        -- Extract minute for aggregation\n        DATE_TRUNC('minute', deposit_timestamp) AS deposit_minute\n    FROM \"across_analytics\".\"dbt_intermediate\".\"int_deposit_fill_matching\"\n),\n\n-- Chain name mapping for readability\nchain_names AS (\n    SELECT \n        chain_id,\n        chain_name\n    FROM (VALUES\n        (1, 'Ethereum'),\n        (42161, 'Arbitrum'),\n        (137, 'Polygon'),\n        (59144, 'Linea'),\n        (480, 'Worldchain'),\n        (130, 'Unichain'),\n        (999, 'HyperEVM'),\n        (143, 'Monad')\n    ) AS chains(chain_id, chain_name)\n),\n\n-- Token metadata for symbol lookup\ntoken_metadata AS (\n    SELECT * FROM \"across_analytics\".\"dbt\".\"token_metadata\"\n),  \n\n-- Aggregate by minute, route, and token\nminute_metrics AS (\n    SELECT\n        deposit_minute,\n        origin_chain_id,\n        destination_chain_id,\n        route_id,\n        deposit_token,\n        \n        -- Volume metrics\n        COUNT(*) AS total_deposits,\n        SUM(CASE WHEN is_filled THEN 1 ELSE 0 END) AS total_fills,\n        SUM(CASE WHEN NOT is_filled THEN 1 ELSE 0 END) AS unfilled_count,\n        \n        -- Fill rate (% of deposits filled)\n        ROUND(\n            (SUM(CASE WHEN is_filled THEN 1 ELSE 0 END)::NUMERIC / NULLIF(COUNT(*), 0)) * 100, \n            2\n        ) AS fill_rate_pct,\n        \n        -- Amount metrics\n        SUM(deposit_amount) AS total_deposit_volume,\n        SUM(CASE WHEN NOT is_filled THEN deposit_amount ELSE 0 END) AS unfilled_volume,\n        \n        -- Latency metrics (only for filled deposits)\n        ROUND(AVG(CASE WHEN is_filled THEN fill_latency_seconds END)::NUMERIC, 2) AS avg_fill_latency_seconds,\n        ROUND(PERCENTILE_CONT(0.5) WITHIN GROUP (ORDER BY fill_latency_seconds) FILTER (WHERE is_filled)::NUMERIC, 2) AS p50_latency_seconds,\n        ROUND(PERCENTILE_CONT(0.95) WITHIN GROUP (ORDER BY fill_latency_seconds) FILTER (WHERE is_filled)::NUMERIC, 2) AS p95_latency_seconds,\n        MIN(CASE WHEN is_filled THEN fill_latency_seconds END) AS min_latency_seconds,\n        MAX(CASE WHEN is_filled THEN fill_latency_seconds END) AS max_latency_seconds,\n        \n        -- Slippage metrics\n        -- slippage here is the difference between expected (what despoitor is promised to get) and actual output (what depositor actually get)\n        ROUND(AVG(slippage_percent)::NUMERIC, 4) AS avg_slippage_pct, \n        \n        -- Bridge fee metrics (difference between deposit and fill amounts)\n        ROUND(AVG(bridge_fee_nominal)::NUMERIC, 4) AS avg_bridge_fee_nominal,\n        ROUND(AVG(bridge_fee_percent)::NUMERIC, 4) AS avg_bridge_fee_pct,\n        \n        -- Unique participants\n        COUNT(DISTINCT depositor_address) AS unique_depositors,\n        COUNT(DISTINCT relayer_address) FILTER (WHERE is_filled) AS unique_relayers\n        \n    FROM base_data\n    GROUP BY \n        deposit_minute,\n        origin_chain_id,\n        destination_chain_id,\n        route_id,\n        deposit_token\n)\n\nSELECT\n    -- Time dimension\n    mm.deposit_minute,\n    \n    -- Route identifiers with human-readable names\n    mm.origin_chain_id,\n    oc.chain_name AS origin_chain_name,\n    mm.destination_chain_id,\n    dc.chain_name AS destination_chain_name,\n    mm.route_id,\n    oc.chain_name || ' \u2192 ' || dc.chain_name AS route_name,\n    mm.deposit_token,\n    tm.token_symbol,\n    \n    -- Volume metrics\n    mm.total_deposits,\n    mm.total_fills,\n    mm.unfilled_count,\n    mm.fill_rate_pct,\n    \n    -- Amount metrics\n    mm.total_deposit_volume,\n    mm.unfilled_volume,\n    \n    -- Latency metrics\n    mm.avg_fill_latency_seconds,\n    mm.p50_latency_seconds,\n    mm.p95_latency_seconds,\n    mm.min_latency_seconds,\n    mm.max_latency_seconds,\n    \n    -- Slippage\n    mm.avg_slippage_pct,\n    \n    -- Bridge fees\n    mm.avg_bridge_fee_nominal,\n    mm.avg_bridge_fee_pct,\n    \n    -- Participants\n    mm.unique_depositors,\n    mm.unique_relayers\n\nFROM minute_metrics mm\nLEFT JOIN chain_names oc ON mm.origin_chain_id = oc.chain_id\nLEFT JOIN chain_names dc ON mm.destination_chain_id = dc.chain_id\nLEFT JOIN token_metadata tm ON mm.origin_chain_id = tm.chain_id \n    AND LOWER(mm.deposit_token) = LOWER(tm.token_address)\n\nORDER BY mm.deposit_minute DESC, mm.total_deposits DESC", "relation_name": "\"across_analytics\".\"dbt_marts\".\"mart_bridge_performance\"", "batch_results": null}], "elapsed_time": 5.319175958633423, "args": {"indirect_selection": "eager", "log_level_file": "debug", "require_generic_test_arguments_property": true, "vars": {}, "skip_nodes_if_on_run_start_fails": false, "strict_mode": false, "exclude": [], "send_anonymous_usage_stats": true, "populate_cache": true, "partial_parse": true, "static_parser": true, "defer": false, "write_json": true, "log_format_file": "debug", "require_batched_execution_for_custom_microbatch_strategy": false, "validate_macro_args": false, "use_colors_file": true, "quiet": false, "empty": false, "state_modified_compare_vars": false, "require_explicit_package_overrides_for_builtin_materializations": true, "show_resource_report": false, "profiles_dir": "C:\\Users\\Longin\\.dbt", "printer_width": 80, "project_dir": "C:\\Users\\Longin\\Desktop\\Projects\\across_analytics", "require_all_warnings_handled_by_warn_error": false, "upload_to_artifacts_ingest_api": false, "introspect": true, "warn_error_options": {"error": [], "warn": [], "silence": []}, "log_file_max_bytes": 10485760, "invocation_command": "dbt run --select marts", "require_nested_cumulative_type_params": false, "favor_state": false, "log_path": "C:\\Users\\Longin\\Desktop\\Projects\\across_analytics\\logs", "log_format": "default", "cache_selected_only": false, "require_resource_names_without_spaces": true, "select": ["marts"], "state_modified_compare_more_unrendered_values": false, "show_all_deprecations": false, "use_colors": true, "macro_debugging": false, "use_fast_test_edges": false, "print": true, "source_freshness_run_project_hooks": true, "version_check": true, "which": "run", "require_yaml_configuration_for_mf_time_spines": false, "log_level": "info", "partial_parse_file_diff": true}}